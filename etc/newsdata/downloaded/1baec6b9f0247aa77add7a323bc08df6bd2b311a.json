{"pub": "axios", "url": "https://axios.com/ai-automation-bias-trust-62ee0445-1fda-4143-b3d8-7d7ee8e328f6.html", "downloaded_at": "2019-10-19 16:20:06.052825+00:00", "title": "In AI we trust \u2014 too much", "language": "en", "text": "Why it matters: Over-reliance on potentially faulty AI can harm the people whose lives are shaped by critical decisions about employment, health care, legal proceedings and more. The big picture: This phenomenon is called automation bias. Early studies focused on autopilot for airplanes \u2014 but as automation technology becomes more complex, the problem could get much worse with more dangerous consequences.\n\nAI carries an aura of legitimacy and accuracy, burnished by overeager marketing departments and underinformed users.\n\nBut AI is just fancy math. Like any equation, if you give it incorrect inputs, it will return wrong answers. And if it learns patterns that don't reflect the real world, its output will be equally flawed.\n\nAutomation bias caused by simpler technologies has already been blamed for real-world disasters .\n\nIn 2016, a patient was prescribed the wrong medication when a pharmacist chose a similarly named drug from a list on a computer. A nurse noticed \u2014 but administered the meds anyway, assuming the electronic record was correct. The patient had heart and blood pressure problems as a result.\n\nIn 2010, a pipeline dumped nearly 1 million gallons of crude oil into Michigan wetlands and rivers after operators repeatedly ignored \"critical alarms.\" They were desensitized because of previous false alarms, according to a 2016 post-mortem report \u2014 showing another threat from over-reliance on machines.\n\n\"When people have to make decisions in relatively short timeframes, with little information \u2014 this is when people will tend to just trust whatever the algorithm gives them,\" says Ryan Kennedy, a University of Houston professor who researches trust and automation.\n\n\"The worst-case scenario is somebody taking these algorithmic recommendations, not understanding them, and putting us in a life or death situation,\" Kennedy tells Axios.\n\nNow, institutions are pushing AI systems further into high-stakes decisions.\n\nIn hospitals: A forthcoming study found that Stanford physicians \"followed the advice of [an AI] model even when it was pretty clearly wrong in some cases,\" says Matthew Lungren, a study author and the associate director of the university's Center for Artificial Intelligence in Medicine and Imaging.\n\nA forthcoming study found that Stanford physicians \"followed the advice of [an AI] model even when it was pretty clearly wrong in some cases,\" says Matthew Lungren, a study author and the associate director of the university's Center for Artificial Intelligence in Medicine and Imaging. At war: Weapons are increasingly automated, but usually still require human approval before they shoot to kill. In a 2004 paper, Missy Cummings, now the director of Duke University's Humans and Autonomy Lab, wrote that automated aids for aviation or defense \"can cause new errors in the operation of a system if not designed with human cognitive limitations in mind.\"\n\nWeapons are increasingly automated, but usually still require human approval before they shoot to kill. In a 2004 paper, Missy Cummings, now the director of Duke University's Humans and Autonomy Lab, wrote that automated aids for aviation or defense \"can cause new errors in the operation of a system if not designed with human cognitive limitations in mind.\" On the road: Sophisticated driver assists like Tesla's Autopilot still require people to intervene in dangerous situations. But a 2015 Duke study found that humans lose focus when they're just monitoring a car rather than driving it.\n\nAnd in the courtroom, human prejudice mixes in.\n\nIn a recent Harvard experiment, participants deviated from automated risk assessments presented to them \u2014 they were more likely to decrease their own risk predictions for white defendants but increase them for black defendants.\n\nAnd in a working paper, Alex Albright, a Harvard PhD candidate, found that Kentucky judges were more likely to choose harsher bail conditions than algorithms recommended for black defendants than for white defendants.\n\nWhat's next: More information about an algorithm's confidence level can give people clues for how much they should lean on it. Lungren says the Stanford physicians made fewer mistakes when they were given a recommendation and an accuracy estimate.", "description": "Computers can end up effectively making choices for people, thanks to human faith in machines.", "authors": [], "top_image": "https://images.axios.com/2Ubl97FPgOduZou8AzZ-csuGKpw=/0x0:1920x1080/1920x1080/2019/10/18/1571440562164.jpg", "published_at": "2019-10-19"}