{"pub": "theverge", "url": "https://theverge.com/2019/10/15/20908083/google-pixel-4-project-soli-radar-motion-sense-explainer", "downloaded_at": "2019-10-15 16:24:58.727206+00:00", "title": "Google\u2019s Project Soli: the tech behind Pixel 4\u2019s Motion Sense radar", "language": "en", "text": "By now, you\u2019ve heard: the new Google Pixel 4 has a tiny radar chip inside it, which allows you to swipe or wave your hand to do a few things. More importantly, Motion Sense (as Google has branded it) is designed to detect your presence. It knows if you\u2019re there. The technology comes from Project Soli, which was first demonstrated publicly in 2015 and is now inside the Pixel 4 as its first major commercial implementation. Responding to a few air gestures is fairly minor, but Google sees the potential for it to eventually become much more.\n\nThat\u2019s always the way with new computing interfaces. The mouse and the touchscreen led to giant revolutions in computing, so you see the potential for a new one to do the same thing. It\u2019s a trap Apple CEO Tim Cook himself fell into when he introduced the Digital Crown on the Apple Watch, saying it was as important as the mouse. (It wasn\u2019t.)\n\nLuckily, Google isn\u2019t claiming quite so much for Motion Sense, but it does have a similar problem. The gap between things that Motion Sense could do and what it actually does in this first version is huge. In theory, putting radar on a phone is a revolution. In practice, it could be seen as just a gimmick.\n\nMotion Sense has essentially three types of interactions, according to Brandon Barbello, Pixel product manager at Google. Understanding why each of them matters to the overall experience of using a Pixel 4 is key to understanding why Google thinks Motion Sense is more than the sum of its features \u2014 more than just a gimmick.\n\nMotion Sense has three main interactions: presence, reach, and gestures\n\nThe first type is presence. The Project Soli radar chip inside the Pixel 4 creates a small bubble of awareness around the phone when it\u2019s sitting on a table. (It\u2019s only on when it\u2019s facing up or out.) It\u2019s a hemisphere with a radius of a foot or two. It just keeps an eye out to ensure you\u2019re nearby and turns off the always-on display if you\u2019re not.\n\nThe second type is reach. This isn\u2019t much: the phone just pays attention to see if you\u2019re reaching for it and then quickly turns on the screen and activates the face unlock sensors. If an alarm or ringtone is going, the phone automatically quiets down a bit when it sees you\u2019re reaching for it.\n\nFinally, there are the actual gestures, of which there are only two. You can give it a quick wave to dismiss those calls or snooze alarms. You can also swipe left or right if music is playing to go forward or back. There are a couple more specific things you can do, but Google isn\u2019t opening up gestures to third-party developers for a while.\n\n\u201cIt\u2019s that it\u2019s so much better, and you\u2019re not going to notice it.\u201d\n\nMost of these features have been done before with other sensors. You\u2019ve been able to wave at phones and have their cameras detect it. When you pick up an iPhone, the accelerometer feels that happen and starts up Face ID. So I asked: will using radar for these features make the experience so much better that people will really notice it?\n\n\u201dIt isn\u2019t that it\u2019s so much better that you\u2019re going to notice it,\u201d says Barbello. \u201cIt\u2019s that it\u2019s so much better, and you\u2019re not going to notice it. [You will just think] that it\u2019s supposed to be this way.\u201d\n\nThere are technical reasons to prefer a radar chip to a camera, says Ivan Poupyrev, director of engineering for Google\u2019s ATAP division. Radar takes up much less power than a camera, for one thing. For another, it\u2019s... not a camera, and it can\u2019t personally identify you. \u201cIf you look at the radar signal, there is no discerning human characteristics,\u201d Poupyrev says.\n\nProject Soli also doesn\u2019t need line-of-sight like a camera. It can even work through other materials. There\u2019s no in-principle reason Soli couldn\u2019t work while the phone is face-down or even in your bag. The Soli technology could also, theoretically, work up to seven meters away (though power would be an issue).\n\nBut Soli, instantiated as Motion Sense on the Pixel 4, is much more limited to start. It\u2019s not a strictly technical limitation. Barbello says that \u201cwe can sense motions as precise as a butterfly\u2019s wings.\u201d I believe that, by the way, at least in theory. Three years ago, when Poupyrev first showed me Project Soli, I was able to turn a virtual dial on a smartwatch with the tiniest movement of my thumb and finger hovering above it.\n\nStill, Google has had to overcome some technical issues to get radar to work at this miniature scale. Fairly late in the project, Poupyrev admits that his team had to junk their original machine learning models and start over from scratch. \u201cIt was down to the wire.\u201d Plus, he adds, \u201cit\u2019s not like I can go buy a book \u2026 We had to invent every single thing. We had to invent from scratch. Every time you go read a paper about it, it assumes this gigantic radar to track satellites with an aperture of 10 meters.\u201d\n\n\u201cGetting this new technology into the phone is nothing short of a technology miracle, from my perspective,\u201d Poupyrev says. So while, in theory, Project Soli could detect anything from a butterfly\u2019s wings to a person standing seven meters away behind a wall, in practice, it probably needs more training to get there.\n\n\u201cIt was down to the wire.\u201d\n\nMainly, though, Google is limiting what Motion Sense can do because it is creating an entirely new interaction language, and it needs to limit the vocabulary at the beginning. Otherwise, it would be overwhelming \u2014 or at least super annoying.\n\nTake the swipe gesture: waving your hand over the phone to go forward or back a track when music is playing. \u201cWhat is swiping?\u201d Poupyrev asks. \u201cWe spent weeks trying to figure out swiping ... We have dozens and dozens of ways people do swiping.\u201d Some people do a \u201cthese aren\u2019t the droids\u201d gesture, some just flit their hand, some hold it flat, some sideways.\n\nIf Google had to show you a tutorial detailing exactly what way to hold your hand when you do a swipe, the jig would be up. You\u2019d hate it. There is a tutorial \u2014 featuring pok\u00e9mon \u2014 but it just handles the basics.\n\nGoogle even learned that we don\u2019t actually have a common language for what \u201cswipe left\u201d and \u201cswipe right\u201d mean. Different people move their hands in different directions depending on their mental model of whether the direction refers to their hand or the thing they\u2019re virtually interacting with. Google ended up having to put a preference setting in for those who wanted to flip the defaults.\n\nNobody needs radar; the key question is if it makes the experience better\n\nI keep going back to the idea that none of this is strictly necessary and therefore kind of gimmicky. Do I really need my phone to wake up a half-second before I touch it, saving me a tap on the screen? Is it really that difficult to hit the snooze button?\n\nIt\u2019s not. But then again, Google isn\u2019t promising the world here. It\u2019s just promising a slightly nicer, more seamless experience. Poupyrev points out that lots of people use the little autoreply buttons instead of just typing out \u201cyes\u201d manually. \u201cAt the end of the day, the technology that wins is what\u2019s easy to use,\u201d he argues. \u201cIt\u2019s just as simple as that. And removing a small amount of friction is what gets people more and more adapted to the technology.\u201d\n\nIf you just look at \u201cwhat it does for you,\u201d what Poupyrev calls \u201cthe toothbrush use case,\u201d he argues you\u2019re missing the point. The point isn\u2019t to layer on a ton of new features; it\u2019s to improve \u201cthe emotional components of interaction.\u201d\n\nSome of that is just fun. In addition to that pok\u00e9mon tutorial, there\u2019s a Pok\u00e9mon wallpaper you can wave at or even tickle, and a game co-developed with UsTwo. Most of it, though, is what Barbello says is \u201cmaking the core device functions a lot more natural.\u201d\n\nPoupyrev puts it a different way: \u201cHow does the toothbrush make you happy? You know it makes your life better, but it doesn\u2019t make you happy. A pok\u00e9mon makes you happy.\u201d", "description": "The new Google Pixel 4 has a tiny little radar chip inside it, which allows you to swipe and wave your hand over to do a few things. More importantly, Motion Sense (as Google has branded it) is designed to detect your presence. It knows if you\u2019re there. The technology comes from Project Soli.", "authors": ["Dieter Bohn", "Oct"], "top_image": "https://cdn.vox-cdn.com/thumbor/BwiInrvSlgt30sAuuNUcgpirHY4=/0x185:2040x1253/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/19285203/vpavic_191013_3731_0024.jpg", "published_at": "2019-10-15"}