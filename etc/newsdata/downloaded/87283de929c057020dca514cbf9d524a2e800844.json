{"pub": "nytimes", "url": "https://nytimes.com/2019/10/08/opinion/artificial-intelligence.html", "downloaded_at": "2019-10-09 00:35:42.567756+00:00", "title": "Opinion | How to Stop Superhuman A.I. Before It Stops Us", "language": "en", "text": "The solution, then, is to change the way we think about A.I. Instead of building machines that exist to achieve their objectives, we want a model that looks like this:\n\n\u201cMachines are beneficial to the extent that their actions can be expected to achieve our objectives.\u201d\n\nThis fix might seem small, but it is crucial. Machines that have our objectives as their only guiding principle will be necessarily uncertain about what these objectives are, because they are in us \u2014 all eight billion of us, in all our glorious variety, and in generations yet unborn \u2014 not in the machines.\n\nUncertainty about objectives might sound counterproductive, but it is actually an essential feature of safe intelligent systems. It implies that no matter how intelligent they become, machines will always defer to humans. They will ask permission when appropriate, they will accept correction, and, most important, they will allow themselves to be switched off \u2014 precisely because they want to avoid doing whatever it is that would give humans a reason to switch them off.\n\nOnce the focus shifts from building machines that are \u201cintelligent\u201d to ones that are \u201cbeneficial,\u201d controlling them will become a far easier feat. Consider it the difference between nuclear power and nuclear explosions: a nuclear explosion is nuclear power in an uncontrolled form, and we greatly prefer the controlled form.\n\nOf course, actually putting a model like this into practice requires a great deal of research. We need \u201cminimally invasive\u201d algorithms for decision making that prevent machines from messing with parts of the world whose value they are unsure about, as well as machines that learn more about our true, underlying preferences for how the future should unfold. Such machines will then face an age-old problem of moral philosophy: how to apportion benefits and costs among different individuals with conflicting desires.\n\nAll this could take a decade to complete \u2014 and even then, regulations will be required to ensure provably safe systems are adopted while those that don\u2019t conform are retired. This won\u2019t be easy. But it\u2019s clear that this model must be in place before the abilities of A.I. systems exceed those of humans in the areas that matter.\n\nIf we manage to do that, the result will be a new relationship between humans and machines, one that I hope will enable us to navigate the next few decades successfully.\n\nIf we fail, we may face a difficult choice: curtail A.I. research and forgo the enormous benefits that will flow from it, or risk losing control of our own future.\n\nSome skeptics within the A.I. community believe they see a third option: continue with business as usual, because superintelligent machines will never arrive. But that\u2019s as if a bus driver, with all of humanity as passengers, said, \u201cYes, I\u2019m driving as fast as I can toward a cliff, but trust me, we\u2019ll run out of gas before we get there!\u201d I\u2019d rather not take the risk.\n\nStuart Russell is the author of \u201cHuman Compatible: Artificial Intelligence and the Problem of Control.\u201d\n\nThe Times is committed to publishing a diversity of letters to the editor. We\u2019d like to hear what you think about this or any of our articles. Here are some tips. And here\u2019s our email: letters@nytimes.com.\n\nFollow The New York Times Opinion section on Facebook, Twitter (@NYTopinion) and Instagram.", "description": "The answer is to design artificial intelligence that\u2019s beneficial, not just smart.", "authors": ["Stuart Russell"], "top_image": "https://static01.nyt.com/images/2019/10/08/opinion/08Russell1/08Russell1-facebookJumbo.jpg", "published_at": "2019-10-08"}