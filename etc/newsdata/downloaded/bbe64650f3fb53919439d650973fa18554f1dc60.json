{"pub": "zdnet", "url": "https://www.zdnet.com/article/ai-and-ethics-the-debate-that-needs-to-be-had", "downloaded_at": "2019-09-16 05:36:55.744944+00:00", "title": "AI and ethics: The debate that needs to be had", "language": "en", "text": "Whether we know it or not, artificial intelligence (AI) is already steeped into everyday life. It's present in the way social media feeds are organised; the way predictive searches show up on Google; and how music services such as Spotify make song suggestions.\n\nThe technology is also helping transform the way enterprises do business.\n\nCommonwealth Bank of Australia, for instance, has applied AI to analyse 200 billion data points to free up more time so its customer service officers can focus on doing exactly what their title suggests: servicing customers. As a result, the bank has seen a 400% uplift in customer engagement.\n\nIBM is using the technology to preserve Australia's iconic beaches from washing away. Scientists are using the capabilities to put their time towards addressing coastal erosion, rather than on mapping it \u2013 which is very time-consuming.\n\nAccording to Data61 principal scientist in strategy and foresight Stefan Hajkowicz, AI creates a \"window of problem-solving capabilities\".\n\n\"AI is going to be able to save many people from cancer, it will improve mental health by AI-enabled counselling session, it will help reduce road accidents -- there are huge benefits in the future to your life due to AI,\" he said.\n\n\"Humanity in Australia desperately needs it. AI is going to be critical in solving dilemmas in healthcare, for instance, where healthcare expenditure is growing at unsustainable rates. AI is going to be crucial technology that is going to help pretty much every sector in our society.\"\n\nA PwC report released in 2017 predicted that AI will boost global GDP by 14% -- or $15.7 trillion \u2013 by 2030.\n\nWhat not to do\n\nHead of the School of Philosophy at the Australian National University (ANU) Seth Lazar believes that given the impact AI will have, there's scope to make the technology better.\n\n\"There are so many ways in which we could use AI for social good, but over the last year or two it has become apparent that there are potentially a lot of unintended consequences -- not to mention in which AI could potentially be used for bad reasons -- so there's huge demand and interest for developing AI with our values,\" he said.\n\nEvidence of when AI has gone wrong could be pinpointed to the time in the United States when AI algorithms were used to provide recommendations on prison sentences. A report from ProPublica concluded that the AI system was bias against black defendants as it consistently recommended longer sentences in comparison to white counterparts for the same crime.\n\nSee also: Artificial intelligence ethics policy (TechRepublic)\n\nThe United Nations Educational, Scientific, and Cultural Organisation (UNESCO) recently accused Apple's Siri, Microsoft's Cortana, and Amazon's Alexa, along with other female-voice digital assistants, of reinforcing \"commonly held gender biases\".\n\n\"Because the speech of most voice assistants is female, it sends a signal that women are obliging, docile and eager-to-please helpers, available at the touch of a button or with a blunt voice command like 'hey' or 'OK'. The assistant holds no power of agency beyond what the commander asks of it,\" the I'd Blush If I Could report outlined.\n\n\"It honours commands and responds to queries regardless of their tone or hostility. In many communities, this reinforces commonly held gender biases that women are subservient and tolerant of poor treatment.\"\n\nAnother example would have to be when Microsoft's AI bot, Tay, which was originally designed to interact with people online through casual and playful conversation, ended up hoovering good, bad, and ugly interactions. After less than 16 hours of launch, Tay turned into a brazen anti-Semite, stating, \"The Nazis were right\".\n\nProfessor of AI at the University of New South Wales Toby Walsh said: \"There's plentiful examples of how our algorithms can inherit the bias that exists in society we have if we're not careful\".\n\nHe noted however, if AI is carefully programmed to ask the right questions and designed by diverse teams, \"it will make much more just decisions\".\n\nFor both Lazar and Hajkowicz, their greatest concern about existing AI are the people who build these systems, which they say are just extracting the value of people working in Silicon Valley that come from elitist backgrounds.\n\n\"One of the key concerns that is often raised is that AI is being built by a lot of young white males in the 20-30 age bracket, because that's the AI workforce,\" Hajkowicz said.\n\n\"I think it immediately means they are building AI that is bias, but I think it's worth a look into how that is happening, and whether they are creating AI that is genuinely reflective of the diverse world.\"\n\nBuilding ethical AI with diversity\n\nPart of the solution to help overcome these systemic biases that are built into existing AI systems, according to Lazar, is to have open conversations about ethics -- with input from diverse views in terms of culture, gender, age, and socio-economic background -- and how it could be applied to AI.\n\n\"What we need to do is figure out how to develop systems that incorporate democratic values and we need to start the discussion within Australian society about what we want those values to be,\" he said.\n\n\"It's all about constant review and revision and recognising we do evolve as a society and hopefully we evolve to becoming morally better.\"\n\nspecial feature Managing AI and ML in the Enterprise The AI and ML deployments are well underway, but for CXOs the biggest issue will be managing these initiatives, and figuring out where the data science team fits in and what algorithms to buy versus build. Read More\n\nAt ANU, a research project, which is being led by Lazar, is currently underway and is focused on designing Australian values into AI systems. Part of it will also involve building a design framework for moral machine intelligence that can be widely deployed.\n\n\"We have to decide as a country is whether in the end, we want to be massive importers of technology, given that when you're importing technologies, you'll also be importing the values,\" he said.\n\nHajkowicz warned that if Australia fails to engage in the global AI ecosystem, it would put the country at risk of being exposed to other ethics that may not be compatible with Australia's.\n\n\"There are huge differences in the way countries approach AI. Some countries are using facial recognition to track the movements of people, for example,\" he said.\n\n\"On the other hand, it's much more limited and there's much more caution around how much it enters into somebody's personal life.\n\n\"I think Australia needs to think about what kind of AI future it wants. It's an open discussion and an AI ethics framework is the start; people need to drive us into the AI future that we want.\"\n\nBut of course, like anything, the approach needs to be considered. Otherwise, there's the potential to make mistakes, much like the one Stanford University made when it launched its Institute for Human-Centred Artificial Intelligence.\n\nThe goal for the institute was for a diverse group of people to have conversations about AI's impact and potential.\n\n\"Now is our opportunity to shape that future by putting humanists and social scientists alongside people who are developing artificial intelligence,\" Stanford President Marc Tessier-Lavigne said.\n\nExcept of the 121 faculty members that were initially announced, the majority of them were white and male.\n\nSetting ethics from the inside\n\nTechnology companies are also making a concerted effort to ensure the data that is fed into AI algorithms is ethical.\n\nLast year, Google set out its AI principles to ensure that all AI applications it builds meet seven key objectives: It is socially beneficial; avoids creating or reinforcing unfair bias; is built and tested for safety; is accountable to people; incorporates privacy design principles; upholds high standards of scientific excellence; and is made available for uses that accord with these principles.\n\nIn a blog post, Google CEO Sundar Pichai said the principles mark the company's recognition that such powerful technology raises equally powerful questions about its use.\n\n\"How AI is developed and used will have a significant impact on society for many years to come. As a leader in AI, we feel a deep responsibility to get this right,\" he said, assuring the principles are not \"theoretical concepts; they are concrete standards that will actively govern our research and product development and will impact our business decisions\".\n\nThe company explained how it's putting these principles into action through internal education, building tools and carrying research on topics in responsible AI, and reviewing its processes, as well as engaging with external stakeholders.\n\nAdditionally, the global tech giant announced the establishment of an external advisory council for the responsible development of AI. The makeup of the Advanced Technology External Advisory Council featured a mix of women and men from diverse backgrounds and different age groups.\n\nHowever, only a few weeks after inception, Google axed the group, after thousands of Google workers signed a petition protesting the appointment of a member who was \"vocally anti-trans, anti-LGBTQ, and anti-immigrant\".\n\n\"We're ending the council and going back to the drawing board,\" Google said. \"We'll continue to be responsible in our work on the important issues that AI raises, and will find different ways of getting outside opinions on these topics.\"\n\nBut Google isn't alone. Microsoft has its own set of AI principles and Facebook has co-founded an AI ethics research centre in Germany.\n\nGoverning the right from wrong\n\nWhile determining AI ethics will undoubtedly be a group effort, Walsh believes it will ultimately come down to regulators to set the boundaries, much like anything else today.\n\n\"If we don't regulate, for instance, to ensure that there are X% of taxis that are wheelchair accessible there wouldn't be. Uber is not going to provide cars for the disabled, unless it's regulated,\" he said.\n\nThis process of regulating AI in Australia has begun. The Commonwealth Scientific and Industrial Research Organisation (CSIRO) digital innovation arm Data61 published a discussion paper on key issues raised by large-scale AI, seeking answers to a handful of questions that are expected to inform the Australian government's approach to AI ethics.\n\nRead more: The real reason businesses are failing at AI (TechRepublic)\n\nAt the time, then-Minister for Human Services and Digital Transformation Michael Keenan said the government would use the paper's findings and the feedback received during the consultation period to develop a national AI ethics framework.\n\nIt is expected the framework will include a set of principles and practical measures that organisations and individuals can use as a guide to ensure their design, development, and use of AI \"meets community expectations\".\n\nLazar said legislators will bring cohesion to the AI ethics conversation in Australia.\n\n\"It would be good not to leave it up to individuals or companies.\u2026I think it's worth acknowledging that within each tech companies there are people taking these questions very seriously and doing superb work on how to do it in an ethical way,\" he said.\n\n\"But there are plenty of people in those companies who see it as important profit measures, but obviously they will behave in a way they behave and that's a worry, which is why legislation is crucial.\"\n\nRelated Coverage", "description": "Like anything, frameworks and boundaries need to be set -- and artificial intelligence should be no different.", "authors": ["Aimee Chanthadavong"], "top_image": "https://zdnet3.cbsistatic.com/hub/i/r/2019/09/12/4a7f9763-8301-4c54-9f5b-07ad86844c37/thumbnail/770x578/d20d5c3504b542e770ce0e599265716b/ai.jpg", "published_at": "2019-09-16"}