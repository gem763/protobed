{"pub": "guardian", "url": "https://theguardian.com/technology/2019/oct/03/google-data-harvesting-facial-recognition-people-of-color", "downloaded_at": "2019-10-03 23:30:32.584652+00:00", "published_at": "2019-10-03", "title": "Google reportedly targeted homeless people with 'dark skin' to improve AI", "language": "en", "text": "Subcontracted workers were instructed to persuade subjects to agree to face scans, mischaracterizing them as a \u2018selfie game\u2019 or \u2018survey\u2019\n\nFacial recognition technology\u2019s failures when it comes to accurately identifying people of color have been well documented and much criticized. But an attempt by Google to improve its facial recognition algorithms by collecting data from people with dark skin is raising further concerns about the ethics of the data harvesting.\n\nGoogle has been using subcontracted workers to collect face scans from members of the public in exchange for $5 gift cards, according to a report from the New York Daily News. The face scan collection project had been previously reported, but anonymous sources described unethical and deceptive practices to the Daily News.\n\nThe subcontracted workers were employed by staffing firm Randstad but directed by Google managers, according to the report. They were instructed to target people with \u201cdarker skin tones\u201d and those who would be more likely to be enticed by the $5 gift card, including homeless people and college students.\n\nPlan for massive facial recognition database sparks privacy concerns Read more\n\n\u201cThey said to target homeless people because they\u2019re the least likely to say anything to the media,\u201d a former contractor told the Daily News. \u201cThe homeless people didn\u2019t know what was going on at all.\u201d\n\n\u201cI feel like they wanted us to prey on the weak,\u201d another contractor told the Daily News.\n\nRandstad did not immediately respond to a request for comment. Google defended the project but said it was investigating allegations of wrongdoing.\n\nThe contractors also described using deceptive tactics to persuade subjects to agree to the face scans, including mischaracterizing the face scan as a \u201cselfie game\u201d or \u201csurvey\u201d, pressuring people to sign a consent form without reading it, and concealing the fact that the phone the research subjects were handed to \u201cplay with\u201d was taking video of their faces.\n\n\u201cWe\u2019re taking these claims seriously and investigating them,\u201d a Google spokesperson said in a statement. \u201cThe allegations regarding truthfulness and consent are in violation of our requirements for volunteer research studies and the training that we provided.\u201d\n\nThe spokesperson added that the \u201ccollection of face samples for machine learning training\u201d were intended to \u201cbuild fairness\u201d into the \u201cface unlock feature\u201d for the company\u2019s new phone, Pixel 4.\n\n\u201cIt\u2019s critical we have a diverse sample, which is an important part of building an inclusive product,\u201d the spokesperson said, adding that the face unlock feature will provide users with \u201ca powerful new security measure\u201d.\n\nBut the project has drawn harsh condemnation from digital civil rights and racial justice advocates.\n\nThe controversy touches on tricky questions about algorithmic bias and data privacy. Is it better to improve facial recognition for people of all skin colors \u2013 or ban the technology entirely, as a handful of US cities have done this year? And how much should users be compensated for providing companies such as Google with their personal data?\n\n\u201cThough research shows clearly that facial recognition system disproportionately misidentify black and brown faces, the goal should not be to improve the accuracy on this extremely invasive system,\u201d said Malkia Cyril, founder and executive director of MediaJustice, a national racial justice organization advancing media and technology rights. \u201cIn the context of existing racial bias in the criminal legal system and in counter-terrorism, it should be no one\u2019s goal to make the technology easier to use against people of color \u2013 especially black, AMEMSA [Arab, Middle Eastern, Muslim and South Asian] communities and undocumented people.\u201d\n\n\u201cThis is totally unacceptable conduct from a Google contractor. It\u2019s why the way AI is built today needs to change,\u201d said Jake Snow, an attorney with the ACLU of Northern California. \u201cThe answer to algorithmic bias is not to target the most vulnerable.\u201d\n\n\u201cWhether it\u2019s racist because it\u2019s accurate or because it\u2019s inaccurate, facial recognition and biometric tools in general fuel racial bias,\u201d said Cyril. \u201cNo amount of money or informed consent is enough to produce a weakly regulated technology already being used to violate the human rights of millions.\u201d\n\n\n\n", "description": "Subcontracted workers were instructed to persuade subjects to agree to face scans, mischaracterizing them as a \u2018selfie game\u2019 or \u2018survey\u2019", "authors": ["Julia Carrie Wong"], "top_image": "https://i.guim.co.uk/img/media/aa8ce649acfbca3f81f5bdea813d3539e96964d6/0_0_3000_1800/master/3000.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=918629e5b4791429a20cd3e7f58958a1"}