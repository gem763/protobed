{"pub": "thenextweb", "url": "https://thenextweb.com/artificial-intelligence/2019/10/17/study-facial-recognition-ais-alright-if-youre-cisgender-and-white", "downloaded_at": "2019-10-18 01:04:34.395067+00:00", "title": "Study: Facial recognition AI\u2019s alright, if you\u2019re cisgender and white", "language": "en", "text": "I\u2019m a cisgender white male. I can unlock my phone, sign into my bank account, and breeze through border patrol check points using my face with 98 percent accuracy.\n\nFacial recognition software is great for people who look like me. Sure, I still face the same existential dangers as everyone else: in the US and China we live in a total surveillance state. Law enforcement can track us without a warrant and put us into categories and groups such as \u201cdissident\u201d or \u201canti-cop,\u201d without ever investigating us. If I show my face in public, it\u2019s likely I\u2019m being tracked. But that doesn\u2019t make me special.\n\nWhat makes me special is that I look like a white man. My beard, short hair, and other features remind facial recognition software that I\u2019m the \u201cdefault\u201d when it comes to how AI categorizes people. If I were black, brown, a woman, transgender, or non-binary \u201cthe AI\u201d would struggle or fail to identify me. And, in this domain, that means cutting-edge technology from Microsoft, Amazon, IBM, and others inherently discriminates against anyone who doesn\u2019t look like me.\n\nUnfortunately, facial recognition proponents often don\u2019t see this as a problem. Scientists from the University of Boulder in Colorado recently conducted a study to demonstrate how poorly AI performs when attempting to recognize the faces of transgender and non-binary people. This is a problem that\u2019s been framed as horrific by people who believe AI should work for everyone, and \u201cnot a problem\u201d by those who think only in unnatural, binary terms.\n\nThe only YouTube comment on the University of Boulder\u2019s video discussing bias against transgender and non-binary individuals.\n\nIt\u2019s easy for a bigot to dismiss the tribulations of those whose identity falls outside of their world-view, but these people are missing the point entirely. We\u2019re teaching AI to ignore basic human physiology.\n\nResearcher Morgan Klaus Scheuerman, who worked on the Boulder study, appears to be a cis-male. But because he has long hair, IBM\u2019s facial recognition software labels him \u201cfemale.\u201d\n\nAnd then there\u2019s beards. About 1 in 14 women have a condition called hirsutism that causes them to grow \u201cexcess\u201d facial hair. Almost every human, male or female, grows some facial hair. However, at a rate of about 100 percent, AI concludes that facial hair is a male trait. Not because it is, but because it\u2019s socially unacceptable for a woman to have facial hair.\n\nIn 20 years, if it suddenly becomes posh for women to grow beards and men to maintain a smooth face, AI trained on datasets of binary images would label those with beards as women, whether they are or not.\n\nIt\u2019s important for people to understand that AI is stupid, it doesn\u2019t understand gender or race anymore than a toaster understands thermodynamics. It just tries to understand how the people developing it see race and gender \u2013 meaning those who set its rewards and success parameters determine the threshold for accuracy. If you\u2019re all white, everything\u2019s alright.\n\nIf you\u2019re black? You could be a member of Congress, but Amazon\u2018s AI (the same system used by many law enforcement agencies in the US) is likely to mislabel you as a criminal instead. Google\u2019s might think you\u2019re a gorilla. Worse, if you\u2019re a black woman, all of the major facial recognition systems have a strong chance of labeling you as a man.\n\nBut if you\u2019re non-binary or transgender, things become even worse. According to one researcher who worked on the Boulder study:\n\nIf you\u2019re a cisgender man, or a cisgender women, you\u2019re doing pretty okay in these systems. If you\u2019re a trans woman, not as well. And if you\u2019re a trans man\u2026 looking at Amazon\u2019s Rekognition\u2026 you\u2019re at about 61 percent. But if we step beyond people who have binary gender identities\u2026 a hundred percent of the time you\u2019re going to be classified incorrectly.\n\nFacial recognition software reinforces the flawed social constructs that men with long hair are feminine, women with short hair are masculine, intersex people don\u2019t matter, and the bar for viability in an AI product is \u201cif it works for cis-gender white men, it\u2019s ready for launch.\u201d\n\nIt\u2019s easy to ignore this problem if it doesn\u2019t affect you, because it\u2019s hard to see the \u201cdangers\u201d of facial recognition software. Black people and women can use Apple\u2019s Face ID, so we assume that this onboard example of machine learning represents the database-connected reality of general recogntion. It does not.\n\nFaceID compares the face it sees to a database that consists of just you. General detection, such as finding a face in the wild, is done through programmable thresholds. This just means that Amazon\u2019s Rekognition, for example, can be set to 99 percent confidence (it won\u2019t make a decision if it\u2019s not 99 percent sure), but then it becomes useless as it\u2019ll only work on white cisgender men and women with perfect portrait photos and great lighting. Law enforcement agencies lower the accuracy threshold far below Amazon\u2019s recommended minimum setting so that the system starts making \u201cguesses\u201d about non-whites.\n\nCops, politicians, banks, airports, border patrol, ICE, UK passport offices, and thousands of other organizations and government entities use facial recognition every day despite the fact it creates and automates cisgender white privilege.\n\nIf Microsoft released a version of Windows that demonstrably and qualitatively worked better for blacks, Asians, or Middle-Easterners, it\u2019s likely the outrage would be enough to shake the trillion-dollar company\u2019s iron-like vise on the technology world. Never mind that almost all AI technology that labels or categorizes people into subsets based on inherent human features such as sex, gender, and race exacerbates secondary systemic bigotry.\n\nFacial recognition software designed to make things generally more efficient is a \u2018cisgender whites only\u2019 sign barring access to the future. It gives cisgender white men a premium advantage over the rest of the world.\n\nThere\u2019s certainly hope that, one day, researchers will figure out a way to combat these biases. But, right now, any government or business deploying these technologies for broad use risks intentionally using AI to spread, codify, and reinforce current notions of bigotry, racism, and misogyny.\n\nRead next: Rant: Google has no good excuse for the Pixel 4's missing ultrawide camera", "description": "", "authors": ["Tristan Greene"], "top_image": "https://img-cdn.tnwcdn.com/image/tnw?filter_last=1&fit=1280%2C640&url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2019%2F10%2Fwhitesonly.jpg&signature=8e9a698ec8f08801303b0ac5d5d9e3a6", "published_at": "2019-10-17"}