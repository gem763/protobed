{"pub": "arstechnica", "url": "https://arstechnica.com/science/2019/10/i-couldve-told-you-that-might-have-a-useful-role-to-play-in-science", "downloaded_at": "2019-10-25 00:37:33.735210+00:00", "title": "\u201cI could\u2019ve told you that\u201d might have a useful role to play in science", "language": "en", "text": "Last year, a huge group of researchers collaborated to try to replicate the results of some very famous social science research. They determined that only 62% of the studies found similar results when they were repeated. But the researchers found something else intriguing: other scientists were astonishingly good at guessing which of the results would replicate.\n\nDoes that mean we can just ask scientists for their hunch on what research is robust? It's a lot more complicated than that, but predictions could have a useful role to play in science, and new projects are springing up to make use of them.\n\nObvious or not?\n\nThe success of scientists' predictions doesn't mean that every individual scientist (or non-scientist) can just rely on their gut to guess which scientific results are true. We have a term for that: \"confirmation bias.\" But that doesn't mean that scientists can't have informed, well-founded suspicions about some research. Predictions taken from across a whole group of scientists could average out the bias and highlight the well-informed doubts and thus be a very useful contribution to scientific research.\n\nWriting in Science this week, economists Stefano DellaVigna, Devin Pope, and Eva Vivalt point out some of the benefits that gathering predictions could have for science.\n\nRight now, we have very little sense of what people think about a topic before a study is conducted and whether they update their beliefs afterward. Everyone is susceptible to hindsight bias, which means that a new study, even if it's surprising, can easily induce a reaction of \"Pfft, I knew that already.\" But is that actually true?\n\nWithout getting people's thoughts down before the results are known, it's difficult to determine whether the results of a study are truly what was expected or whether people have just nudged their beliefs to match up with the new data. Of course, the opposite can be true: people can dig their heels in and refuse to change their beliefs.\n\nEveryone\u2014including scientists\u2014is susceptible to foibles like hindsight bias and confirmation bias. So having data on how predictions and belief updating actually work could be really useful to help researchers understand how the people they most need to convince\u2014like policy-makers and fellow scientists\u2014respond to evidence. And on a truly grand scale, predictions could make it possible to understand how scientific fields shift their consensus over time.\n\nFiguring out which studies are genuinely surprising would also help to put scientific results in their proper context. If they're properly astonishing, maybe they need a lot more replication and sanity checking. And cold, hard evidence of surprising results could help with publication bias, suggest DellaVigna, Pope, and Vivalt. Journals often refuse to publish results that seem boring and not \"surprising\" enough\u2014having the forecasts to show how surprising results really are could help researchers jump through this hoop. (The authors don't question the wisdom of having that hoop in the first place.)\n\nWhat makes predictions tick\n\nAs a range of different forecasting projects pop up, different ones are figuring out distinct ways to get people to provide them and how to make use of predictions. Some projects use betting markets, in which people can see other participants' guesses and update their own accordingly. DellaVigna and Vivalt's forecasting platform keeps people's responses private to avoid them being biased by other people's opinions.\n\nGetting predictions about research could look very different from one project to the next. Target audiences could differ (scientists, policy-makers, the general public). Researchers might want to ask people for more general predictions, like \"Do you think this study will have a significant result?\"\u2014or for very specific predictions, like \"What do you think the average result will be?\"\n\nAs more projects start to use forecasting, it should become possible to understand how to make predictions of scientific research more accurate. This could help scientists winnow down their options when they're figuring out which research avenue could be the most promising.\n\nIt could also help researchers figure out why betting markets are good at predicting which work will be robust. A huge prediction project at the University of Melbourne, repliCATS, is doing just that: figuring out how to tap into the instincts of scientists about their peers' work, establishing what makes scientists trust each other's studies, and how well-reasoned predictions from large groups of scientists could accurately estimate which research is likely to replicate.\n\nThe use of predictions in science is very much in its infancy, but as it grows, its data could become increasingly useful\u2014like a second, meta-dataset on top of the actual research itself.\n\nScience, 2018. DOI: 10.1126/science.aaz1704 (About DOIs).\n\nListing image by flickr user: anlopelope", "description": "Individual predictions might not tell us much, but group predictions are useful.", "authors": ["Cathleen O'Grady"], "top_image": "https://cdn.arstechnica.net/wp-content/uploads/2019/10/3425579431_402f2a5ddb_h-760x380.jpg", "published_at": "2019-10-24"}