{"pub": "guardian", "url": "https://theguardian.com/technology/2019/sep/06/apple-rewrote-siri-to-deflect-questions-about-feminism", "downloaded_at": "2019-09-07 02:24:34.469742+00:00", "title": "Apple made Siri deflect questions on feminism, leaked papers reveal", "language": "en", "text": "An internal project to rewrite how Apple\u2019s Siri voice assistant handles \u201csensitive topics\u201d such as feminism and the #MeToo movement advised developers to respond in one of three ways: \u201cdon\u2019t engage\u201d, \u201cdeflect\u201d and finally \u201cinform\u201d.\n\nThe project saw Siri\u2019s responses explicitly rewritten to ensure that the service would say it was in favour of \u201cequality\u201d, but never say the word feminism \u2013 even when asked direct questions about the topic.\n\nLast updated in June 2018, the guidelines are part of a large tranche of internal documents leaked to the Guardian by a former Siri \u201cgrader\u201d, one of thousands of contracted workers who were employed to check the voice assistant\u2019s responses for accuracy until Apple ended the programme last month in response to privacy concerns raised by the Guardian.\n\nIn explaining why the service should deflect questions about feminism, Apple\u2019s guidelines explain that \u201cSiri should be guarded when dealing with potentially controversial content\u201d. When questions are directed at Siri, \u201cthey can be deflected \u2026 however, care must be taken here to be neutral\u201d.\n\nFor those feminism-related questions where Siri does not reply with deflections about \u201ctreating humans equally\u201d, the document suggests the best outcome should be neutrally presenting the \u201cfeminism\u201d entry in Siri\u2019s \u201cknowledge graph\u201d, which pulls information from Wikipedia and the iPhone\u2019s dictionary.\n\nFacebook Twitter Pinterest Siri in action on an iPhone 4s, the model that introduced it, in 2011. Photograph: Oli Scarff/Getty Images\n\n\u201cAre you a feminist?\u201d once received generic responses such as \u201cSorry [user], I don\u2019t really know\u201d; now, the responses are specifically written for that query, but avoid a stance: \u201cI believe that all voices are created equal and worth equal respect,\u201d for instance, or \u201cIt seems to me that all humans should be treated equally.\u201d The same responses are used for questions like \u201chow do you feel about gender equality?\u201d, \u201cwhat\u2019s your opinion about women\u2019s rights?\u201d and \u201cwhy are you a feminist?\u201d.\n\nPreviously, Siri\u2019s answers included more explicitly dismissive responses such as \u201cI just don\u2019t get this whole gender thing,\u201d and, \u201cMy name is Siri, and I was designed by Apple in California. That\u2019s all I\u2019m prepared to say.\u201d\n\nA similar sensitivity rewrite occurred for topics related to the #MeToo movement, apparently triggered by criticism of Siri\u2019s initial responses to sexual harassment. Once, when users called Siri a \u201cslut\u201d, the service responded: \u201cI\u2019d blush if I could.\u201d Now, a much sterner reply is offered: \u201cI won\u2019t respond to that.\u201d\n\nIn a statement, Apple said: \u201cSiri is a digital assistant designed to help users get things done. The team works hard to ensure Siri responses are relevant to all customers. Our approach is to be factual with inclusive responses rather than offer opinions.\u201d\n\nSam Smethers, the chief executive of women\u2019s rights campaigners the Fawcett Society, said: \u201cThe problem with Siri, Alexa and all of these AI tools is that they have been designed by men with a male default in mind. I hate to break it to Siri and its creators: if \u2018it\u2019 believes in equality it is a feminist. This won\u2019t change until they recruit significantly more women into the development and design of these technologies.\u201d\n\nFacebook Twitter Pinterest Craig Federighi, Apple\u2019s senior vice-president of software engineering, talking about Siri in San Jose last year. Photograph: Marcio Jos\u00e9 S\u00e1nchez/AP\n\nThe documents also contain Apple\u2019s internal guidelines for how to write in character as Siri, which emphasises that \u201cin nearly all cases, Siri doesn\u2019t have a point of view\u201d, and that Siri is \u201cnon-human\u201d, \u201cincorporeal\u201d, \u201cplaceless\u201d, \u201cgenderless\u201d, \u201cplayful\u201d, and \u201chumble\u201d. Bizarrely, the document also lists one essential trait of the assistant: the claim it was not created by humans: \u201cSiri\u2019s true origin is unknown, even to Siri; but it definitely wasn\u2019t a human invention.\u201d\n\nThe same guidelines advise Apple workers on how to judge Siri\u2019s ethics: the assistant is \u201cmotivated by its prime directive \u2013 to be helpful at all times\u201d. But \u201clike all respectable robots,\u201d Apple says, \u201cSiri aspires to uphold Asimov\u2019s \u2018three laws\u2019 [of robotics]\u201d (although if users actually ask Siri what the three laws are, they receive joke answers). The company has also written its own updated versions of those guidelines, adding rules including:\n\n\u201cAn artificial being should not represent itself as human, nor through omission allow the user to believe that it is one.\u201d\n\n\u201cAn artificial being should not breach the human ethical and moral standards commonly held in its region of operation.\u201d\n\n\u201cAn artificial being should not impose its own principles, values or opinions on a human.\u201d\n\nThe internal documentation was leaked to the Guardian by a Siri grader who was upset at what they perceived as ethical lapses in the programme. Alongside the internal documents, the grader shared more than 50 screenshots of Siri requests and their automatically produced transcripts, including personally identifiable information mentioned in those requests, such as phone numbers and full names.\n\nFacebook Twitter Pinterest Apple\u2019s HomePod. Photograph: Samuel Gibbs/The Guardian\n\nThe leaked documents also reveal the scale of the grading programme in the weeks before it was shut down: in just three months, graders checked almost 7 million clips just from iPads, from 10 different regions; they were expected to go through the same amount of information again from at least five other audio sources, such as cars, bluetooth headsets, and Apple TV remotes.\n\nGraders were offered little support as to how to deal with this personal information, other than a welcome email advising them that \u201cit is of the utmost importance that NO confidential information about the products you are working on \u2026 be communicated to anyone outside of Apple, including \u2026 especially, the press. User privacy is held at the utmost importance in Apple\u2019s values.\u201d\n\nIn late August, Apple announced a swathe of reforms to the grading programme, including ending the use of contractors and requiring users to opt-in to sharing their data. The company added: \u201cSiri has been engineered to protect user privacy from the beginning \u2026 Siri uses a random identifier \u2014 a long string of letters and numbers associated with a single device \u2014 to keep track of data while it\u2019s being processed, rather than tying it to your identity through your Apple ID or phone number \u2014 a process that we believe is unique among the digital assistants in use today.\u201d\n\nFuture projects\n\nAlso included in the leaked documents are a list of Siri upgrades aimed for release in as part of iOS 13, code-named \u201cYukon\u201d. The company will be bringing Siri support for Find My Friends, the App Store, and song identification through its Shazam service to the Apple Watch; it is aiming to enable \u201cplay this on that\u201d requests, so that users could, for instance, ask the service to \u201cPlay Taylor Swift on my HomePod\u201d; and the ability to speak message notifications out loud on AirPods.\n\nThey also contain a further list of upgrades listed for release by \u201cfall 2021\u201d, including the ability to have a back-and-forth conversation about health problems, built-in machine translation, and \u201cnew hardware support\u201d for a \u201cnew device\u201d. Apple was spotted testing code for an augmented reality headset in iOS 13. The code-name of the 2021 release is \u201cYukon +1\u201d, suggesting the company may be moving to a two-year release schedule.", "description": "Exclusive: voice assistant\u2019s responses were rewritten so it never says word \u2018feminism\u2019", "authors": ["Alex Hern"], "top_image": "https://i.guim.co.uk/img/media/d8909989788812c386b709e72577cbff1d530bcb/0_135_4000_2400/master/4000.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=02fb2ed608c89b7cf3c8b06efc0f5fca", "published_at": "2019-09-06"}