{"pub": "axios", "url": "https://axios.com/computer-vision-robot-factory-manufacturing-25e1e562-9337-4487-8ca5-1e007b65655c.html", "downloaded_at": "2019-10-27 00:07:22.894428+00:00", "title": "Teaching robots to see \u2014 and understand", "language": "en", "text": "The big picture: Machine vision can help robots navigate spaces previously closed off to them, like a crowded warehouse floor or a cluttered front lawn. And it's critical for tasks that require dexterity, like packing a box with oddly shaped objects.\n\nPlus, AI can help make sense of the avalanche of video footage recorded daily, which far outstrips humanity's ability to digest it.\n\nCompanies are scrambling to make use of that data to understand how people and vehicles move, or to check for tiny imperfections in new products.\n\nThe rise of AI-monitored cameras is also making surveillance inescapable at work and in public spaces.\n\nDriving the news: In a report first shared with Axios, LDV Capital, a venture firm that invests in visual technologies, predicts an upheaval in manufacturing and logistics, driven primarily by computer vision.\n\n\"The majority of global factories, ports, and warehouses are understaffed and ill-equipped to meet still-rising requirements,\" the report reads. Visual technologies will help change that, LDV argues.\n\nIn China, some \"lights-off\" factories have been built to operate without a single human present. But the U.S. will largely see robots employed in factories and warehouses not custom-built for robots, says Abby Hunter-Syed, VP of operations at LDV.\n\nYes, but: It'll take more than just high-fidelity cameras and fast AI perception to make an intelligent robot.\n\nA big unsolved challenge is imbuing robots with a deeper understanding of the world around them, so that they can interpret what they see and react to it.\n\n\"Domestic robots, for example, are just not going to arrive until machines can interpret scenes well,\" says Gary Marcus, co-founder of robotics company Robust.ai. \"You can do Roomba, but not Rosie the Robot.\"\n\nA broad understanding of the world helps us humans avoid confounding errors when we look around.\n\nEven if we see a cloud perfectly shaped like a horse, we never actually think it's a flying horse because we get how clouds work.\n\nThe same ability helps us handle objects easily \u2014 even ones we've never seen before. Humans can generally guess how to place an item on a surface so that it stays upright, for example, rather than tipping over.\n\n\"We've built physics models in our heads, and we've not quite been able to transfer them to robots,\" says Avideh Zakhor, a Berkeley professor who studies computer vision.\n\nThe big question: How much of the problem is solvable with incremental improvements in machine vision, before robots need better common sense?\n\nEvan Nisselson, a partner at LDV, argues that industry can get 85% or 90% of the way toward lucrative automation with better machine vision.\n\nBut that depends on how much warehouses and factories can remove variability and chaos from the areas where robots are working.\n\nThe bottom line: \"The Rubicon here, which we haven't crossed yet, is to not just be able to see objects,\" says Marcus. \"It's interpreting scenes that will be the breakthrough.\"", "description": "It'll take more than just high-quality computer vision to make an intelligent robot.", "authors": [], "top_image": "https://images.axios.com/BxbtRBOjRt1I9AS18KJCNAFvgZ0=/0x0:1920x1080/1920x1080/2019/10/25/1572025251591.jpg", "published_at": "2019-10-26"}