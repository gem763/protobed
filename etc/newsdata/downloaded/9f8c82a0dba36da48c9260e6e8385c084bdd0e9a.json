{"pub": "techcrunch", "url": "https://techcrunch.com/2019/10/15/databricks-brings-its-delta-lake-open-source-project-to-the-linux-foundation", "downloaded_at": "2019-10-16 05:10:12.491160+00:00", "title": "Databricks brings its Delta Lake project to the Linux Foundation \u2013 TechCrunch", "language": "en", "text": "Databricks, the big data analytics service founded by the original developers of Apache Spark, today announced that it is bringing its Delta Lake open-source project for building data lakes to the Linux Foundation and under an open governance model. The company announced the launch of Delta Lake earlier this year and even though it\u2019s still a relatively new project, it has already been adopted by many organizations and has found backing from companies like Intel, Alibaba and Booz Allen Hamilton.\n\n\u201cIn 2013, we had a small project where we added SQL to Spark at Databricks [\u2026] and donated it to the Apache Foundation,\u201d Databricks CEO and co-founder Ali Ghodsi told me. \u201cOver the years, slowly people have changed how they actually leverage Spark and only in the last year or so it really started to dawn upon us that there\u2019s a new pattern that\u2019s emerging and Spark is being used in a completely different way than maybe we had planned initially.\u201d\n\nThis pattern, he said, is that companies are taking all of their data and putting it into data lakes and then do a couple of things with this data, machine learning and data science being the obvious ones. But they are also doing things that are more traditionally associated with data warehouses, like business intelligence and reporting. The term Ghodsi uses for this kind of usage is \u2018Lake House.\u2019 More and more, Databricks is seeing that Spark is being used for this purpose and not just to replace Hadoop and doing ETL (extract, transform, load). \u201cThis kind of Lake House patterns we\u2019ve seen emerge more and more and we wanted to double down on it.\u201d\n\nSpark 3.0, which is launching today, enables more of these use cases and speeds them up significantly, in addition to the launch of a new feature that enables you to add a pluggable data catalog to Spark.\n\nData Lake, Ghodsi said, is essentially the data layer of the Lake House pattern. It brings support for ACID transactions to data lakes, scalable metadata handling, and data versioning, for example. All the data is stored in the Apache Parquet format and users can enforce schemas (and change them with relative ease if necessary).\n\nIt\u2019s interesting to see Databricks choose the Linux Foundation for this project, given that its roots are in the Apache Foundation. \u201cWe\u2019re super excited to partner with them,\u201d Ghodsi said about why the company chose the Linux Foundation. \u201cThey run the biggest projects on the planet, including the Linux project but also a lot of cloud projects. The cloud-native stuff is all in the Linux Foundation.\u201d\n\n\u201cBringing Delta Lake under the neutral home of the Linux Foundation will help the open source community dependent on the project develop the technology addressing how big data is stored and processed, both on-prem and in the cloud,\u201d said Michael Dolan, VP of Strategic Programs at the Linux Foundation. \u201cThe Linux Foundation helps open source communities leverage an open governance model to enable broad industry contribution and consensus building, which will improve the state of the art for data storage and reliability.\u201d", "description": "Databricks, the big data analytics service founded by the original developers of Apache Spark, today announced that it is bringing its Delta Lake open-source project for building data lakes to the Linux Foundation and under an open governance model. The company announced the launch of Delta Lake earlier this year and even though it\u2019s still [\u2026]", "authors": [], "top_image": "https://techcrunch.com/wp-content/uploads/2019/10/GettyImages-916191726.jpg?w=600", "published_at": "2019-10-15"}