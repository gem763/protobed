{"pub": "axios", "url": "https://axios.com/deepfake-cda-230-cb30028b-7108-4825-8113-75f23c5aad68.html", "downloaded_at": "2019-10-19 17:03:54.673692+00:00", "title": "The roots of the deepfake threat", "language": "en", "text": "Driving the news: Speaking at a Friday conference hosted by the Notre Dame Technology Ethics Center, deepfake experts from law, business and computer science described an entrenched problem with roots far deeper than the first AI-manipulated videos that surfaced two years ago.\n\nThe technology that powers them goes back to the beginning of the decade, when harmful AI-generated revenge porn or fraudulent audio deepfakes weren't yet on the map.\n\n\"We as researchers did not have this in mind when we created this software,\" Notre Dame computer scientist Pat Flynn says. \"We should have. I admit to a failing as a community.\"\n\nBut the story begins in earnest back in the 1990s, along with the early internet.\n\nWhen web browsers started supporting images, people predictably uploaded porn with celebrities' faces pasted on. That, it turns out, was just the beginning. Now, 96% of deepfakes are nonconsensual porn, nearly all of them targeting women.\n\n\"There was something much more dark coming if we sat back [in the 90s] and let people use women's faces and bodies in ways they never consented to,\" Mary Anne Franks, a law professor at the University of Miami, points out.\n\nPart of a 1996 law, the Communications Decency Act allowed internet platforms to keep their immunity from lawsuits over user-created content even when they moderated or \"edited\" the postings.\n\nNow, lawmakers are toying with revising it \u2014 or even (less likely) yanking it completely, Axios tech policy reporter Margaret Harding McGill reported this week.\n\nThe argument is that companies are not holding up their end of the bargain. \"The responsibility lies with platforms. They are exploiting these types of fake content,\" Franks said. \"We can't keep acting like they're simply innocent bystanders.\"\n\nA massive challenge for platforms is dealing with misinformation quickly, before it can cause widespread damage.\n\nSer-Nam Lim, a Facebook AI research manager, described the company's goal: an automated system that flags potentially manipulated media to humans for fact checking.\n\nBut, as I argued on a separate panel Friday, platforms are the first line of defense against viral forgeries. Facebook's human fact-checking can be painfully slow \u2014 in one recent case, it took more than a day and a half \u2014 and so the company's immediate reaction, or lack thereof, carries a lot of weight.\n\nGo deeper: Social media reconsiders its relationship with the truth", "description": "It's a long-brewing mess that involves a decades-old law and tech companies that profit from viral lies and forgeries.", "authors": [], "top_image": "https://images.axios.com/1d79CmSsIABENcLrZtC9BIzbPh4=/0x0:1920x1080/1920x1080/2019/10/19/1571460015199.jpg", "published_at": "2019-10-19"}