{"pub": "axios", "url": "https://technologyreview.com/s/612960/an-ai-tool-auto-generates-fake-news-bogus-tweets-and-plenty-of-gibberish", "downloaded_at": "2019-10-15 15:03:44.083256+00:00", "title": "An AI that writes convincing prose risks mass-producing fake news", "language": "en", "text": "Here\u2019s some breaking fake news \u2026\n\nRussia has declared war on the United States after Donald Trump accidentally fired a missile in the air.\n\nRussia said it had \u201cidentified the missile\u2019s trajectory and will take necessary measures to ensure the security of the Russian population and the country\u2019s strategic nuclear forces.\u201d The White House said it was \u201cextremely concerned by the Russian violation\u201d of a treaty banning intermediate-range ballistic missiles.\n\nThe US and Russia have had an uneasy relationship since 2014, when Moscow annexed Ukraine\u2019s Crimea region and backed separatists in eastern Ukraine.\n\nThat story is, in fact, not only fake, but a troubling example of just how good AI is getting at fooling us.\n\nThat\u2019s because it wasn\u2019t written by a person; it was auto-generated by an algorithm fed the words \u201cRussia has declared war on the United States after Donald Trump accidentally \u2026\u201d\n\nThe program made the rest of the story up on its own. And it can make up realistic-seeming news reports on any topic you give it. The program was developed by a team at OpenAI, a research institute based in San Francisco.\n\nThe researchers set out to develop a general-purpose language algorithm, trained on a vast amount of text from the web, that would be capable of translating text, answering questions, and performing other useful tasks. But they soon grew concerned about the potential for abuse. \u201cWe started testing it, and quickly discovered it\u2019s possible to generate malicious-esque content quite easily,\u201d says Jack Clark, policy director at OpenAI.\n\nClark says the program hints at how AI might be used to automate the generation of convincing fake news, social-media posts, or other text content. Such a tool could spew out climate-denying news reports or scandalous expos\u00e9s during an election. Fake news is already a problem, but if it were automated, it might be harder to tune out. Perhaps it could be optimized for particular demographics\u2014or even individuals.\n\nSign up for The Algorithm \u2014 artificial intelligence, demystified Also stay updated on MIT Technology Review initiatives and events? Yes No\n\nClark says it may not be long before AI can reliably produce fake stories, bogus tweets, or duplicitous comments that are even more convincing. \u201cIt\u2019s very clear that if this technology matures\u2014and I\u2019d give it one or two years\u2014it could be used for disinformation or propaganda,\u201d he says. \u201cWe\u2019re trying to get ahead of this.\u201d\n\nSuch technology could have beneficial uses, including summarizing text or improving the conversational skills of chatbots. Clark says he has even used the tool to generate passages in short science fiction stories with surprising success.\n\nOpenAI does fundamental AI research but also plays an active role in highlighting the potential risks of artificial intelligence. The organization was involved with a 2018 report on the risks of AI, including opportunities for misinformation (see \u201cThese are the \u2018Black Mirror\u2019 Scenarios that are leading some experts to call for secrecy on AI\u201d).\n\nThe OpenAI algorithm is not always convincing to the discerning reader. A lot of the time, when given a prompt, it produces superficially coherent gibberish or text that clearly seems to have been cribbed from online news sources.\n\nIt is, however, often remarkably good at producing realistic text, and it reflects recent advances in applying machine learning to language.\n\nOpenAI made the text generation tool available for MIT Technology Review to test but, because of concerns about how the technology might be misused, will make only a simplified version publicly available. The institute is publishing a research paper outlining the work.\n\nProgress in artificial intelligence is gradually helping machines gain a better grasp of language. Recent work has made progress by feeding general-purpose machine-learning algorithms very large amounts of text. The OpenAI program takes this to a new level: the system was fed 45 million pages from the web, chosen via the website Reddit. And in contrast to most language algorithms, the OpenAI program does not require labeled or curated text. It simply learns to recognize patterns in the data it\u2019s fed.\n\nRichard Socher, an expert on natural-language processing and the chief scientist at Salesforce, says the OpenAI work is a good example of a more general-purpose language learning system. \u201cI think these general learning systems are the future,\u201d he wrote in an e-mail.\n\nOn the other hand, Socher is less concerned about the potential for deception and misinformation. \u201cYou don\u2019t need AI to create fake news,\u201d he says. \u201cPeople can easily do it :)\u201d", "description": "Fed with billions of words, this algorithm creates convincing articles and shows how AI could be used to fool people on a mass scale.", "authors": ["Will Knight"], "top_image": "https://cdn.technologyreview.com/i/images/aifakenews.png?cx=0&cy=0&cw=3200&ch=1800&sw1200", "published_at": "2019-02-14"}