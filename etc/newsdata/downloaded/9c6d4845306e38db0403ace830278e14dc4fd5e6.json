{"pub": "nytimes", "url": "https://nytimes.com/2019/09/06/opinion/ai-explainability.html", "downloaded_at": "2019-09-07 13:36:02.071993+00:00", "title": "Opinion | How to Build Artificial Intelligence We Can Trust", "language": "en", "text": "Without the concepts of time, space and causality, much of common sense is impossible. We all know, for example, that any given animal\u2019s life begins with its birth and ends with its death; that at every moment during its life it occupies some particular region in space; that two animals can\u2019t ordinarily be in the same space at the same time ; that two animals can be in the same space at different times; and so on.\n\nWe don\u2019t have to be taught this kind of knowledge explicitly. It is the set of background assumptions, the conceptual framework, that makes possible all our other thinking about the world.\n\nYet few people working in A.I. are even trying to build such background assumptions into their machines. We\u2019re not saying that doing so is easy \u2014 on the contrary, it\u2019s a significant theoretical and practical challenge \u2014 but we\u2019re not going to get sophisticated computer intelligence without it.\n\nIf we build machines equipped with rich conceptual understanding, some other worries will go away. The philosopher Nick Bostrom, for example, has imagined a scenario in which a powerful A.I. machine instructed to make paper clips doesn\u2019t know when to stop and eventually turns the whole world \u2014 people included \u2014 into paper clips.\n\nIn our view, this kind of dystopian speculation arises in large part from thinking about today\u2019s mindless A.I. systems and extrapolating from them. If all you can calculate is statistical correlation, you can\u2019t conceptualize harm. But A.I. systems that know about time, space and causality are the kinds of things that can be programmed to follow more general instructions, such as \u201cA robot may not injure a human being or, through inaction, allow a human being to come to harm\u201d (the first of Isaac Asimov\u2019s three laws of robotics).\n\nWe face a choice. We can stick with today\u2019s approach to A.I. and greatly restrict what the machines are allowed to do (lest we end up with autonomous-vehicle crashes and machines that perpetuate bias rather than reduce it). Or we can shift our approach to A.I. in the hope of developing machines that have a rich enough conceptual understanding of the world that we need not fear their operation. Anything else would be too risky.\n\nGary Marcus (@GaryMarcus), the founder and chief executive of Robust AI, and Ernest Davis, a professor of computer science at New York University, are the authors of the forthcoming book \u201cRebooting AI: Building Artificial Intelligence We Can Trust,\u201d from which this essay is adapted.\n\nThe Times is committed to publishing a diversity of letters to the editor. We\u2019d like to hear what you think about this or any of our articles. Here are some tips. And here\u2019s our email: letters@nytimes.com.\n\nFollow The New York Times Opinion section on Facebook, Twitter (@NYTopinion) and Instagram.", "description": "Computer systems need to understand time, space and causality. Right now they don\u2019t.", "authors": ["Gary Marcus", "Ernest Davis"], "top_image": "https://static01.nyt.com/images/2019/09/06/opinion/06MarcusDavis/06MarcusDavis-facebookJumbo.jpg", "published_at": "2019-09-06"}