{"pub": "bbc", "url": "https://bbc.com/news/technology-50045919", "downloaded_at": "2019-10-15 12:54:13.798969+00:00", "title": "YouTube regrets: Anecdotal claims of damaged users", "language": "en", "text": "Image copyright Mozilla Image caption Mozilla's not-for-profit advocacy arm campaigns for more responsible use of recommendation algorithms\n\n\"My 10-year-old sweet daughter innocently searched for 'tap dance videos',\" one parent wrote.\n\n\"Now she is in this spiral of... videos that give her horrible unsafe body-harming and body-image-damaging advice.\"\n\nThis is one of hundreds of accounts outlining damage said to have been caused by YouTube's recommendations algorithm.\n\nIt's a phenomenon some refer to as \"falling down the YouTube rabbit hole\" with users directed to controversial and potentially dangerous content they might never have stumbled on otherwise.\n\nThe accounts have been gathered by Mozilla, the organisation best known for its Firefox web browser, which competes against Google's Chrome. The BBC was unable to corroborate the posts, as the foundation said they had been collected anonymously.\n\nIt's impossible to know if all the details are true. But Mozilla says it has shared a representative sample of the messages it received. And some read like horror stories.\n\n\"She is now restricting her eating and drinking,\" the parent continued.\n\n\"I heard her downstairs saying, 'Work to eat. Work to drink.'\n\n\"I don't know how I can undo the damage that's been done to her impressionable mind.\"\n\nImage copyright Getty Images Image caption The examples Mozilla collected describe a broad range of harmful content recommended by YouTube's algorithm\n\nWhite supremacists\n\nMozilla asked the public to share their \"YouTube regrets\" - videos recommended to users of the video clip platform, which led them down bizarre or dangerous paths.\n\n\"The hundreds of responses we received were frightening: users routinely report being recommended racism, conspiracies, and violence after watching innocuous content,\" said Ashley Boyd, Mozilla's vice-president of advocacy.\n\n\"After watching a YouTube video about Vikings, one user was recommended content about white supremacy.\n\n\"Another user who watched confidence-building videos by a drag queen was then inundated by clips of homophobic rants.\"\n\nImage copyright Mozilla Image caption Some contributors described how family members had been taken in by videos about UFOs, the Illuminati and government conspiracies\n\nYouTube is the second most visited website in the world. Its recommendation engine drives 70% of total viewing time on the site, by tailoring suggestions to keep viewers watching.\n\nIts owner Google has yet to comment on Mozilla's report.\n\nBut managers have previously denied suggestions that their algorithms deliberately promote extremist or harmful content because it boosts watch-time or benefits the business in some other way.\n\nAnd they have added that YouTube has begun tackling videos that contain misinformation and conspiracy theories by showing \"warning labels\" and \"knowledge panels\" containing trustworthy information.\n\nEven so, claims that its recommendations have a tendency to lead users astray persist.\n\n\"We urge YouTube and all platforms to act with integrity, to listen to stories and experiences of users,\" said Lauren Seager-Smith, chief executive of children's protection charity Kidscape, which is not involved in Mozilla's campaign.\n\n\"[It needs] to reflect on when content may have caused harm - however inadvertently - and to prioritise system change that improves protection of children and those most at risk.\"\n\nFear and hate\n\nMozilla said it received more than 2,000 responses in five languages to its call.\n\nIt has published 28 of the anecdotes.\n\n\"My ex-wife, who has mental health problems, started watching conspiracy videos three years ago and believed every single one,\" recalled one contributor.\n\n\"YouTube just kept feeding her paranoia, fear and anxiety, one video after another.\"\n\nImage copyright Getty Images Image caption Contributors from the LGBT community voiced concerns about being recommended homophobic content\n\nMembers of the LGBT community also raised concerns.\n\n\"In coming out to myself and close friends as transgender, my biggest regret was turning to YouTube to hear the stories of other trans and queer people,\" one person wrote.\n\n\"Simply typing in the word 'transgender' brought up countless videos that were essentially describing my struggle as a mental illness and as something that shouldn't exist. YouTube reminded me why I hid in the closet for so many years.\"\n\nThe LGBT Foundation - a Manchester-based charity - called for YouTube and other social media companies to take more responsibility for the content promoted by their algorithms.\n\n\"Hateful content online is on the rise, and something that is of increasing concern,\" the foundation's Emma Meehan told the BBC.\n\n\"Social media giants have a responsibility for what is shared on their platforms and the real-world impact this may have, and need to work to take a more dedicated approach to combating hate online.\"\n\nResearch challenges\n\nYouTube's recommendations system poses difficulties for researchers outside the company as the business does not share its own recommendations data.\n\nSince each user is given different suggestions, it is hard to determine why some choices are made and how many others have had the same content promoted to them.\n\n\"By sharing these stories, we hope to increase pressure on YouTube to empower independent researchers and address its recommendation problem,\" Mozilla's Ashley Boyd said.\n\n\"While users should be able to view and publish the content they like, YouTube's algorithm shouldn't actively be pushing harmful content into the mainstream.\"\n\nHave recommendations by an algorithm confronted you with false or harmful content? You can get in touch by emailing haveyoursay@bbc.co.uk.\n\nPlease include a contact number if you are willing to speak to a BBC journalist. You can also contact us in the following ways:", "description": "Fresh evidence that Google's video clip service is promoting harmful and hate-filled content.", "authors": ["Flora Carmichael", "Bbc World Service"], "top_image": "https://ichef.bbci.co.uk/news/1024/branded_news/124B/production/_109238640_e9338a43-135f-4b97-82be-56c3c6e4379f.jpg", "published_at": "2019-10-15"}