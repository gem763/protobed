{"pub": "cointelegraph", "url": "https://magazine.cointelegraph.com/2019/08/30/leave-the-guitar-smash-the-laptop-the-story-of-music-making-ai", "downloaded_at": "2019-10-05 13:10:53.009472+00:00", "title": "Leave the Guitar, Smash the Laptop: The Story of Music-Making AI \u2013 Magazine by Cointelegraph", "language": "en", "text": "Ten years ago people asked: What can artificial intelligence even do? Today, the better question might be: What can\u2019t AI do? The answer to that may be surprising. Even though AI has excelled at difficult tasks \u2014 e.g., beating Chinese champion Ke Jie at the game of go \u2014 there are a few skills that not even the smartest computer has yet been able to master. This is the story of AI and music. A review of technology\u2019s influence on what we listen to, a critical look at AI\u2019s current role in music production, and a prediction for the future. The year is 2066, do we have an AI Hendrix?\n\nA brief history of technology changing music\n\nOn a fine July evening in the year of 1965, Bob Dylan brought to the stage not his usual acoustic guitar, but a Fender Stratocaster. The fans, it can be said with only mild exaggeration, went nuts. People cried, booed and cheered as the face of folk music plugged in and electrified the crowd. In a single night, one man broke with tradition and introduced a new style of playing music. Several decades later, autotune was introduced.\n\nCher, a singer of international note, used autotune on her 1998 song, \u201cBelieve.\u201d In that song, we hear for the first time the musical style that would become synonymous with pop in the 21st century. For better or for worse, wires and transistors had again reshaped the sound of music. Not everyone was pleased.\n\nBut there is one particularly sinister invention that has been putting extra shine on pop vocals since the 1990s. It is a piece of software called Autotune. Essentially, it takes a poorly sung note and transposes it, placing it dead centre of where it was meant to be. Neil McCormick, Music Critic\n\nOne can feel his disdain for any person unrefined enough to consider autotune a good idea. What happened to natural talent? Where did the real musicians go? Neil may wish he was born half a century earlier, until a band called BTS was invented.\n\nYet, here we are \u2014 so accustomed to technologically enhanced music that nobody gives it a second thought. Entire songs are produced on computers, voices corrected so that they\u2019re unrecognizable, shower singers shot to stardom in the age of filters and synthesizers. Computers have been sitting co-pilot in music production since the turn of the century, so is it surprising that the next logical step is for AI to try its hand at flying plane?\n\nHow AI is making music today, sort of\u2026\n\nTechnophobes ought not to lose sleep over the state of the AI-generated music industry. It\u2019s young, plagued with issues and really only works when humans have a heavy-handed final edit. Even the best artificial intelligence systems are not yet capable of creating full length, catchy songs that people actually want to listen to. Yet, within its domain of expertise, AI is doing some pretty cool stuff.\n\nAmper is a company at the forefront of AI\u2019s bid for dominance in the musical arts. Founded in 2014, its sale\u2019s pitch was simple: Use our program to help you create music. You choose the genre, length of the song and any number of other small details. Our AI will compose a completely original piece of music for your use. Amper\u2019s production quality was nothing to turn up the radio for, but in a limited number of circumstances, it proved beneficial.\n\nA singer who does not know how to compose a melody, for instance, could use Amper to create a background track. She could tweak a number of variables to get the sound she desired, maybe send it to a sound engineer for the final tuning, and then release a song without ever having worked with a band.\n\nIn this example Amper\u2019s artificial intelligence really did compose something, a completely original computer creation. Yet, almost never is that product good enough to stand on its own. It only works because of the heavy editing and creative input that happens afterward. Amper fizzled. It was used here and there \u2014 notably in one case, which we\u2019ll discuss later \u2014 but as a whole, it did not offer a groundbreaking solution that bands were clamoring to use. Then the company pivoted\u2026\n\nThe commercial use of music-making AI\n\nAmper Score, an enterprise program designed to, among other things, produce background music for commercials. Featuring a foolproof design, a boardroom suit just has to pick a genre of music and the mood that he would like the song to be composed in. Then, Amper Score builds the track from scratch. The great composer listens, changes the tempo, swaps the instruments, speeds it up or slows it down. Then, it\u2019s off to production, ready to be incorporated into whatever is being produced that day. In this context, Amper proves invaluable, as it allows people with no musical instinct to produce entire tracks. Here\u2019s how it works.\n\nTo create a song, Amper looks at a body of music within a certain genre and takes note of which instruments are used, the tempo, melodic patterns, commonly used keys and so forth. Amper\u2019s creators even described the program as being able to read the mood of a piece. Whether that\u2019s true or it\u2019s just marketing speak, Amper analyzes the data and uses patterns to create a new song, similar to others in the genre. Boom, presto, creation!\n\nMusic without the hassle seems to be a good selling point, if Amper\u2019s $4 million seed funding in 2018 is any indication. While Amper Score is only available at an enterprise level, before the company cut off its individual users, there was one person in particular who took advantage of it.\n\nThose who compose with computers\n\nSinger and songwriter Taryn Southern can\u2019t play the piano \u2014 at least, not well enough to produce a rich melody. Traditionally she might have partnered with a musician to help her make an album, but Southern felt that perhaps tradition wasn\u2019t the best choice. She chose instead to work with Amper in order to create the instrumentals for her song, \u201cBreak Free.\u201d The music-making AI did all of the work, crafting chords and harmonies, and then Southern tweaked it to her taste. The result was a perfectly adequate, if somewhat flavorless, background track.\n\nWhen asked about the experience of working with an AI-music program, Southern responded, \u201cI find that really fun, and because I\u2019m able to iterate with the music and give it feedback and parameters and edit as many times as I need, it still feels like it\u2019s mine in a sense.\u201d She went on to claim that Amper made the process of writing a song 20 times faster. Presumably, for those well-versed in music theory, the program might not be nearly as useful, but for someone like Taryn, it was a great solution. So great, in fact, that she decided to employ several different music-making AIs to compose every background track on her entire album.\n\nMaking use of not just Amper (before the company pivoted to corporate only), she also worked with Google\u2019s Magenta and IBM\u2019s Watson Beat. Reflecting on whether she could properly claim credit for songs that were in no small part made possible by artificial intelligence, Southern stated, \u201cBecause I\u2019m able to iterate with the music and give it feedback and parameters and edit as many times as I need, it still feels like it\u2019s mine in a sense.\u201d\n\nThat the music is its own, despite an AI aiding and abetting, is a claim Yacht would also be likely to make. The Los Angeles-based dance-pop group has been around since 2002, but that didn\u2019t stop it from experimenting with decidedly newer technology. \u201c(Downtown) Dancing,\u201d the lead single on the band\u2019s new album, Chain Tripping, is largely the work of AI. While the group retained some creative control over the final product, a so-called \u201cneural network\u201d crafted the melodies, wrote the lyrics and contributed in a dozen other small ways to the production. Here\u2019s how.\n\nFirst, the band took 82 of its past songs and chopped them up into short segments that could be read by a computer. They fed those into Magenta\u2019s MusicVAE. which analyzed Yacht\u2019s samples and, based on their unique sound, created an entirely new melody. The band then took those suggestions from the AI and put them together to create a coherent sound \u2014 as a full composition was beyond the means of the computer.\n\nClaire L. Evans, Yacht\u2019s lead singer, confirmed this when she told a reporter from Spin magazine, \u201cThere\u2019s a lot of tools that can help you generate fragmentary things like melodies or text, but we\u2019re not quite at a point now with machine learning where it can help us make structured pop songs.\u201d\n\nIn another interview, this time with music blog Brooklyn Vegan, she went on to say that the benefit of working with AI wasn\u2019t the sound, but it was the interactions that it fostered. \u201cIt\u2019s not so much about collaborating with the machine as it is about collaborating with people through the medium of the machine. Creating this kind of work requires that creative and technical people work together, and that\u2019s a good thing in our increasingly siloed culture.\u201d\n\nInterestingly, although the composition of \u201c(Downtown) Dancing\u201d was heavily influenced by artificial intelligence, there is no dead giveaway that this is the case. It sounds like a normal indie dance song, and people familiar with the group\u2019s work say that it is in keeping with Yacht\u2019s style. The real headlines, in other words, are the headlines themselves, not the actual product.\n\nWhen computers can sing\n\nAlthough Yacht drew the headlines, it\u2019s not the only group experimenting with computer generated vocals. Amnesia Scanner, a Finnish group based in Berlin, has created a virtual vocalist that the band has named \u201cOracle.\u201d The voice is generated by what Amnesia Scanner refers to as, \u201ca stack of software.\u201d Interestingly, the group has gone the opposite direction of other musicians working with AI, and by producing the background tracks and tasking a computer with the vocals, the group has helped to show what the potential of a digitally led music revolution could be.\n\nYet, it\u2019s worth noting that while Oracle sounds like it could refer to the AI from \u201c2001 a Space Odyssey,\u201d it isn\u2019t actually a true AI. Instead, it\u2019s a finely tuned audio program that gives Amnesia Scanner a chance to tweak vocals in a very specific way. This can be done consistently, and the group hopes that in time, people will come to know this trademark sound as one associated with Amnesia Scanner. For those interested, the entire album is available on Soundcloud, including \u201cAs Spectacult,\u201d featuring Oracle.\n\nAsh Koosha is another musician using a computer-powered vocalist. For the last couple of years, he has spent his time creating Yona, an artificial singer. Speaking on the work required to create such a program, Koosha stated in a Dazed Digital interview: \u201cI think Yona will become the ultimate musical computational intelligence or knowledge base. Over the last year or so I have made more than 100 trials, but very recently Yona\u2019s output started sounding like music, and Yona\u2019s voice is shaping in a way that creates emotional impact.\u201d\n\nThe result is an intriguing sound, one that would be difficult to label as artificial on the first listen. It\u2019s not that it sounds natural \u2014 far from it, actually \u2014 but so many singers tune their voices toward the robotic that Yona ends up sounding eerily similar to a heavily distorted pop singer. Later in the interview, Koosha spoke on the mechanics behind the artificial songstress, \u201cYona\u2019s engine is comprised of a series of generative software that generates sentences, melodies and singing via a complex text-to-speech process.\u201d\n\nKoosha claims that although he does the mixing and final edit, the majority of each song is inspired by chords, melodies and lyrics created by Yona. To do so, Yona derives her creativity from Koosha, as the music-making program analyzes a backlog of his music in order to create a new sound. However, as the technology advances, it\u2019s hard to say what Yona could be capable of. Koosha is hoping for the best, saying, \u201cShe performs the music, and as she improves, her unique voice is on the path to becoming a full electronic pop musician.\u201d\n\nAnti-chart topping AI\n\n\u201cDaddy\u2019s Car\u201d was one of the first songs to have ever been created by AI. It was produced in a more primitive time, back when man led a simpler, more natural life: 2016. To create the song, the producers used Flow Machines, a program that\u2019s still around today. While the AI was responsible for crafting the score, as we\u2019ve seen before, it took a talented human \u2014 in this case, the French composer Beno\u00eet Carr\u00e9 \u2014 to bring the pieces together into a melodic whole.\n\nThe song is intended to be stylistically similar to the Beatles, a goal that it accomplishes to some extent. However, the most interesting aspect of this composition is that \u201cDaddy\u2019s Car\u201d doesn\u2019t really sound all that much better or worse than other, more recent AI compositions like \u201cBreak Free\u201d or \u201c(Downtown) Dancing.\u201d Three years in the world of silicon and bits is like a decade in the real world, so for there to be little noticeable progress in the field of music-making AI is a good indication of how slowly the technology is advancing.\n\nWhat we have not seen yet is the killer app \u2014 the use case that only an AI can fulfill. Aside from a few avant-garde artists, the music-making AI of today is most widely used to aid in the creation of background tracks. For that small subset of the population that has a use for this, music-making AI could be a godsend. It can reduce costs and speed up production times. That\u2019s fine and good, but the AI is not doing something a human can\u2019t do. It\u2019s not creating a song completely unique to AI, a sound that is impossible to get otherwise.\n\nIn researching this article, what became clear is that music-making AI is not on a significant growth curve. Look at the number of people who own smartphones, the number of Bitcoin wallets or the growth in Instagram users. These are technologies that, when viewed from an annual perspective, are clearly growing. When we look at music-making AI, there appears to be little progress. Every year, a couple of bands experiment with it, some articles get written and concert tickets are sold, but nothing phenomenal is taking shape. There is, at this point, a lack of brilliance in the field of computer-generated music.\n\nWelcome to the world of tomorrow\n\nHaving said that, there are some people who believe that regardless of what\u2019s happening today, AI\u2019s future influence is going to be tremendous. \u201cIt may be as big a deal as the original shift from people making music primarily with their own bodies to crafting instruments that had their own unique properties and sounds,\u201dcommunications manager at Google Jason Freidenfelds predicted, believing that AI\u2019s impact in music will exceed any one technological advancement.\n\nIf he\u2019s right, music-making artificial intelligence may move music in a direction that no human could have conceived of. As autotune is ubiquitous today, so may AI\u2019s influence on music be taken for granted in the coming decades. The results could be surprisingly beautiful, an unexpected combination of digital and analog giving us music greater than the sum of its parts.\n\nBut it could turn out differently\u2026 Maybe, just maybe, music will prove to be a unicorn, the lost city of Atlantis, the ineffable art that humans remain outdone by emotionless thinking machines. AI has conquered chess and poker, cancer diagnosis and insurance claims settlement. There is no human at the controls when SpaceX\u2019s rockets touch down. Every year, AI takes on more. Yet, if there is any hope for humanity to hold its edge, that hope lies in music. Emotion-driven, soul-gripping music. It could be that in 20 years, people will tuck themselves into their robot taxis, turn on their earphone implants and listen to something a human made \u2014 a soulful sound no computer can replicate.\n\n", "description": "", "authors": ["Sam Has Been Reading Thick Books", "Writing Interesting Words For The Last Ten Years. He Thinks Crypto Is The Coolest Thing Since Streaming Video", "Ethereum Is His Favorite Project. Currently He'S Surfing In South East Asia.", "Min Read"], "top_image": "https://magazine.cointelegraph.com/files/2019/08/Music-Making-AI-1024x576.jpg", "published_at": "2019-08-30"}