{"pub": "theverge", "url": "https://theverge.com/2019/10/25/20931657/google-bert-search-context-algorithm-change-10-percent-langauge", "downloaded_at": "2019-10-25 16:00:43.784163+00:00", "title": "Google is improving 10 percent of searches by understanding language context", "language": "en", "text": "Google is currently rolling out a change to its core search algorithm that it says could change the rankings of results for as many as one in ten queries. It\u2019s based on cutting-edge natural language processing (NLP) techniques developed by Google researchers and applied to its search product over the course of the past 10 months.\n\nIn essence, Google is claiming that it is improving results by having a better understanding of how words relate to each other in a sentence. In one example Google discussed at a briefing with journalists yesterday, its search algorithm was able to parse the meaning of the following phrase: \u201cCan you get medicine for someone pharmacy?\u201d\n\nThe old Google search algorithm treated that sentence as a \u201cbag of words,\u201d according to Pandu Nayak, Google fellow and VP of search. So it looked at the important words, medicine and pharmacy, and simply returned local results. The new algorithm was able to understand the context of the words \u201cfor someone\u201d to realize it was a question about whether you could pick up somebody else\u2019s prescription \u2014 and it returned the right results.\n\nBefore, Google treated queries like \u201ca bag of words\u201d\n\nThe tweaked algorithm is based on BERT, which stands for \u201cBidirectional Encoder Representations from Transformers.\u201d Every word of that acronym is a term of art in NLP, but the gist is that instead of treating a sentence like a bag of words, BERT looks at all the words in the sentence as a whole. Doing so allows it to realize that the words \u201cfor someone\u201d shouldn\u2019t be thrown away, but rather are essential to the meaning of the sentence.\n\nThe way BERT recognizes that it should pay attention to those words is basically by self-learning on a titanic game of Mad Libs. Google takes a corpus of English sentences and randomly removes 15 percent of the words, then BERT is set to the task of figuring out what those words ought to be. Over time, that kind of training turns out to be remarkably effective at making a NLP model \u201cunderstand\u201d context, according to Jeff Dean, Google senior fellow & SVP of research.\n\nAnother example Google cited was \u201cparking on a hill with no curb.\u201d The word \u201cno\u201d is essential to this query, and prior to implementing BERT in search Google\u2019s algorithms missed that.\n\nGoogle says that it has been rolling the algorithm change out for the past couple of days and that, again, it should affect about 10 percent of search queries made in English in the US. Other languages and countries will be addressed later.\n\nAll changes to search are run through a series of tests to ensure they\u2019re actually improving results. One of those tests involves using Google\u2019s cadre of human reviewers who train the company\u2019s algorithms by rating the quality of search results \u2014 Google also conducts live live A/B tests.\n\nNot every single query will be affected by BERT, it\u2019s just the latest of many different tools Google uses to rank search results. How exactly all of it works together is a bit of a mystery. Some of that process is kept intentionally mysterious by Google to keep spammers from gaming its systems. But it\u2019s also mysterious for another important reason: when a computer uses machine learning techniques to make a decision, it can be hard to know why it made those choices.\n\nBERT could affect as many as 10 percent of all Google searches\n\nThat so-called \u201cblack box\u201d of machine learning is a problem because if the results are wrong in some way, it can be hard to diagnose why. Google says that it has worked to ensure that adding BERT to its search algorithm doesn\u2019t increase bias \u2014 a common problem with machine learning whose training models are themselves biased. Since BERT is trained on a giant corpus of English sentences, which are also inherently biased, it\u2019s an issue to keep an eye on.\n\nThe company also says that it doesn\u2019t anticipate significant changes in how much or where its algorithm will direct traffic, at least when it comes to large publishers. Any time Google signals a change in its search algorithm, the entire web sits up and takes notice. Companies have lived and died by Google\u2019s search rank changes.\n\nEverybody who makes money on web traffic absolutely should take notice. When it comes to the quality of its search results, Payak says that \u201cthis is the single biggest ... most positive change we\u2019ve had in the last five years and perhaps one of the biggest since the beginning.\u201d", "description": "Say hello to BERT", "authors": ["Dieter Bohn", "Oct"], "top_image": "https://cdn.vox-cdn.com/thumbor/ur79HGb9ir4zq0hf6rrbqClBA0s=/0x146:2040x1214/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/10801249/acastro_180508_1777_google_IO_0002.jpg", "published_at": "2019-10-25"}