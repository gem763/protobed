{"pub": "dailymail", "url": "https://dailymail.co.uk/news/article-7561145/Home-Office-knew-passport-checker-struggle-dark-skin.html", "downloaded_at": "2019-10-11 02:28:45.110936+00:00", "title": "Home Office knew passport checker would struggle with dark skin", "language": "en", "text": "The Home Office launched a system for checking passport photos despite knowing it would struggle with dark skin, it has emerged.\n\nThe technology used in the online checking system was known to have difficulty with very light or very dark skin tones, but officials decided it worked well enough.\n\nThe issue recently made headlines after Joshua Bada, a black man from west London, was told by the facial recognition technology that his lips looked like an open mouth.\n\nMr Bada, 28, was forced to explain: 'My mouth is closed, I just have big lips.'\n\nJoshua Bada revealed that the automated photo checker on the Government's passport renewal website mistook his lips for an open mouth\n\nA Freedom of Information request reported by New Scientist has since shown that the Home Office knew the facial recognition technology would fail for some ethnic groups.\n\nThe issue arose when testing was carried out before the system went live in 2016, but officials decided it worked well enough to be deployed.\n\nThe automated facial detection system informs people when it thinks the photo uploaded may not meet strict requirements.\n\nThese include a plain expression and a closed mouth, though users can override the outcome if they believe the system is wrong.\n\n'User research was carried out with a wide range of ethnic groups and did identify that people with very light or very dark skin found it difficult to provide an acceptable passport photograph, however, the overall performance was judged sufficient to deploy,' the department said in its response to the request, submitted by Sam Smith, of campaign group MedConfidential.\n\n'We are constantly gathering customer feedback and carrying out further user testing to enable us to work alongside our supplier to keep refining the algorithm.'\n\nEducational technologist Cat Hallam was told by the system that it looked like her eyes were closed and that it could not find the outline of her head\n\nIn another case, Cat Hallam, an educational technologist from Staffordshire, was told that it appeared her eyes were closed.\n\nShe said she does not believe it amounts to racism, but thinks it is a result of algorithmic bias.\n\nA Home Office spokeswoman said: 'We are determined to make the experience of uploading a digital photograph as simple as possible, and will continue working to improve this process for all of our customers.'\n\nExperts believe the problem could be a result of algorithmic bias, meaning the data fed into the system may not have been large or diverse enough.\n\nNoel Sharkey, professor of artificial intelligence and robotics at the University of Sheffield, said the Home Office should 'hang its head in shame'.\n\n'Inequality, inequality, inequality needs to be stamped on when it raises its ugly head,' he said.", "description": "The issue recently made headlines after Joshua Bada, a black man from west London, was told by the facial recognition technology that his lips looked like an open mouth (pictured).", "authors": ["Tim Stickings For Mailonline"], "top_image": "https://i.dailymail.co.uk/1s/2019/10/11/01/19536848-0-Joshua_Bada_revealed_to_the_PA_news_agency_that_the_automated_ph-a-4_1570754998269.jpg", "published_at": "2019-10-11"}