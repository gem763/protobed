{"pub": "axios", "url": "https://axios.com/algorithmic-transparency-ai-takano-hud-a63c2d65-3c69-4ba2-9d17-538ac817d849.html", "downloaded_at": "2019-10-05 22:51:28.034223+00:00", "title": "Algorithmic transparency becomes a flashpoint", "language": "en", "text": "Why it matters: The systems are so complex that it can be hard to know how they arrive at answers \u2014 and so valuable that their creators often try to restrict access to their inner workings, making it potentially impossible to challenge their consequential results.\n\nDriving the news: Two recent proposals are pulling in opposite directions.\n\nA bill from Rep. Mark Takano, a California Democrat, would block companies that design AI systems for criminal justice from withholding details about their algorithms by claiming they\u2019re trade secrets.\n\nA proposal from the Department of Housing and Urban Development (HUD) would protect landlords, lenders and insurers that want to use algorithms for important determinations, shielding them from claims that the algorithms unintentionally have a more negative impact on certain groups of people.\n\nThese are among the earliest attempts to set down rules and definitions for algorithmic transparency. How they shake out could set rough precedents for how the government will approach the many future questions that will emerge.\n\nProponents of more access say it's vital to test whether walled-off systems are making serious mistakes or unfair determinations \u2014 and argue that the potential for harm should outweigh companies' interest in protecting their secrets.\n\nDevelopers regularly invoke trade-secret rights to keep their algorithms \u2014 used for key evidence like DNA matches or bullet traces \u2014 away from the accused, says Rebecca Wexler, a UC Berkeley law professor who consulted on Takano's bill.\n\n\"We need to give defendants the rights to get the source code and [not] allow intellectual property rights to be able to trump due process rights,\" Takano tells Axios. His bill also asks the government to set standards for forensic algorithms and test every program before it is used.\n\nThe HUD proposal would require someone to show that an algorithmic decision was based on an illegal proxy, like race or gender, in order to succeed in a lawsuit. But critics say that can be impossible to determine without understanding the system.\n\n\"By creating a safe harbor around algorithms that do not use protected class variables or close proxies, the rule would set a precedent that both permits the proliferation of biased algorithms and hampers efforts to correct for algorithmic bias,\" says Alice Xiang, a researcher at the Partnership on AI.\n\nHUD is soliciting comments on the proposal until later this month.\n\nThe other side: \"The goal here is to bring more certainty into this area of the law,\" said HUD General Counsel Paul Compton in an August press conference. He said the proposal \"frees up parties to innovate, take risks and meet the needs of their customers without the fear that their efforts will be second-guessed through statistics years down the line.\"", "description": "At issue: How much access people should have to AI systems that make critical decisions about them.", "authors": [], "top_image": "https://images.axios.com/pC8sBUSTuAwBtFZEGxaK_pGrXB4=/0x0:1920x1080/1920x1080/2019/10/05/1570237329559.jpg", "published_at": "2019-10-05"}