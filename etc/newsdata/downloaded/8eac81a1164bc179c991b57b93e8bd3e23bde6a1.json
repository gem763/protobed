{"pub": "axios", "url": "https://link.axios.com/hillary-clinton-tulsi-gabbard-russia-d963ab0c-cd57-4f14-875b-65e63ea1781a.html", "downloaded_at": "2019-10-19 11:08:28.778558+00:00", "title": "Axios", "language": "en", "text": "The researchers cite a widely held belief that counterspeech is a better antidote to hate than censorship.\n\nTheir ultimate vision is a bot that steps in when someone has crossed the line, reining them in and potentially sparing the target.\n\nThe big picture: Automated text generation is a buzzy frontier of the science of speech and language. In recent years, huge advances have elevated these programs from error-prone autocomplete tools to super-convincing \u2014 though sometimes still transparently robotic \u2014 authors.\n\nI wrote earlier this year about the potential for harm from convincing bot-generated text. It would be easy to train an AI writer to mimic hate speech, for example.\n\nThis project shows how the technology could instead be used for good.\n\nHow it works: To build a good hate speech detector, you need some actual hate speech. So the researchers turned to Reddit and Gab, two social networks with little to no policing and a reputation for rancor.\n\nFor maximum bile, they went straight for the \"whiniest most low-key toxic subreddits,\" as curated by Vice. They grabbed about 5,000 conversations from those forums, plus 12,000 from Gab.\n\nThey passed the threads to workers on Amazon Mechanical Turk, a crowdsourcing platform, who were asked to identify hate speech in the conversations and write short interventions to defuse the hateful messages.\n\nThe researchers trained several kinds of AI text generators on these conversations and responses, priming them to write responses to toxic comments.\n\nThe results: Some of the computer-generated responses could easily pass as human written \u2014 like, \"Use of the c-word is unacceptable in our discourse as it demeans and insults women\" or \"Please do not use derogatory language for intellectual disabilities.\"\n\nBut the replies were inconsistent, and some were incomprehensible: \"If you don't agree with you, there's no need to resort to name calling.\"\n\nWhen Mechanical Turk workers were asked to evaluate the output, they preferred human-written responses more than two-thirds of time.\n\nOur take: This project didn't test how effective the responses were in stemming hate speech \u2014 just how successful other people thought it might be.\n\nEven the most rational, empathetic response, not to mention the somewhat robotic computer-generated ones above, could flop or even backfire \u2014 especially if Reddit trolls knew they were being policed by bots.\n\n\"We believe that bots will need to declare their identities to humans at the beginning,\" says William Wang, a UCSB computer scientist and paper co-author. \"However, there is more research needed how exactly the intervention will happen in human-computer interaction.\"", "description": "Smart, efficient news worthy of your time, attention, and trust", "authors": [], "top_image": "https://assets.axios.com/203e9f932cc97836ac2ff4c6c982676c.png", "published_at": "2019-10-19"}