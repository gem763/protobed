{"pub": "axios", "url": "https://axios.com/misinformation-disinformation-human-actors-e490d5ee-4480-44a8-8922-e724cd852acb.html", "downloaded_at": "2019-10-17 14:36:03.217485+00:00", "title": "Human actors are changing the spread of disinformation", "language": "en", "text": "Why it matters: Understanding this changing nature is critical to preparing for the next generation of information threats, including those facing the 2020 presidential campaign.\n\n1Speaking at Stanford University Tuesday, researcher Kate Starbird \u2014 a University of Washington professor who runs a lab that studies mass participation \u2014 traced the change across the stories of three different campaigns.\n\n1. Russian interference in the 2016 election: Starbird's work started not with studying disinformation, but with an analysis of the debate that raged on Twitter over the Black Lives Matters movement.\n\nIt was only after Twitter released data on Russian propagandists in November 2017 that her team realized that some of the most prolific posters \u2014 on both sides of the debate \u2014 were fictional personas created by the Russians.\n\n\"In a few cases, we can see them arguing with themselves,\" said Starbird.\n\n2. Syria's \"White Helmets\": In this case, an aid group known as the White Helmets working in Syria was attacked by online critics for a host of alleged atrocities.\n\nHere Russia was actively involved in stirring the pot, but the posters themselves were neither bots nor trolls, but activists who adopted the issue as their own.\n\n\"These are real people who are sincere believers of the content they are sharing,\" Starbird said.\n\nRussian media, including Sputnik and RT, made the movement appear significantly larger, though, by interviewing activists and giving them both a platform and a veneer of legitimacy.\n\n3. Conspiracy theories tied to mass casualty events: People are predisposed to find conspiracies in every tragedy, and conspiracy theories have accompanied all manner of mass-casualty events such as the Boston Marathon bombing and Sandy Hook shooting.\n\nThe theories crop up organically, though Russian or other disinformation promoters can and do help amplify the messages.\n\nTerms like \"false flag\" and \"crisis actors\" are applied to the victims, flipping the script of whatever has transpired.\n\n\"It's almost like a self-sustaining community, but you can see it's been shaped by disinformation campaigns of the past,\" Starbird said.\n\nAll these factors, she said, makes these cases the \"most frightening\" she's studied.\n\nBetween the lines: Not all the disinformation has come from Russia, Starbird said, but added: \"They have been innovators in this space.\"\n\nWhat's next: Starbird recommended a couple of actions for the tech companies.\n\nFirst, she urged them to look at entire campaigns, rather than focusing on the veracity of individual posts. While Twitter and Facebook tend to look at posts in isolation, the creators of disinformation are focused on an overall campaign, a set of narratives with a larger point, she argues.\n\nStarbird also suggests the tech companies discount false claims of conservative bias that, she suggests, are being leveled by the disinformation's beneficiaries.\n\n\"The people that have benefited are now in power in a lot of places,\" she said. \"Anything the companies do to take a chunk [of their power away] is going to be called bias.\"\n\nMeanwhile: Many of the next disinformation threats may be domestic, notes former Facebook security chief Alex Stamos, who now teaches at Stanford. And those will be harder for law enforcement to investigate given that in many cases there is no law being broken.\n\nGo deeper: Read more from Axios' Misinformation Age series", "description": "It's switching from employees to volunteers.", "authors": [], "top_image": "https://images.axios.com/hZFX1iu9hmmpzYOu-cSJN-CQ79E=/0x0:1920x1080/1920x1080/2019/10/17/1571320162506.jpg", "published_at": "2019-10-17"}