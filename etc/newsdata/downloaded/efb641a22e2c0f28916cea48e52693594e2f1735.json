{"pub": "theverge", "url": "https://theverge.com/2019/10/30/20939147/deepmind-google-alphastar-starcraft-2-research-grandmaster-level", "downloaded_at": "2019-10-30 23:19:34.684234+00:00", "title": "DeepMind\u2019s StarCraft 2 AI is now better than 99.8 percent of all human players", "language": "en", "text": "DeepMind today announced a new milestone for its artificial intelligence agents trained to play the Blizzard Entertainment game StarCraft II. The Google-owned AI lab\u2019s more sophisticated software, still called AlphaStar, is now grandmaster level in the real-time strategy game, capable of besting 99.8 percent of all human players in competition. The findings are to be published in a research paper in the scientific journal Nature.\n\nNot only that, but DeepMind says it also evened the playing field when testing the new and improved AlphaStar against human opponents who opted into online competitions this past summer. For one, it trained AlphaStar to use all three of the game\u2019s playable races, adding to the complexity of the game at the upper echelons of pro play. It also limited AlphaStar to only viewing the portion of the map a human would see and restricted the number of mouse clicks it could register to 22 non-duplicated actions every five seconds of play, to align it with standard human movement.\n\nAlphaStar is the first ever AI grandmaster in Starcraft II\n\nStill, the AI was capable of achieving grandmaster level, the highest possible online competitive ranking, and marks the first ever system to do so in StarCraft II. DeepMind sees the advancement as more proof that general-purpose reinforcement learning, which is the machine learning technique underpinning the training of AlphaStar, may one day be used to train self-learning robots, self-driving cars, and create more advanced image and object recognition systems.\n\n\u201cThe history of progress in artificial intelligence has been marked by milestone achievements in games. Ever since computers cracked Go, chess and poker, StarCraft has emerged by consensus as the next grand challenge,\u201d said David Silver, a DeepMind principle research scientist on the AlphaStar team, in a statement. \u201cThe game\u2019s complexity is much greater than chess, because players control hundreds of units; more complex than Go, because there are 10^26 possible choices for every move; and players have less information about their opponents than in poker.\u201d\n\nBack in January, DeepMind announced that its AlphaStar system was able to best top pro players 10 matches in a row during a prerecorded session, but it lost to pro player Grzegorz \u201cMaNa\u201d Komincz in a final match streamed live online. The company kept improving the system between January and June, when it said it would start accepting invites to play the best human players from around the world. The ensuing matches took place in July and August, DeepMind says.\n\nThe results were stunning: AlphaStar had become among the most sophisticated Starcraft II players on the planet, but remarkably still not quite superhuman. There are roughly 0.2 percent of players capable of defeating it, but it is largely considered only a matter of time before the system improves enough to crush any human opponent.\n\nThis research milestone closely aligns with a similar one from San Francisco-based AI research company OpenAI, which has been training AI agents using reinforcement learning to play the sophisticated five-on-five multiplayer game Dota 2. Back in April, the most sophisticated version of the OpenAI Five software, as it\u2019s called, bested the world champion Dota 2 team after only narrowly losing to two less capable e-sports teams the previous summer. The leap in OpenAI Five\u2019s capabilities mirrors that of AlphaStar\u2019s, and both are strong examples of how this approach to AI can produce unprecedented levels of game-playing ability.\n\nSimilar to OpenAI\u2019s Dota 2 bots and other game-playing agents, the goal with this type of AI research is not just to crush humans in various games just to prove it can be done. Instead, it\u2019s to prove that \u2014 with enough time, effort, and resources \u2014 sophisticated AI software can best humans at virtually any competitive cognitive challenge, be it a board game or a modern video game. It\u2019s also to show the benefits of reinforcement learning, a special brand of machine learning that\u2019s seen massive success in the last few years when combined with huge amounts of computing power and training methods like virtual simulation.\n\nLike OpenAI, DeepMind trains its AI agents against versions of themselves and at an accelerated pace, so that the agents can clock hundreds of years of play time in the span of a few months. That has allowed this type of software to stand on equal footing with some of the most talented human players of Go and, now, much more sophisticated games like Starcraft and Dota.\n\nThis type of AI may one day control smarter, safer, self-learning robots\n\nYet the software is still restricted to the narrow discipline it\u2019s designed to tackle. The Go-playing agent cannot play Dota, and vice versa. (DeepMind did let a more general-purpose version of its Go-playing agent try its hand in chess, which it mastered in a matter of eight hours.) That\u2019s because the software isn\u2019t programmed with easy-to-replace rule sets or directions. Instead, DeepMind and other research institutions use reinforcement learning to let the agents figure out how to play on their own, which is why the software often develops novel and wildly unpredictable play styles that have since been adopted by top human players.\n\n\u201cAlphaStar is an intriguing and unorthodox player \u2014 one with the reflexes and speed of the best pros but strategies and a style that are entirely its own. The way AlphaStar was trained, with agents competing against each other in a league, has resulted in gameplay that\u2019s unimaginably unusual; it really makes you question how much of StarCraft\u2019s diverse possibilities pro players have really explored,\u201d Diego \u201cKelazhur\u201d Schwimer, a pro player for team Panda Global, said in a statement. \u201cThough some of AlphaStar\u2019s strategies may at first seem strange, I can\u2019t help but wonder if combining all the different play styles it demonstrated could actually be the best way to play the game.\u201d\n\nDeepMind hopes advances in reinforcement learning achieved by its lab and fellow AI researchers may be more widely applicable at some point in the future. The most likely real-world application for such software is robotics, where the same techniques can properly train AI agents how to perform real-world tasks, like the operation of robotic hands, in virtual simulation. Then, after simulating years upon years of motor control, the AI can take the reins of a physical robotic arms, and maybe one day even control full-body robots. But DeepMind also sees increasingly more sophisticated \u2014 and therefore safer \u2014 self-driving cars as another venue for its specific approach to machine learning.\n\nCorrection: A previous version of this article stated DeepMind restricted AlphaStar to 20 actions every five minutes. That is incorrect; the restriction was to 22 non-duplicated actions every five seconds. We regret the error.", "description": "DeepMind today announced a new milestone for its artificial intelligence agents trained to play the Blizzard Entertainment game StarCraft II. The Google-owned AI lab\u2019s more sophisticated software, still called AlphaStar, is now grandmaster level in the real-time strategy game, capable of besting 99.8 percent of all human players in competition. The findings are to be published in a research paper in the scientific journal Nature.", "authors": ["Nick Statt", "Oct"], "top_image": "https://cdn.vox-cdn.com/thumbor/SbJCSfaTRAs9WQVTwMjV4e9zr-8=/0x171:3360x1930/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/19331712/AS_Bnet_Protoss.jpg", "published_at": "2019-10-30"}