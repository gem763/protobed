{"pub": "thenextweb", "url": "https://thenextweb.com/artificial-intelligence/2019/10/10/why-facebooks-ai-guru-isnt-scared-of-killer-robots", "downloaded_at": "2019-10-11 00:16:48.102114+00:00", "title": "Why Facebook\u2019s AI guru isn\u2019t scared of killer robots", "language": "en", "text": "\u201cBut until we have a hint of a beginning of a design, with some visible path towards autonomous AI systems with non-trivial intelligence, we are arguing about the sex of angels.\u201d Yann LeCun.\n\nThere\u2019s yet another online debate raging between world-renowned AI experts. This time it\u2019s the big one: will AI rise up and murder us all? While this isn\u2019t a new topic \u2013 humans have postulated about AI overlords for centuries \u2013 the timing and people involved in this debate make it interesting.\n\nWe\u2019re absolutely in the AI era now, and these dangers are no longer fictional. The architects of intelligence working on AI today could, potentially, be the ones who cause (or protect us from) an actual robot apocalypse. That makes what they have to say about the existential threat their work poses to our species pretty important.\n\nThe debate isn\u2019t about the general idea of killer robots. It\u2019s about instrumental convergence. Stuart Russell, an expert whose resume includes a gig as a professor of computer science at Berkeley and one at UC San Francisco as an adjunct-professor of neurological surgery, explains it by imagining a robot designed to fetch coffee:\n\nIt is trivial to construct a toy MDP [Markov decision process] in which the agent\u2018s only reward comes from fetching the coffee. If, in that MDP, there is another \u201chuman\u201d who has some probability, however small, of switching the agent off, and if the agent has available a button that switches off that human, the agent will necessarily press that button as part of the optimal solution for fetching the coffee. No hatred, no desire for power, no built-in emotions, no built-in survival instinct, nothing except the desire to fetch the coffee successfully. This point cannot be addressed because it\u2019s a simple mathematical observation.\n\nThis is, essentially, Nick Bostrom\u2019s Paperclip Maximizer \u2013 build an AI that makes paperclips and it\u2019ll eventually turn the whole world into a paperclip factory \u2013 but a coffee-fetcher works too.\n\nYann LeCun, Facebook\u2019s AI guru, and the person who sparked the debate by co-writing an article telling everyone to stop worrying about killer robots, responded by laying out five escalating reasons why he disagrees with Stuart:\n\nOnce the robot has brought you coffee, its self-preservation instinct disappears. You can turn it off.\n\nOne would have to be unbelievably stupid to build open-ended objectives in a super-intelligent (and super-powerful) machine without some safeguard terms in the objective.\n\nOne would have to be rather incompetent not to have a mechanism by which new terms in the objective could be added to prevent previously-unforeseen bad behavior. For humans, we have education and laws to shape our objective functions and complement the hardwired terms built into us by evolution.\n\nThe power of even the most super-intelligent machine is limited by physics, and its size and needs make it vulnerable to physical attacks. No need for much intelligence here. A virus is infinitely less intelligent than you, but it can still kill you.\n\nA second machine, designed solely to neutralize an evil super-intelligent machine will win every time, if given similar amounts of computing resources (because specialized machines always beat general ones).\n\nStuart, and others who agree with him, don\u2019t see the problem the same way. They argue that, as with climate change, existential threats can arise from systems not inherently designed to be \u201charmful,\u201d yet proper protocols may have prevented the problem in the first place. This makes sense, but, so does the alternative viewpoint posed on Twitter by NYU\u2019s Gary Marcus:\n\nIn the real world, current-day robots struggle to turn doorknobs, and Teslas driven in \u2018Autopilot\u2019 mode keep rear-ending parked emergency vehicles. It\u2019s as if people in the fourteenth century were worrying about traffic accidents, where good hygiene might have been a whole lot more helpful.\n\nAs with anything in the realm of science, whether we should be worried about existential threats like killer robots or focusing on immediate issues like bias and regulating AI depends on how you frame the question.\n\nHow much time, energy, and other resources do you put into a problem that\u2019s only theoretical and, by many expert estimates, has a very-close-to-zero chance of ever occurring?\n\nRead the entire debate here (huge tip of the hat to Ben Pace for putting it all-together in a single post!).\n\nRead next: Google banned a game about the Hong Kong protests", "description": "", "authors": ["Tristan Greene"], "top_image": "https://img-cdn.tnwcdn.com/image/tnw?filter_last=1&fit=1280%2C640&url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2018%2F04%2Frobots_wall.jpg&signature=4811bf154aef602bf0029023973dcf6b", "published_at": "2019-10-10"}