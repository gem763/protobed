{"pub": "techcrunch", "url": "https://techcrunch.com/2019/10/29/tech-giants-still-not-doing-enough-to-fight-fakes-says-european-commission", "downloaded_at": "2019-10-29 23:57:35.670101+00:00", "title": "Tech giants still not doing enough to fight fakes, says European Commission \u2013 TechCrunch", "language": "en", "text": "It\u2019s a year since the European Commission got a bunch of adtech giants together to spill ink on a voluntary Code of Practice to do something \u2014 albeit, nothing very quantifiable \u2014 as a first step to stop the spread of disinformation online.\n\nIts latest report card on this voluntary effort sums to the platforms could do better.\n\nThe Commission said the same in January. And will doubtless say it again. Unless or until regulators grasp the nettle of online business models that profit by maximizing engagement. As the saying goes, lies fly while the truth comes stumbling after. So attempts to shrink disinformation without fixing the economic incentives to spread BS in the first place are mostly dealing in cosmetic tweaks and optics.\n\nSignatories to the Commission\u2019s EU Code of Practice on Disinformation are: Facebook, Google, Twitter, Mozilla, Microsoft and several trade associations representing online platforms, the advertising industry, and advertisers \u2014 including the Internet Advertising Bureau (IAB) and World Federation of Advertisers (WFA).\n\nIn a press release assessing today\u2019s annual reports, compiled by signatories, the Commission expresses disappointment that no other Internet platforms or advertising companies have signed up since Microsoft joined as a late addition to the Code this year.\n\n\u201cWe commend the commitment of the online platforms to become more transparent about their policies and to establish closer cooperation with researchers, fact-checkers and Member States. However, progress varies a lot between signatories and the reports provide little insight on the actual impact of the self-regulatory measures taken over the past year as well as mechanisms for independent scrutiny,\u201d write commissioners V\u011bra Jourov\u00e1, Julian King, and Mariya Gabriel said in a joint statement. [emphasis ours]\n\n\u201cWhile the 2019 European Parliament elections in May were clearly not free from disinformation, the actions and the monthly reporting ahead of the elections contributed to limiting the space for interference and improving the integrity of services, to disrupting economic incentives for disinformation, and to ensuring greater transparency of political and issue-based advertising. Still, large-scale automated propaganda and disinformation persist and there is more work to be done under all areas of the Code. We cannot accept this as a new normal,\u201d they add.\n\nThe risk, of course, is that the Commission\u2019s limp-wristed code risks rapidly cementing a milky jelly of self-regulation in the fuzzy zone of disinformation as the new normal, as we warned when the Code launched last year.\n\nThe Commission continues to leave the door open (a crack) to doing something platforms can\u2019t (mostly) ignore \u2014 i.e. actual regulation \u2014 saying it\u2019s assessment of the effectiveness of the Code remains ongoing.\n\nBut that\u2019s just a dangled stick. At this transitionary point between outgoing and incoming Commissions, it seems content to stay in a \u2018must do better\u2019 holding pattern. (Or: \u201cIt\u2019s what the Commission says when it has other priorities,\u201d as one source inside the institution put it.)\n\nA comprehensive assessment of how the Code is working is slated as coming in early 2020 \u2014 i.e. after the new Commission has taken up its mandate. So, yes, that\u2019s the sound of the can being kicked a few more months on.\n\nSumming up its main findings from signatories\u2019 self-marked \u2018progress\u2019 reports, the outgoing Commission says they have reported improved transparency between themselves vs a year ago on discussing their respective policies against disinformation.\n\nBut it flags poor progress on implementing commitments to empower consumers and the research community.\n\n\u201cThe provision of data and search tools is still episodic and arbitrary and does not respond to the needs of researchers for independent scrutiny,\u201d it warns.\n\nThis is ironically an issue that one of the signatories, Mozilla, has been an active critic of others over \u2014 including Facebook, whose political ad API it reviewed damningly this year, finding it not fit for purpose and \u201cdesigned in ways that hinders the important work of researchers, who inform the public and policymakers about the nature and consequences of misinformation\u201d. So, er, ouch.\n\nThe Commission is also critical of what it says are \u201csignificant\u201d variations in the scope of actions undertaken by platforms to implement \u201ccommitments\u201d under the Code, noting also differences in implementation of platform policy; cooperation with stakeholders; and sensitivity to electoral contexts persist across Member States; as well as differences in EU-specific metrics provided.\n\nBut given the Code only ever asked for fairly vague action in some pretty broad areas, without prescribing exactly what platforms were committing themselves to doing, nor setting benchmarks for action to be measured against, inconsistency and variety is really what you\u2019d expect. That and the can being kicked down the road.\n\nThe Code did extract one quasi-firm commitment from signatories \u2014 on the issue of bot detection and identification \u2014 by getting platforms to promise to \u201cestablish clear marking systems and rules for bots to ensure their activities cannot be confused with human interactions\u201d.\n\nA year later it\u2019s hard to see clear sign of progress on that goal. Although platforms might argue that what they claim is increased effort toward catching and killing malicious bot accounts before they have a chance to spread any fakes is where most of their sweat is going on that front.\n\nTwitter\u2019s annual report, for instance, talks about what it\u2019s doing to fight \u201cspam and malicious automation strategically and at scale\u201d on its platform \u2014 saying its focus is \u201cincreasingly on proactively identifying problematic accounts and behaviour rather than waiting until we receive a report\u201d; after which it says it aims to \u201cchallenge\u2026 accounts engaging in spammy or manipulative behavior before users are \u200bexposed to \u200bmisleading, inauthentic, or distracting content\u201d.\n\nSo, in other words, if Twitter does this perfectly \u2014 and catches every malicious bot before it has a chance to tweet \u2014 it might plausibly argue that bot labels are redundant. Though it\u2019s clearly not in a position to claim it\u2019s won the spam/malicious bot war yet. Ergo, its users remain at risk of consuming inauthentic tweets that aren\u2019t clearly labeled as such (or even as \u2018potentially suspect\u2019 by Twitter). Presumably because these are the accounts that continue slipping under its bot-detection radar.\n\nThere\u2019s also nothing in Twitter\u2019s report about it labelling even (non-malicious) bot accounts as bots \u2014 for the purpose of preventing accidental confusion (after all satire misinterpreted as truth can also result in disinformation). And this despite the company suggesting a year ago that it was toying with adding contextual labels to bot accounts, at least where it could detect them.\n\nIn the event it\u2019s resisted adding any more badges to accounts. While an internal reform of its verification policy for verified account badges was put on pause last year.\n\nFacebook\u2019s report also only makes a passing mention of bots, under a section sub-headed \u201cspam\u201d \u2014 where it writes circularly: \u201cContent actioned for spam has increased considerably, since we found and took action on more content that goes against our standards.\u201d\n\nIt includes some data-points to back up this claim of more spam squashed \u2014 citing a May 2019 Community Standards Enforcement report \u2014 where it states that in Q4 2018 and Q1 2019 it acted on 1.8 billion pieces of spam in each of the quarters vs 737 million in Q4 2017; 836 million in Q1 2018; 957 million in Q2 2018; and 1.2 billion in Q3 2018.\n\nThough it\u2019s lagging on publishing more up-to-date spam data now, noting in the report submitted to the EC that: \u201cUpdated spam metrics are expected to be available in November 2019 for Q2 and Q3 2019\u2033 \u2014 i.e. conveniently late for inclusion in this report.\n\nFacebook\u2019s report notes ongoing efforts to put contextual labels on certain types of suspect/partisan content, such as labelling photos and videos which have been independently fact-checked as misleading; labelling state-controlled media; and labelling political ads.\n\nLabelling bots is not discussed in the report \u2014 presumably because Facebook prefers to focus attention on self-defined spam-removal metrics vs muddying the water with discussion of how much suspect activity it continues to host on its platform, either through incompetence, lack of resources or because it\u2019s politically expedient for its business to do so.\n\nLabelling all these bots would mean Facebook signposting inconsistencies in how it applies its own policies \u2013in a way that might foreground its own political bias. And there\u2019s no self-regulatory mechanism under the sun that will make Facebook fess up to such double-standards.\n\nFor now, the Code\u2019s requirement for signatories to publish an annual report on what they\u2019re doing to tackle disinformation looks to be the biggest win so far. Albeit, it\u2019s very loosely bound self-reporting. While some of these \u2018reports\u2019 don\u2019t even run to a full page of A4-text \u2014 so set your expectations accordingly.\n\nThe Commission has published all the reports here. It has also produced its own summary and assessment of them (here).\n\n\u201cOverall, the reporting would benefit from more detailed and qualitative insights in some areas and from further big-picture context, such as trends,\u201d it writes. \u201cIn addition, the metrics provided so far are mainly output indicators rather than impact indicators.\u201d\n\nOf the Code generally \u2014 as a \u201cself-regulatory standard\u201d \u2014 the Commission argues it has \u201cprovided an opportunity for greater transparency into the platforms\u2019 policies on disinformation as well as a framework for structured dialogue to monitor, improve and effectively implement those policies\u201d, adding: \u201cThis represents progress over the situation prevailing before the Code\u2019s entry into force, while further serious steps by individual signatories and the community as a whole are still necessary.\u201d", "description": "It\u2019s a year since the European Commission got a bunch of adtech giants together to spill ink on a voluntary Code of Practice to do something \u2014 albeit, nothing very quantifiable \u2014 as a first step to stop the spread of disinformation online. Its latest report card on this voluntary effort sums to the platforms [\u2026]", "authors": [], "top_image": "https://techcrunch.com/wp-content/uploads/2018/05/gettyimages-961412842.jpg?w=600", "published_at": "2019-10-29"}