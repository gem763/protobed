{"pub": "businessinsider", "url": "https://gizmodo.com.au/2019/10/how-a-neutral-health-algorithm-ended-up-hurting-black-patients", "downloaded_at": "2019-10-25 22:03:25.835687+00:00", "title": "How A 'Neutral' Health Algorithm Ended Up Hurting Black Patients", "language": "en", "text": "Image: Marco Verch, Flickr (CC BY 2.0)\n\nA health care algorithm used in hospitals across the U.S. has been discriminating against black patients, according to new research. The study found that the algorithm consistently prioritised less-sick white patients and screened out black patients from a program meant to help people who need more intensive care.\n\nPredictive algorithms have found their way into many areas of society, including health care. But plenty of research has shown these AIs can have the same sort of biases that their creators do, despite being designed to be \u201cneutral.\u201d These biases exist even in medicine, where systematic racial and gender discrimination toward patients remains commonplace.\n\n\u201cThis is an extremely important study that indicates why we should not blindly trust AI to solve our most pressing social and societal problems.\u201d\n\nAccording to the authors behind the new paper, though, researchers have rarely had the opportunity to study up close how and why bias can creep into these algorithms. Many algorithms are proprietary, meaning the exact details of how they were programmed\u2014including the sources of data used to train them\u2014are off-limits to independent scientists. That didn\u2019t turn out to be the case in this study, published Thursday in Science.\n\nThe authors looked at data from an algorithm developed by the company Optum that\u2019s widely used in hospitals and health care centres, including the hospital where some of the authors worked.\n\nThe AI was meant to weigh in on which patients would most benefit from access to a high-risk health care management program. Among other things, the program would allow these patients to have dedicated health care staff when sick and extra appointment slots to visit their doctor as outpatients. But when they compared the risk score generated by the AI to other measures of health in their real-life patients, such as how many chronic illnesses a patient had, black patients were consistently undervalued. Under the AI\u2019s estimate, for instance, 18 per cent of patients who deserved to be in these programs would be black; but the authors estimated that the real number should be closer to 47 per cent.\n\n\u201cThis is an extremely important study that indicates why we should not blindly trust AI to solve our most pressing social and societal problems,\u201d Desmond Patton, a data scientist at Columbia University\u2019s School of Social Work who isn\u2019t unaffiliated with the new research, told Gizmodo.\n\nThe AI\u2019s decision-making process was designed to be race-neutral. As the authors found out, though, the other assumptions it was programmed with biased it against black people. A key variable it studied was how much money had been spent on patients\u2019 health care up until then, with those who had the most money spent being considered more in need of the program. But black patients don\u2019t see the doctor or get medical care as much as white patients, often because they\u2019re poorer. This is compounded by the fact that black patients are then typically sicker by the time they visit a hospital, because their chronic health problems had gone untreated.\n\n\u201cThe bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients,\u201d the authors wrote.\n\nThese disparities in medicine and elsewhere aren\u2019t exactly a secret. But if an AI isn\u2019t programmed to account for them or trained with lots of different groups of people, then they go ignored, according to Atul Butte, a senior researcher in biomedical informatics at the University of California San Francisco.\n\n\u201cThe analogy I have used in the past is that you or I probably would not be comfortable getting into a self-driving car trained only in Mountain View, California,\u201d Butte, who was not involved in the new research, told Gizmodo. \u201cSo we really should be wary about medical algorithms trained with only a small population or in just one race or ethnicity.\u201d\n\nThe findings, according to Jessie Tenenbaum, an assistant professor of biostatistics and bioinformatics at Duke University, also not involved in the new work, show why it\u2019s important for outside scientists and companies to work together on improving algorithms once they enter the real world.\n\n\u201cI\u2019m a fan of using AI where it can be helpful, but it\u2019s going to be impossible to anticipate all of the ways these biases can creep in and affect results,\u201d she said. \u201cWhat\u2019s important, then, is to think about how biased data could affect a given application, to check results for such bias, and as much as possible to use AI methods that enable explainability\u2014understanding why an algorithm came to the conclusion it did.\u201d\n\nTo that end, the authors of the current study told the Washington Post that they are already working with Optum to recalibrate the algorithm and that they hope other companies will have their AIs audited as well.\n\n\u201cIt\u2019s truly inconceivable to me that anyone else\u2019s algorithm doesn\u2019t suffer from this,\u201d senior study author Sendhil Mullainathan, a professor of computation and behavioural science at the University of Chicago Booth School of Business, told the Washington Post. \u201cI\u2019m hopeful that this causes the entire industry to say, \u2018Oh, my, we\u2019ve got to fix this.\u2019\u201d\n\nRegulatory agencies like the Food and Drug Administration should also proactively enforce better training of these algorithms and require transparent data-sharing from the companies that make them, Butte said.", "description": "A health care algorithm used in hospitals across the U.S. has been discriminating against black patients, according to new research. The study found that the algorithm consistently prioritised less-sick white patients and screened out black patients from a  program meant to help people who need more intensive care....", "authors": ["Beth Elderkin", "Sarah Basford"], "top_image": "https://i.kinja-img.com/gawker-media/image/upload/c_lfill,w_1200,h_628,q_90/egaksthgi4ensflu2okk.jpg", "published_at": "2019-10-25"}