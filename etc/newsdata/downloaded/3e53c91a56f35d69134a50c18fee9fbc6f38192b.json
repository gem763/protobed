{"pub": "thenextweb", "url": "https://thenextweb.com/artificial-intelligence/2019/10/15/mit-researchers-unveil-new-system-to-improve-fake-news-detection", "downloaded_at": "2019-10-15 14:56:22.558418+00:00", "title": "MIT researchers unveil new system to improve fake news detection", "language": "en", "text": "Bad actors are increasingly using more advanced methods to generate fake news and fool readers into thinking they are legitimate. AI-based text generators, including OpenAI\u2019s GPT-2 model, which try and imitate human writers play a big part in this.\n\nTo mitigate this, researchers have developed tools to detect artificially generated text. However, new research from MIT suggests there might be a fundamental flaw in the way these detectors work.\n\nTraditionally, these tools trace back a text\u2019s writing style to determine if it\u2019s written by humans or a bot. They assume text written by humans is always legitimate and the text generated by bots is always fake. That means if even if a machine can generate legitimate text for some uses cases, it is deemed fake by these models.\n\nPlus, the research highlights attackers can use tools to manipulate human-generated text. Researchers trained AI to use a using GPT-2 model to corrupt human-generated text to alter its meaning.\n\nTal Schuster, an MIT student and lead author on the research, said it\u2019s important to detect factual falseness of a text rather than determining if it was generated by a machine or a human:\n\nWe need to have the mindset that the most intrinsic \u2018fake news\u2019 characteristic is factual falseness, not whether or not the text was generated by machines. Text generators don\u2019t have a specific agenda \u2013 it\u2019s up to the user to decide how to use this technology.\n\nMIT professor Regina Barzilay said this research highlighted the lack of credibility of current misinformation classifiers.\n\nTo overcome these flaws, the same set of researchers used the world\u2019s largest fact-checking database, Fact Extraction, and Verification (FEVER), to develop new detection systems.\n\nHowever, the research team found the model developed through FEVER was prone to errors due to the datasets\u2019 bias.\n\nSchuster said negated phrases were often deemed to be false by the model:\n\nMany of the statements created by human annotators contain give-away phrases. For example, phrases like \u2018did not\u2019 and \u2018yet to\u2019 appear mostly in false statements.\n\nHowever, when the team created a data set by debiasing FEVER, the detection model\u2019s accuracy fell from 86 to 58 percent showing there\u2019s more work to be done to train AI on non-biased data.\n\nHe said the model had taken the language of the claim into account without any external evidence. So, there\u2019s a chance a detector can deem a future event false because it hasn\u2019t used external sources as part of its verification process.\n\nThe team hopes to improve the model to detect new types of misinformation by combining fact-checking with existing defense mechanisms.", "description": "", "authors": ["Ivan Mehta"], "top_image": "https://img-cdn.tnwcdn.com/image/tnw?filter_last=1&fit=1280%2C640&url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2019%2F10%2FMIT-researchers.jpg&signature=e373005768a4c3f8b9b1230652336d58", "published_at": "2019-10-15"}