{"pub": "washingtontimes", "url": "https://washingtontimes.com/news/2019/sep/16/army-artificial-intelligence-task-force-combat-ai-", "downloaded_at": "2019-09-17 08:25:39.076077+00:00", "title": "'Killer robots': Pentagon advances combat AI despite fears, opposition", "language": "en", "text": "PITTSBURGH \u2014 The Army\u2019s Artificial Intelligence Task Force, with headquarters in the heart of the Rust Belt, is a crucial component of a grand Pentagon plan to incorporate robots and machine learning into 21st-century warfare.\n\nFor skeptics, the task force is the concrete embodiment of how the U.S. is headed down a rocky, uncertain road that could put humanity itself in danger.\n\nThe raging debate over AI, including its implications for the human race and the morality of its use in warfare, has divided the U.S. from some traditional allies. It also is fueling a growing band of activists who warn that \u201ckiller robots\u201d are on the horizon of a military that has no comprehensive plan to stop them or understand their implications.\n\nDeep, philosophical questions about the ramifications of AI technology \u2014 who is responsible for writing ethical guidelines, to what extent must humans remain in the loop, how much easier is it to fight a war fought (initially) by machines, and who bears the blame if a robot or drone ultimately targets humans \u2014 are just beginning to be confronted in a systematic way.\n\nSo far, global rules of the road have hit impenetrable roadblocks. A multinational effort to ban \u201clethal autonomous weapons systems\u201d fell flat again last month during a high-level U.N. gathering in Geneva.\n\nMilitary and diplomatic leaders say that with AI\u2019s combat applications in their infancy, it would be foolish to ban them preemptively. The nation\u2019s national security, they say, could ultimately suffer, and they reject the notion that AI technology ultimately will displace humans and bring about Terminator-like, apocalyptic battlefields.\n\nBut Army officials readily acknowledge the deep ethical concerns that come with the work conducted at the AI team\u2019s headquarters, where military personnel collaborate with Carnegie Mellon University researchers on a host of cutting-edge projects.\n\nIn an exclusive interview with The Washington Times, Army AI Task Force Deputy Director Col. Doug Matty stressed that the ethics of artificial intelligence \u2014 and the real potential that its capabilities could fall into the wrong hands \u2014 are pressing considerations as the technology moves inexorably ahead.\n\n\u201cYou have to have ethical considerations both from concept all the way through development, all the way to fielding,\u201d he said.\n\n\u201cIt\u2019s omnipresent,\u201d he said in reference to the Pentagon-wide mandate that the ethics of AI warfare be kept foremost in mind. \u201cIf you do it as an afterthought, then you\u2019ll have a gap where it\u2019ll allow exploitation from the friendly side, or potentially others.\u201d\n\nInside the Pentagon, the ethics of AI development are a top priority. The Defense Department\u2019s Joint Artificial Intelligence Center (JAIC), a Pentagon-wide project that encompasses AI initiatives in all corners of the military, recently announced that it would hire its first \u201cAI ethicist.\u201d\n\nThe job, officials say, will be to develop a comprehensive ethics policy that addresses a host of key questions around the legality, morality and practicality of the use of weapons and vehicles that require minimal human involvement in the business of potentially killing an enemy.\n\n\u201cWe are going to bring in someone who will have a deep background in ethics, and then the lawyers within the department will be looking at how we actually bake this into the Department of Defense,\u201d Air Force Lt. Gen. Jack Shanahan, JAIC director, told reporters this month.\n\nAt the core of the cutting-edge projects at the Army\u2019s AI center in Pittsburgh is a focus on how they will ultimately help soldiers in the field, officials say. The technology, they say, cannot be looked at in a vacuum and instead should be viewed through a national security lens.\n\n\u201cTo separate the technology from the mission is a misnomer,\u201d Col. Matty said. \u201cThe capability is developed to support the mission.\u201d\n\nGrowing fears\n\nBut military officials also are keenly aware of the deeper issues at play. In the Navy, for example, leaders don\u2019t deny uneasiness with their research into autonomous weapons.\n\n\u201cTrust is something that is difficult to come by with a computer, especially as we start working with our test and evaluation community,\u201d Steve Olson, deputy branch head of the Navy\u2019s mine warfare office, told the publication Defense News.\n\n\u201cI\u2019ve worked with our test and evaluation director, and a lot of times it\u2019s: \u2018Hey, what\u2019s that thing going to do?\u2019 And I say: \u2018I don\u2019t know, it\u2019s going to pick the best path,\u2019\u201d he said. \u201cAnd they don\u2019t like that at all because autonomy makes a lot of people nervous. But the flip side of this is that there is one thing that we have to be very careful of, and that\u2019s that we don\u2019t overtrust. \u2026 The last thing we want to see is the whole \u2018Terminator going crazy\u2019 scenario.\u201d\n\nCritics of the AI revolution argue that world governments, led by the U.S., must adopt a sweeping treaty to govern the use of robots in combat. Leading international powers gathered in Geneva last month to discuss that very topic, and a growing group of nations are renewing a push for a worldwide ban on lethal autonomous weapons systems.\n\nJordan is the latest of at least 29 countries that have signed on to the idea.\n\nBut the world\u2019s leading military powers, including the U.S., Russia and Britain, have resisted such a move. Critics argue that those countries are paving the way for a grim future.\n\nUltimately, they say, the tide of public opinion will turn against AI and its applications for war.\n\n\u201cRussia and the U.S. are continuing their losing fight to prevent the creation of the inevitable treaty that\u2019s coming for killer robots,\u201d said Mary Wareham, advocacy director of the arms division at Human Rights Watch and coordinator of the Campaign to Stop Killer Robots. \u201cNations cannot allow their ambition and desire to create a new treaty on these weapons systems to be limited by these military powers.\u201d\n\nThe Trump administration remains opposed to a full ban, though officials stressed that Washington would not object to the drafting of international principles to govern the military use of AI.\n\n\u201cThese guiding principles included statements that human responsibility for decisions on the use of weapons systems must be retained, since accountability cannot be transferred to machines, and that human-machine interaction,\u201d a State Department official told The Washington Times.\n\nThose principles, the officials added, \u201cmay take various forms and be implemented at various stages of the life cycle of a weapon\u201d and should ensure that any use of AI weaponry \u201cis in compliance with applicable international law, in particular international humanitarian law.\u201d\n\nSign up for Daily Newsletters\n\nCopyright \u00a9 2019 The Washington Times, LLC. Click here for reprint permission.", "description": "The raging debate over AI, including its implications for the human race and the morality of its use in warfare, has divided the U.S. from some traditional allies.", "authors": ["The Washington Times Http", "Ben Wolfgang"], "top_image": "https://twt-thumbs.washtimes.com/media/image/2019/08/18/8_182019_ap-162038409803208201_c0-16-5384-3155_s1770x1032.jpg?cee6f36978f0f9cd5e75c1164621bb1ef091c4a1", "published_at": "2019-09-16"}