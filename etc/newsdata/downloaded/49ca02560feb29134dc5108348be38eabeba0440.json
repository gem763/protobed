{"pub": "zdnet", "url": "https://www.zdnet.com/article/google-searchs-ability-to-understand-you-just-made-its-biggest-leap-in-5-years", "downloaded_at": "2019-10-25 16:23:17.093090+00:00", "title": "Google: Search's ability to understand you just made its 'biggest leap in 5 years'", "language": "en", "text": "Google's new tool lets you see how polluted these cities really are Google has launched a free online tool that provides urban planners with informed data on selected cities' carbon footprint.\n\nGoogle says its newest algorithms for answering questions, nicknamed BERT, have delivered the biggest improvements to understanding queries in the past five years. The last major boost it got was from RankBrain in 2015.\n\nsee also How to use Google two-factor authentication If you use Google services, and you don't want anyone raiding or wrecking your account and services--not to mention your life--Google two-step verification security is for you. Read More\n\nBERT is short for Bidirectional Encoder Representations from Transformers, a machine-learning technique for generating models in the field of natural-language processing.\n\nBERT can guess a word by looking at the words before it and after it in a sentence, making it 'bidirectional'. Google's BERT one-upped the 'unidirectional' OpenAI GPT, which only looked at words before the one being guessed in a sentence.\n\nThe bidirectional aspect of BERT models offers more context about a word, which should help Google Search understand the intent behind search queries, especially where prepositions like 'to' and 'from' matter to meaning, according to Pandu Nayak, Google fellow and vice president of Search.\n\nGoogle open-sourced BERT last year and detailed the research in its AI blog at the time. According to Google, the complexity of BERT models has necessitated deploying its Cloud TPUs to deliver search results.\n\nBesides search ranking, Google is also applying BERT models to featured snippets. Google says BERT will improve its ability to understand about 10% of search queries in English in the US. Gradually it will bring the new natural-language processing models to more languages and markets.\n\nThe new models should help Google Search better understand the 15% of queries it gets every day that its systems have never seen before.\n\nNayak offered a few examples where Search previously missed the importance of 'for' and 'to' in delivering results.\n\nOne example is: \"2019 brazil traveler to usa need a visa\", where 'to' is important to understand that it's a Brazilian who is traveling to the US. Before BERT, Google's algorithms returned results about Americans traveling to Brazil.\n\nAnother is: \"can you get medicine for someone pharmacy\". Before it delivered general answers about prescriptions but the BERT model understands that the query is about whether you can pick up a prescription for another person.\n\nAs for languages other than English, Google says it is can use BERT learnings from language and apply them to others.\n\nMore on Google and search", "description": "Google's new BERT models demand new cloud TPUs for serving search results.", "authors": ["Liam Tung"], "top_image": "https://zdnet1.cbsistatic.com/hub/i/r/2019/10/25/54b8ea84-5bc8-4197-bc39-19410d22f618/thumbnail/770x578/629da4e83f71f16569d77c064c4940e2/phoneusersistock-999231414.jpg", "published_at": "2019-10-25"}