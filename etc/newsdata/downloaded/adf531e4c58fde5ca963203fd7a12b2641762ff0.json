{"pub": "washingtonpost", "url": "https://washingtonpost.com/technology/2019/09/18/facebook-google-twitter-face-fresh-heat-congress-harmful-online-content", "downloaded_at": "2019-09-18 23:25:14.972281+00:00", "title": "Facebook, Google and Twitter face fresh heat from Congress on harmful online content", "language": "en", "text": "\n\nFrom left, officials for Facebook, Twitter and Google appear before the Senate Commerce, Science and Transportation Committee on Sept. 18. The hearing focused on, mass violence, extremism and digital responsibility. (Mark Wilson/Getty Images)\n\nCongressional lawmakers are drafting a bill to create a \u201cnational commission\u201d at the Department of Homeland Security to study the ways that social media can be weaponized \u2014 and the effectiveness of tech giants\u2019 efforts to protect users from harmful content online.\n\nThe draft House bill obtained by The Washington Post is slated to be introduced and considered next week. If passed, the commission would be empowered \u2014 with the authority to hold hearings and issue subpoenas \u2014 to study the way social media companies police the Web and to recommend potential legislation. It also would create a federal social media task force to coordinate the government\u2019s response to security issues.\n\nThe effort reflects a growing push by members of Congress to combat online hate speech, disinformation and other harmful content online, including a hearing held Wednesday where Senate lawmakers questioned Facebook, Google and Twitter executives to probe whether their platforms have become conduits for real-world violence.\n\n[The New Zealand shooting shows how YouTube and Facebook spread hate and violent images \u2014 yet again]\n\nAll three tech giants told lawmakers at the Wednesday hearing that they have made progress in combating dangerous posts, photos and videos \u2014 improvements they attributed largely to advancements in their artificial-intelligence tools. But some Democrats and Republicans in Congress still contend the companies haven\u2019t acted aggressively enough.\n\n\u201cI would suggest even more needs to be done, and it needs to be better, and you have the resources and technological capability to do more and better,\u201d Democratic Sen. Richard Blumenthal (Conn.) said at the hearing.\n\nLawmakers have grown increasingly concerned about the use of social media sites as conduits for violence and extremism, pointing to recent attacks including the mass shooting in Christchurch, New Zealand. Users uploaded videos of the deadly incidents at two mosques earlier this year, evading tech giants\u2019 censors and then proving difficult to scrub.\n\nBut the most vile content has appeared on sites such as Gab, a haven for the alt-right, and 8chan, an anonymous message board. The latter site has been taken down in the aftermath of a shooting in El Paso this year that left 22 people dead. The suspect there is believed to have posted a manifesto to 8chan before carrying out his attack.\n\n[From helicopter repairman to leader of the Internet\u2019s \u2018darkest reaches\u2019: The life and times of 8chan owner Jim Watkins]\n\nLawmakers led by Rep. Bennie Thompson (D-Miss.), chairman of the House Homeland Security Committee, grilled the owner of 8chan at a private session this year. Thompson later said he had plans for a bill that would create the social media commission.\n\n\u201cOne thing\u2019s for sure \u2014 the challenge of preventing online terrorism content is one of the greatest post-9/11 homeland security challenges,\u201d he said in a statement Wednesday.\n\nIn the Senate, the tech giants faced similar concerns from lawmakers. \u201cIn today\u2019s Internet-connected society, misinformation, fake news, deep fakes and viral online conspiracy theories have become the norm,\u201d said Republican Sen. Roger Wicker (Miss.), the chairman of the Senate Commerce Committee, to open the Wednesday hearing.\n\nIn response, Facebook, Google and Twitter said during their testimony that they had seen success in deploying automated tools to police for hate, violence and terrorist propaganda.\n\nYouTube said nearly 90 percent of the 9 million videos they had removed in the second quarter of the year had been flagged by automated tools. Those played a major role in removing videos, comments and channels flagged for hate speech, which the company said had spiked in recent months.\n\nFacebook said this week it would begin using police training videos to help its automated tools better detect first-person shooting videos like the one recorded in Christchurch. The company said its detection system, which was designed to automatically flag and remove videos showing violence, sex or objectionable content, now finds a rule violation on its live-streaming system in an average of 12 seconds. Also this week, Facebook also announced updates to its efforts to stop and remove hate speech, including unveiling a roughly 40-person independent board that will oversee content decisions and shape company policy.\n\n[Facebook unveils charter for its \u2018Supreme Court,\u2019 where users can go to contest the company\u2019s decisions]\n\nTwitter said its abusive-content-monitoring systems now flag half of the content that is ultimately removed, compared with 20 percent a year ago. The site also said it was piloting a program in which it would alert outside websites when it appears they are hosting videos or other files promoting terrorist content.\n\nBut Twitter found itself embroiled in another controversy in Congress on Wednesday after President Trump retweeted a widely followed conservative comedian, who portrayed a video of Rep. Ilhan Omar (D-Minn.) dancing as a sign she was celebrating on the anniversary of the 9/11 attacks.\n\nOmar responded that the video came from an event for the Congressional Black Caucus, before pointing out Trump\u2019s retweet of it \u2014 who repeatedly has attacked the Democratic lawmaker \u2014 had resulted in her receiving death threats.\n\n\u201cThe President of the United States is continuing to spread lies that put my life at risk,\u201d Omar tweeted. \u201cWhat is Twitter doing to combat this misinformation?\u201d\n\nThe comedian, Terrence K. Williams later deleted his own tweet, Twitter confirmed Wednesday, without providing additional comment.\n\nMeanwhile, major civil rights groups wrote to the chief executives of Facebook, Google, Twitter and YouTube on Tuesday saying they had a \u201cmoral responsibility\u201d to better combat how social media could be exploited \u201cto inflict fear and spread hate.\u201d\n\nThe groups, including the Leadership Conference on Civil and Human Rights and the NAACP Legal Defense and Educational Fund, urged the companies to conduct regular civil rights audits and implement corporate accountability measures that would help address hate on their sites.\n\n\u201cEach massacre makes clearer that, while each of your companies has taken some steps to address white nationalism and white supremacy online, those steps are not enough,\u201d the letter stated.", "description": "The continued struggles of Facebook, Google and Twitter to stop the spread of hate speech, disinformation and other harmful content online have sparked heightened interest on Capitol Hill, where lawmakers are expected to unveil legislation to probe the matter in coming days.", "authors": ["Tony Romm", "Senior Tech Policy Reporter", "Drew Harwell", "Reporter Covering Artificial Intelligence", "September At Pm", "Tony Romm Is A Technology Policy Reporter At The Washington Post. He Has Spent Nearly Ten Years Covering The Ways That Tech Companies Like Apple", "Facebook", "Google Navigate The Corridors Of Government --", "The Regulations That Sometimes Result.", "Drew Harwell Is A Technology Reporter For The Washington Post Covering Artificial Intelligence"], "top_image": "https://www.washingtonpost.com/resizer/Ync_YseFtCMsX0cJ96cDV0gIFEU=/1484x0/arc-anglerfish-washpost-prod-washpost.s3.amazonaws.com/public/OPBU7EW2EYI6TINFCYVYVHE4UI.jpg", "published_at": "2019-09-18"}