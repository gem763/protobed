{"pub": "theverge", "url": "https://theverge.com/2019/2/25/18229714/cognizant-facebook-content-moderator-interviews-trauma-working-conditions-arizona", "downloaded_at": "2019-10-05 14:28:34.320724+00:00", "title": "The secret lives of Facebook moderators in America", "language": "en", "text": "Your browser does not support the video tag.\n\nContent warning: This story contains discussion of serious mental health issues and racism.\n\nThe panic attacks started after Chloe watched a man die.\n\nShe spent the past three and a half weeks in training, trying to harden herself against the daily onslaught of disturbing posts: the hate speech, the violent attacks, the graphic pornography. In a few more days, she will become a full-time Facebook content moderator, or what the company she works for, a professional services vendor named Cognizant, opaquely calls a \u201cprocess executive.\u201d\n\nFor this portion of her education, Chloe will have to moderate a Facebook post in front of her fellow trainees. When it\u2019s her turn, she walks to the front of the room, where a monitor displays a video that has been posted to the world\u2019s largest social network. None of the trainees have seen it before, Chloe included. She presses play.\n\nSomeone is stabbing him, dozens of times, while he screams and begs for his life.\n\nThe video depicts a man being murdered. Someone is stabbing him, dozens of times, while he screams and begs for his life. Chloe\u2019s job is to tell the room whether this post should be removed. She knows that section 13 of the Facebook community standards prohibits videos that depict the murder of one or more people. When Chloe explains this to the class, she hears her voice shaking.\n\nReturning to her seat, Chloe feels an overpowering urge to sob. Another trainee has gone up to review the next post, but Chloe cannot concentrate. She leaves the room, and begins to cry so hard that she has trouble breathing.\n\nNo one tries to comfort her. This is the job she was hired to do. And for the 1,000 people like Chloe moderating content for Facebook at the Phoenix site, and for 15,000 content reviewers around the world, today is just another day at the office.\n\nOver the past three months, I interviewed a dozen current and former employees of Cognizant in Phoenix. All had signed non-disclosure agreements with Cognizant in which they pledged not to discuss their work for Facebook \u2014 or even acknowledge that Facebook is Cognizant\u2019s client. The shroud of secrecy is meant to protect employees from users who may be angry about a content moderation decision and seek to resolve it with a known Facebook contractor. The NDAs are also meant to prevent contractors from sharing Facebook users\u2019 personal information with the outside world, at a time of intense scrutiny over data privacy issues.\n\nBut the secrecy also insulates Cognizant and Facebook from criticism about their working conditions, moderators told me. They are pressured not to discuss the emotional toll that their job takes on them, even with loved ones, leading to increased feelings of isolation and anxiety. To protect them from potential retaliation, both from their employers and from Facebook users, I agreed to use pseudonyms for everyone named in this story except Cognizant\u2019s vice president of operations for business process services, Bob Duncan, and Facebook\u2019s director of global partner vendor management, Mark Davidson.\n\nA content moderator working for Cognizant in Arizona will earn just $28,800 per year.\n\nCollectively, the employees described a workplace that is perpetually teetering on the brink of chaos. It is an environment where workers cope by telling dark jokes about committing suicide, then smoke weed during breaks to numb their emotions. It\u2019s a place where employees can be fired for making just a few errors a week \u2014 and where those who remain live in fear of the former colleagues who return seeking vengeance.\n\nIt\u2019s a place where, in stark contrast to the perks lavished on Facebook employees, team leaders micromanage content moderators\u2019 every bathroom and prayer break; where employees, desperate for a dopamine rush amid the misery, have been found having sex inside stairwells and a room reserved for lactating mothers; where people develop severe anxiety while still in training, and continue to struggle with trauma symptoms long after they leave; and where the counseling that Cognizant offers them ends the moment they quit \u2014 or are simply let go.\n\nKEY FINDINGS Moderators in Phoenix will make just $28,800 per year \u2014 while the average Facebook employee has a total compensation of $240,000.\n\nIn stark contrast to the perks lavished on Facebook employees, team leaders micro-manage content moderators\u2019 every bathroom break. Two Muslim employees were ordered to stop praying during their nine minutes per day of allotted \u201cwellness time.\u201d\n\nEmployees can be fired after making just a handful of errors a week, and those who remain live in fear of former colleagues returning to seek vengeance. One man we spoke with started bringing a gun to work to protect himself.\n\nEmployees have been found having sex inside stairwells and a room reserved for lactating mothers, in what one employee describes as \u201ctrauma bonding.\u201d\n\nModerators cope with seeing traumatic images and videos by telling dark jokes about committing suicide, then smoking weed during breaks to numb their emotions. Moderators are routinely high at work.\n\nEmployees are developing PTSD-like symptoms after they leave the company, but are no longer eligible for any support from Facebook or Cognizant.\n\nEmployees have begun to embrace the fringe viewpoints of the videos and memes that they are supposed to moderate. The Phoenix site is home to a flat Earther and a Holocaust denier. A former employee tells us he no longer believes 9/11 was a terrorist attack.\n\nThe moderators told me it\u2019s a place where the conspiracy videos and memes that they see each day gradually lead them to embrace fringe views. One auditor walks the floor promoting the idea that the Earth is flat. A former employee told me he has begun to question certain aspects of the Holocaust. Another former employee, who told me he has mapped every escape route out of his house and sleeps with a gun at his side, said: \u201cI no longer believe 9/11 was a terrorist attack.\u201d\n\nChloe cries for a while in the break room, and then in the bathroom, but begins to worry that she is missing too much training. She had been frantic for a job when she applied, as a recent college graduate with no other immediate prospects. When she becomes a full-time moderator, Chloe will make $15 an hour \u2014 $4 more than the minimum wage in Arizona, where she lives, and better than she can expect from most retail jobs.\n\nThe tears eventually stop coming, and her breathing returns to normal. When she goes back to the training room, one of her peers is discussing another violent video. She sees that a drone is shooting people from the air. Chloe watches the bodies go limp as they die.\n\nShe leaves the room again.\n\nEventually a supervisor finds her in the bathroom, and offers a weak hug. Cognizant makes a counselor available to employees, but only for part of the day, and he has yet to get to work. Chloe waits for him for the better part of an hour.\n\nWhen the counselor sees her, he explains that she has had a panic attack. He tells her that, when she graduates, she will have more control over the Facebook videos than she had in the training room. You will be able to pause the video, he tells her, or watch it without audio. Focus on your breathing, he says. Make sure you don\u2019t get too caught up in what you\u2019re watching.\n\n\u201dHe said not to worry \u2014 that I could probably still do the job,\u201d Chloe says. Then she catches herself: \u201cHis concern was: don\u2019t worry, you can do the job.\u201d\n\nOn May 3, 2017, Mark Zuckerberg announced the expansion of Facebook\u2019s \u201ccommunity operations\u201d team. The new employees, who would be added to 4,500 existing moderators, would be responsible for reviewing every piece of content reported for violating the company\u2019s community standards. By the end of 2018, in response to criticism of the prevalence of violent and exploitative content on the social network, Facebook had more than 30,000 employees working on safety and security \u2014 about half of whom were content moderators.\n\nThe moderators include some full-time employees, but Facebook relies heavily on contract labor to do the job. Ellen Silver, Facebook\u2019s vice president of operations, said in a blog post last year that the use of contract labor allowed Facebook to \u201cscale globally\u201d \u2014 to have content moderators working around the clock, evaluating posts in more than 50 languages, at more than 20 sites around the world.\n\nThe use of contract labor also has a practical benefit for Facebook: it is radically cheaper. The median Facebook employee earns $240,000 annually in salary, bonuses, and stock options. A content moderator working for Cognizant in Arizona, on the other hand, will earn just $28,800 per year. The arrangement helps Facebook maintain a high profit margin. In its most recent quarter, the company earned $6.9 billion in profits, on $16.9 billion in revenue. And while Zuckerberg had warned investors that Facebook\u2019s investment in security would reduce the company\u2019s profitability, profits were up 61 percent over the previous year.\n\nSince 2014, when Adrian Chen detailed the harsh working conditions for content moderators at social networks for Wired, Facebook has been sensitive to the criticism that it is traumatizing some of its lowest-paid workers. In her blog post, Silver said that Facebook assesses potential moderators\u2019 \u201cability to deal with violent imagery,\u201d screening them for their coping skills.\n\nBob Duncan, who oversees Cognizant\u2019s content moderation operations in North America, says recruiters carefully explain the graphic nature of the job to applicants. \u201cWe share examples of the kinds of things you can see \u2026 so that they have an understanding,\u201d he says. \u201cThe intention of all that is to ensure people understand it. And if they don\u2019t feel that work is potentially suited for them based on their situation, they can make those decisions as appropriate.\u201d\n\nUntil recently, most Facebook content moderation has been done outside the United States. But as Facebook\u2019s demand for labor has grown, it has expanded its domestic operations to include sites in California, Arizona, Texas, and Florida.\n\nCognizant workers\u2019 time is managed down to the second.\n\nThe United States is the company\u2019s home and one of the countries in which it is most popular, says Facebook\u2019s Davidson. American moderators are more likely to have the cultural context necessary to evaluate U.S. content that may involve bullying and hate speech, which often involve country-specific slang, he says.\n\nFacebook also worked to build what Davidson calls \u201cstate-of-the-art facilities, so they replicated a Facebook office and had that Facebook look and feel to them. That was important because there\u2019s also a perception out there in the market sometimes \u2026 that our people sit in very dark, dingy basements, lit only by a green screen. That\u2019s really not the case.\u201d\n\nIt is true that Cognizant\u2019s Phoenix location is neither dark nor dingy. And to the extent that it offers employees desks with computers on them, it may faintly resemble other Facebook offices. But while employees at Facebook\u2019s Menlo Park headquarters work in an airy, sunlit complex designed by Frank Gehry, its contractors in Arizona labor in an often cramped space where long lines for the few available bathroom stalls can take up most of employees\u2019 limited break time. And while Facebook employees enjoy a wide degree of freedom in how they manage their days, Cognizant workers\u2019 time is managed down to the second.\n\nA content moderator named Miguel arrives for the day shift just before it begins, at 7 a.m. He\u2019s one of about 300 workers who will eventually filter into the workplace, which occupies two floors in a Phoenix office park.\n\nSecurity personnel keep watch over the entrance, on the lookout for disgruntled ex-employees and Facebook users who might confront moderators over removed posts. Miguel badges in to the office and heads to the lockers. There are barely enough lockers to go around, so some employees have taken to keeping items in them overnight to ensure they will have one the next day.\n\nThe lockers occupy a narrow hallway that, during breaks, becomes choked with people. To protect the privacy of the Facebook users whose posts they review, workers are required to store their phones in lockers while they work.\n\nWriting utensils and paper are also not allowed, in case Miguel might be tempted to write down a Facebook user\u2019s personal information. This policy extends to small paper scraps, such as gum wrappers. Smaller items, like hand lotion, are required to be placed in clear plastic bags so they are always visible to managers.\n\nTo accommodate four daily shifts \u2014 and high employee turnover \u2014 most people will not be assigned a permanent desk on what Cognizant calls \u201cthe production floor.\u201d Instead, Miguel finds an open workstation and logs in to a piece of software known as the Single Review Tool, or SRT. When he is ready to work, he clicks a button labeled \u201cresume reviewing,\u201d and dives into the queue of posts.\n\nLast April, a year after many of the documents had been published in the Guardian, Facebook made public the community standards by which it attempts to govern its 2.3 billion monthly users. In the months afterward, Motherboard and Radiolab published detailed investigations into the challenges of moderating such a vast amount of speech.\n\n\u201cAutistic people should be sterilized\u201d seems offensive to him, but it stays up\n\nThose challenges include the sheer volume of posts; the need to train a global army of low-paid workers to consistently apply a single set of rules; near-daily changes and clarifications to those rules; a lack of cultural or political context on the part of the moderators; missing context in posts that makes their meaning ambiguous; and frequent disagreements among moderators about whether the rules should apply in individual cases.\n\nDespite the high degree of difficulty in applying such a policy, Facebook has instructed Cognizant and its other contractors to emphasize a metric called \u201caccuracy\u201d over all else. Accuracy, in this case, means that when Facebook audits a subset of contractors\u2019 decisions, its full-time employees agree with the contractors. The company has set an accuracy target of 95 percent, a number that always seems just out of reach. Cognizant has never hit the target for a sustained period of time \u2014 it usually floats in the high 80s or low 90s, and was hovering around 92 at press time.\n\nMiguel diligently applies the policy \u2014 even though, he tells me, it often makes no sense to him.\n\nA post calling someone \u201cmy favorite n-----\u201d is allowed to stay up, because under the policy it is considered \u201cexplicitly positive content.\u201d\n\n\u201cAutistic people should be sterilized\u201d seems offensive to him, but it stays up as well. Autism is not a \u201cprotected characteristic\u201d the way race and gender are, and so it doesn\u2019t violate the policy. (\u201cMen should be sterilized\u201d would be taken down.)\n\nIn January, Facebook distributes a policy update stating that moderators should take into account recent romantic upheaval when evaluating posts that express hatred toward a gender. \u201cI hate all men\u201d has always violated the policy. But \u201cI just broke up with my boyfriend, and I hate all men\u201d no longer does.\n\nMiguel works the posts in his queue. They arrive in no particular order at all.\n\nHere is a racist joke. Here is a man having sex with a farm animal. Here is a graphic video of murder recorded by a drug cartel. Some of the posts Miguel reviews are on Facebook, where he says bullying and hate speech are more common; others are on Instagram, where users can post under pseudonyms, and tend to share more violence, nudity, and sexual activity.\n\n\u201cAccuracy is only judged by agreement...\u201d\n\nEach post presents Miguel with two separate but related tests. First, he must determine whether a post violates the community standards. Then, he must select the correct reason why it violates the standards. If he accurately recognizes that a post should be removed, but selects the \u201cwrong\u201d reason, this will count against his accuracy score.\n\nMiguel is very good at his job. He will take the correct action on each of these posts, striving to purge Facebook of its worst content while protecting the maximum amount of legitimate (if uncomfortable) speech. He will spend less than 30 seconds on each item, and he will do this up to 400 times a day.\n\nWhen Miguel has a question, he raises his hand, and a \u201csubject matter expert\u201d (SME) \u2014 a contractor expected to have more comprehensive knowledge of Facebook\u2019s policies, who makes $1 more per hour than Miguel does \u2014 will walk over and assist him. This will cost Miguel time, though, and while he does not have a quota of posts to review, managers monitor his productivity, and ask him to explain himself when the number slips into the 200s.\n\nFrom Miguel\u2019s 1,500 or so weekly decisions, Facebook will randomly select 50 or 60 to audit. These posts will be reviewed by a second Cognizant employee \u2014 a quality assurance worker, known internally as a QA, who also makes $1 per hour more than Miguel. Full-time Facebook employees then audit a subset of QA decisions, and from these collective deliberations, an accuracy score is generated.\n\nMiguel takes a dim view of the accuracy figure.\n\n\u201cAccuracy is only judged by agreement. If me and the auditor both allow the obvious sale of heroin, Cognizant was \u2018correct,\u2019 because we both agreed,\u201d he says. \u201cThis number is fake.\u201d\n\nFacebook\u2019s single-minded focus on accuracy developed after sustaining years of criticism over its handling of moderation issues. With billions of new posts arriving each day, Facebook feels pressure on all sides. In some cases, the company has been criticized for not doing enough \u2014 as when United Nations investigators found that it had been complicit in spreading hate speech during the genocide of the Rohingya community in Myanmar. In others, it has been criticized for overreach \u2014 as when a moderator removed a post that excerpted the Declaration of Independence. (Thomas Jefferson was ultimately granted a posthumous exemption to Facebook\u2019s speech guidelines, which prohibit the use of the phrase \u201cIndian savages.\u201d)\n\nOne reason moderators struggle to hit their accuracy target is that for any given policy enforcement decision, they have several sources of truth to consider.\n\nThe canonical source for enforcement is Facebook\u2019s public community guidelines \u2014 which consist of two sets of documents: the publicly posted ones, and the longer internal guidelines, which offer more granular detail on complex issues. These documents are further augmented by a 15,000-word secondary document, called \u201cKnown Questions,\u201d which offers additional commentary and guidance on thorny questions of moderation \u2014 a kind of Talmud to the community guidelines\u2019 Torah. Known Questions used to occupy a single lengthy document that moderators had to cross-reference daily; last year it was incorporated into the internal community guidelines for easier searching.\n\nA third major source of truth is the discussions moderators have among themselves. During breaking news events, such as a mass shooting, moderators will try to reach a consensus on whether a graphic image meets the criteria to be deleted or marked as disturbing. But sometimes they reach the wrong consensus, moderators said, and managers have to walk the floor explaining the correct decision.\n\nThe fourth source is perhaps the most problematic: Facebook\u2019s own internal tools for distributing information. While official policy changes typically arrive every other Wednesday, incremental guidance about developing issues is distributed on a near-daily basis. Often, this guidance is posted to Workplace, the enterprise version of Facebook that the company introduced in 2016. Like Facebook itself, Workplace has an algorithmic News Feed that displays posts based on engagement. During a breaking news event, such as a mass shooting, managers will often post conflicting information about how to moderate individual pieces of content, which then appear out of chronological order on Workplace. Six current and former employees told me that they had made moderation mistakes based on seeing an outdated post at the top of their feed. At times, it feels as if Facebook\u2019s own product is working against them. The irony is not lost on the moderators.\n\nAt times, it feels as if Facebook\u2019s own product is working against them.\n\n\u201cIt happened all the time,\u201d says Diana, a former moderator. \u201cIt was horrible \u2014 one of the worst things I had to personally deal with, to do my job properly.\u201d During times of national tragedy, such as the 2017 Las Vegas shooting, managers would tell moderators to remove a video \u2014 and then, in a separate post a few hours later, to leave it up. The moderators would make a decision based on whichever post Workplace served up.\n\n\u201cIt was such a big mess,\u201d Diana says. \u201cWe\u2019re supposed to be up to par with our decision making, and it was messing up our numbers.\u201d\n\nWorkplace posts about policy changes are supplemented by occasional slide decks that are shared with Cognizant workers about special topics in moderation \u2014 often tied to grim anniversaries, such as the Parkland shooting. But these presentations and other supplementary materials often contain embarrassing errors, moderators told me. Over the past year, communications from Facebook incorrectly identified certain U.S. representatives as senators; misstated the date of an election; and gave the wrong name for the high school at which the Parkland shooting took place. (It is Marjory Stoneman Douglas High School, not \u201cStoneham Douglas High School.\u201d)\n\nEven with an ever-changing rulebook, moderators are granted only the slimmest margins of error. The job resembles a high-stakes video game in which you start out with 100 points \u2014 a perfect accuracy score \u2014 and then scratch and claw to keep as many of those points as you can. Because once you fall below 95, your job is at risk.\n\nIf a quality assurance manager marks Miguel\u2019s decision wrong, he can appeal the decision. Getting the QA to agree with you is known as \u201cgetting the point back.\u201d In the short term, an \u201cerror\u201d is whatever a QA says it is, and so moderators have good reason to appeal every time they are marked wrong. (Recently, Cognizant made it even harder to get a point back, by requiring moderators to first get a SME to approve their appeal before it would be forwarded to the QA.)\n\nSometimes, questions about confusing subjects are escalated to Facebook. But every moderator I asked about this said that Cognizant managers discourage employees from raising issues to the client, apparently out of fear that too many questions would annoy Facebook.\n\nThis has resulted in Cognizant inventing policy on the fly. When the community standards did not explicitly prohibit erotic asphyxiation, three former moderators told me, a team leader declared that images depicting choking would be permitted unless the fingers depressed the skin of the person being choked.\n\n\u201cThey would confront me in the parking lot and tell me they were going to beat the shit out of me\u201d\n\nBefore workers are fired, they are offered coaching and placed into a remedial program designed to make sure they master the policy. But often this serves as a pretext for managing workers out of the job, three former moderators told me. Other times, contractors who have missed too many points will escalate their appeals to Facebook for a final decision. But the company does not always get through the backlog of requests before the employee in question is fired, I was told.\n\nOfficially, moderators are prohibited from approaching QAs and lobbying them to reverse a decision. But it is still a regular occurrence, two former QAs told me.\n\nOne, named Randy, would sometimes return to his car at the end of a work day to find moderators waiting for him. Five or six times over the course of a year, someone would attempt to intimidate him into changing his ruling. \u201cThey would confront me in the parking lot and tell me they were going to beat the shit out of me,\u201d he says. \u201cThere wasn\u2019t even a single instance where it was respectful or nice. It was just, You audited me wrong! That was a boob! That was full areola, come on man!\u201d\n\nFearing for his safety, Randy began bringing a concealed gun to work. Fired employees regularly threatened to return to work and harm their old colleagues, and Randy believed that some of them were serious. A former coworker told me she was aware that Randy brought a gun to work, and approved of it, fearing on-site security would not be sufficient in the case of an attack.\n\nCognizant\u2019s Duncan told me the company would investigate the various safety and management issues that moderators had disclosed to me. He said bringing a gun to work was a violation of policy and that, had management been aware of it, they would have intervened and taken action against the employee.\n\nRandy quit after a year. He never had occasion to fire the gun, but his anxiety lingers.\n\n\u201cPart of the reason I left was how unsafe I felt in my own home and my own skin,\u201d he says.\n\nBefore Miguel can take a break, he clicks a browser extension to let Cognizant know he is leaving his desk. (\u201cThat\u2019s a standard thing in this type of industry,\u201d Facebook\u2019s Davidson tells me. \u201cTo be able to track, so you know where your workforce is.\u201d)\n\nMiguel is allowed two 15-minute breaks, and one 30-minute lunch. During breaks, he often finds long lines for the restrooms. Hundreds of employees share just one urinal and two stalls in the men\u2019s room, and three stalls in the women\u2019s. Cognizant eventually allowed employees to use a restroom on another floor, but getting there and back will take Miguel precious minutes. By the time he has used the restroom and fought the crowd to his locker, he might have five minutes to look at his phone before returning to his desk.\n\nMiguel is also allotted nine minutes per day of \u201cwellness time,\u201d which he is supposed to use if he feels traumatized and needs to step away from his desk. Several moderators told me that they routinely used their wellness time to go to the restroom when lines were shorter. But management eventually realized what they were doing, and ordered employees not to use wellness time to relieve themselves. (Recently a group of Facebook moderators hired through Accenture in Austin complained about \u201cinhumane\u201d conditions related to break periods; Facebook attributed the issue to a misunderstanding of its policies.)\n\nAt the Phoenix site, Muslim workers who used wellness time to perform one of their five daily prayers were told to stop the practice and do it on their other break time instead, current and former employees told me. It was unclear to the employees I spoke with why their managers did not consider prayer to be a valid use of the wellness program. (Cognizant did not offer a comment about these incidents, although a person familiar with one case told me a worker requested more than 40 minutes for daily prayer, which the company considered excessive.)\n\nCognizant employees are told to cope with the stress of the jobs by visiting counselors, when they are available; by calling a hotline; and by using an employee assistance program, which offers a handful of therapy sessions. More recently, yoga and other therapeutic activities have been added to the work week. But aside from occasional visits to the counselor, six employees I spoke with told me they found these resources inadequate. They told me they coped with the stress of the job in other ways: with sex, drugs, and offensive jokes.\n\nAmong the places that Cognizant employees have been found having sex at work: the bathroom stalls, the stairwells, the parking garage, and the room reserved for lactating mothers. In early 2018, the security team sent out a memo to managers alerting them to the behavior, a person familiar with the matter told me. The solution: management removed door locks from the mother\u2019s room and from a handful of other private rooms. (The mother\u2019s room now locks again, but would-be users must first check out a key from an administrator.)\n\nA former moderator named Sara said that the secrecy around their work, coupled with the difficulty of the job, forged strong bonds between employees. \u201cYou get really close to your coworkers really quickly,\u201d she says. \u201cIf you\u2019re not allowed to talk to your friends or family about your job, that\u2019s going to create some distance. You might feel closer to these people. It feels like an emotional connection, when in reality you\u2019re just trauma bonding.\u201d\n\nEmployees also cope using drugs and alcohol, both on and off campus. One former moderator, Li, told me he used marijuana on the job almost daily, through a vaporizer. During breaks, he says, small groups of employees often head outside and smoke. (Medical marijuana use is legal in Arizona.)\n\n\u201cI can\u2019t even tell you how many people I\u2019ve smoked with,\u201d Li says. \u201cIt\u2019s so sad, when I think back about it \u2014 it really does hurt my heart. We\u2019d go down and get stoned and go back to work. That\u2019s not professional. Knowing that the content moderators for the world\u2019s biggest social media platform are doing this on the job, while they are moderating content \u2026\u201d\n\n\u201cWe were doing something that was darkening our soul\u201d\n\nHe trailed off.\n\nLi, who worked as a moderator for about a year, was one of several employees who said the workplace was rife with pitch-black humor. Employees would compete to send each other the most racist or offensive memes, he said, in an effort to lighten the mood. As an ethnic minority, Li was a frequent target of his coworkers, and he embraced what he saw as good-natured racist jokes at his expense, he says.\n\nBut over time, he grew concerned for his mental health.\n\n\u201cWe were doing something that was darkening our soul \u2014 or whatever you call it,\u201d he says. \u201cWhat else do you do at that point? The one thing that makes us laugh is actually damaging us. I had to watch myself when I was joking around in public. I would accidentally say [offensive] things all the time \u2014 and then be like, Oh shit, I\u2019m at the grocery store. I cannot be talking like this.\u201d\n\nJokes about self-harm were also common. \u201cDrinking to forget,\u201d Sara heard a coworker once say, when the counselor asked him how he was doing. (The counselor did not invite the employee in for further discussion.) On bad days, Sara says, people would talk about it being \u201ctime to go hang out on the roof\u201d \u2014 the joke being that Cognizant employees might one day throw themselves off it.\n\nOne day, Sara said, moderators looked up from their computers to see a man standing on top of the office building next door. Most of them had watched hundreds of suicides that began just this way. The moderators got up and hurried to the windows.\n\nThe man didn\u2019t jump, though. Eventually everyone realized that he was a fellow employee, taking a break.\n\nLike most of the former moderators I spoke with, Chloe quit after about a year.\n\nAmong other things, she had grown concerned about the spread of conspiracy theories among her colleagues. One QA often discussed his belief that the Earth is flat with colleagues, and \u201cwas actively trying to recruit other people\u201d into believing, another moderator told me. One of Miguel\u2019s colleagues once referred casually to \u201cthe Holohoax,\u201d in what Miguel took as a signal that the man was a Holocaust denier.\n\nConspiracy theories were often well received on the production floor, six moderators told me. After the Parkland shooting last year, moderators were initially horrified by the attacks. But as more conspiracy content was posted to Facebook and Instagram, some of Chloe\u2019s colleagues began expressing doubts.\n\n\u201cI don\u2019t think it\u2019s possible to do the job and not come out of it with some acute stress disorder or PTSD.\u201d\n\n\u201cPeople really started to believe these posts they were supposed to be moderating,\u201d she says. \u201cThey were saying, \u2018Oh gosh, they weren\u2019t really there. Look at this CNN video of David Hogg \u2014 he\u2019s too old to be in school.\u2019 People started Googling things instead of doing their jobs and looking into conspiracy theories about them. We were like, \u2018Guys, no, this is the crazy stuff we\u2019re supposed to be moderating. What are you doing?\u2019\u201d\n\nMost of all, though, Chloe worried about the long-term impacts of the job on her mental health. Several moderators told me they experienced symptoms of secondary traumatic stress \u2014 a disorder that can result from observing firsthand trauma experienced by others. The disorder, whose symptoms can be identical to post-traumatic stress disorder, is often seen in physicians, psychotherapists, and social workers. People experiencing secondary traumatic stress report feelings of anxiety, sleep loss, loneliness, and dissociation, among other ailments.\n\nLast year, a former Facebook moderator in California sued the company, saying her job as a contractor with the firm Pro Unlimited had left her with PTSD. In the complaint, her lawyers said she \u201cseeks to protect herself from the dangers of psychological trauma resulting from Facebook\u2019s failure to provide a safe workplace for the thousands of contractors who are entrusted to provide the safest possible environment for Facebook users.\u201d (The suit is still unresolved.)\n\nChloe has experienced trauma symptoms in the months since leaving her job. She started to have a panic attack in a movie theater during the film Mother!, when a violent stabbing spree triggered a memory of that first video she moderated in front of her fellow trainees. Another time, she was sleeping on the couch when she heard machine gun fire, and had a panic attack. Someone in her house had turned on a violent TV show. She \u201cstarted freaking out,\u201d she says. \u201cI was begging them to shut it off.\u201d\n\nThe attacks make her think of her fellow trainees, especially the ones who fail out of the program before they can start. \u201cA lot of people don\u2019t actually make it through the training,\u201d she says. \u201cThey go through those four weeks and then they get fired. They could have had that same experience that I did, and had absolutely no access to counselors after that.\u201d\n\nLast week, Davidson told me, Facebook began surveying a test group of moderators to measure what the company calls their \u201cresiliency\u201d \u2014 their ability to bounce back from seeing traumatic content and continue doing their jobs. The company hopes to expand the test to all of its moderators globally, he said.\n\nRandy also left after about a year. Like Chloe, he had been traumatized by a video of a stabbing. The victim had been about his age, and he remembers hearing the man crying for his mother as he died.\n\n\u201cEvery day I see that,\u201d Randy says, \u201cI have a genuine fear over knives. I like cooking \u2014 getting back into the kitchen and being around the knives is really hard for me.\u201d\n\nThe job also changed the way he saw the world. After he saw so many videos saying that 9/11 was not a terrorist attack, he came to believe them. Conspiracy videos about the Las Vegas massacre were also very persuasive, he says, and he now believes that multiple shooters were responsible for the attack. (The FBI found that the massacre was the work of a single gunman.)\n\n\u201cThere is an infinite possibility of what\u2019s gonna be the next job, and that does create an essence of chaos\u201d\n\nRandy now sleeps with a gun at his side. He runs mental drills about how he would escape his home in the event that it were attacked. When he wakes up in the morning, he sweeps the house with his gun raised, looking for invaders.\n\nHe has recently begun seeing a new therapist, after being diagnosed with PTSD and generalized anxiety disorder.\n\n\u201cI\u2019m fucked up, man,\u201d Randy says. \u201cMy mental health \u2014 it\u2019s just so up and down. One day I can be really happy, and doing really good. The next day, I\u2019m more or less of a zombie. It\u2019s not that I\u2019m depressed. I\u2019m just stuck.\u201d\n\nHe adds: \u201cI don\u2019t think it\u2019s possible to do the job and not come out of it with some acute stress disorder or PTSD.\u201d\n\nA common complaint of the moderators I spoke with was that the on-site counselors were largely passive, relying on workers to recognize the signs of anxiety and depression and seek help.\n\n\u201cThere was nothing that they were doing for us,\u201d Li says, \u201cother than expecting us to be able to identify when we\u2019re broken. Most of the people there that are deteriorating \u2014 they don\u2019t even see it. And that\u2019s what kills me.\u201d\n\nLast week, after I told Facebook about my conversations with moderators, the company invited me to Phoenix to see the site for myself. It is the first time Facebook has allowed a reporter to visit an American content moderation site since the company began building dedicated facilities here two years ago. A spokeswoman who met me at the site says that the stories I have been told do not reflect the day-to-day experiences of most of its contractors, either at Phoenix or at its other sites around the world.\n\nThe day before I arrived at the office park where Cognizant resides, one source tells me, new motivational posters were hung up on the walls. On the whole, the space is much more colorful than I expect. A neon wall chart outlines the month\u2019s activities, which read like a cross between the activities at summer camp and a senior center: yoga, pet therapy, meditation, and a Mean Girls-inspired event called On Wednesdays We Wear Pink. The day I was there marked the end of Random Acts of Kindness Week, in which employees were encouraged to write inspirational messages on colorful cards, and attach them to a wall with a piece of candy.\n\n\u201cWhat would you do if you weren\u2019t afraid?\u201d\n\nAfter meetings with executives from Cognizant and Facebook, I interview five workers who had volunteered to speak with me. They stream into a conference room, along with the man who is responsible for running the site. With their boss sitting at their side, employees acknowledge the challenges of the job but tell me they feel safe, supported, and believe the job will lead to better-paying opportunities \u2014 within Cognizant, if not Facebook.\n\nBrad, who holds the title of policy manager, tells me that the majority of content that he and his colleagues review is essentially benign, and warns me against overstating the mental health risks of doing the job.\n\n\u201cThere\u2019s this perception that we\u2019re bombarded by these graphic images and content all the time, when in fact the opposite is the truth,\u201d says Brad, who has worked on the site for nearly two years. \u201cMost of the stuff we see is mild, very mild. It\u2019s people going on rants. It\u2019s people reporting photos or videos simply because they don\u2019t want to see it \u2014 not because there\u2019s any issue with the content. That\u2019s really the majority of the stuff that we see.\u201d\n\n\u201cIf we weren\u2019t there doing that job, Facebook would be so ugly\u201d\n\nWhen I ask about the high difficulty of applying the policy, a reviewer named Michael says that he regularly finds himself stumped by tricky decisions. \u201cThere is an infinite possibility of what\u2019s gonna be the next job, and that does create an essence of chaos,\u201d he says. \u201cBut it also keeps it interesting. You\u2019re never going to go an entire shift already knowing the answer to every question.\u201d\n\nIn any case, Michael says, he enjoys the work better than he did at his last job, at Walmart, where he was often berated by customers. \u201cI do not have people yelling in my face,\u201d he says.\n\nThe moderators stream out, and I\u2019m introduced to two counselors on the site, including the doctor who started the on-site counseling program here. Both ask me not to use their real names. They tell me that they check in with every employee every day. They say that the combination of on-site services, a hotline, and an employee assistance program are sufficient to protect workers\u2019 well-being.\n\nWhen I ask about the risks of contractors developing PTSD, a counselor I\u2019ll call Logan tells me about a different psychological phenomenon: \u201cpost-traumatic growth,\u201d an effect whereby some trauma victims emerge from the experience feeling stronger than before. The example he gives me is that of Malala Yousafzai, the women\u2019s education activist, who was shot in the head as a teenager by the Taliban.\n\n\u201cThat\u2019s an extremely traumatic event that she experienced in her life,\u201d Logan says. \u201cIt seems like she came back extremely resilient and strong. She won a Nobel Peace Prize... So there are many examples of people that experience difficult times and come back stronger than before.\u201d\n\nThe day ends with a tour, in which I walk the production floor and talk with other employees. I am struck by how young they are: almost everyone seems to be in their twenties or early thirties. All work stops while I\u2019m on the floor, to ensure I do not see any Facebook user\u2019s private information, and so employees chat amiably with their deskmates as I walk by. I take note of the posters. One, from Cognizant, bears the enigmatic slogan \u201cempathy at scale.\u201d Another, made famous by Facebook COO Sheryl Sandberg, reads \u201cWhat would you do if you weren\u2019t afraid?\u201d\n\nIt makes me think of Randy and his gun.\n\nEveryone I meet at the site expresses great care for the employees, and appears to be doing their best for them, within the context of the system they have all been plugged into. Facebook takes pride in the fact that it pays contractors at least 20 percent above minimum wage at all of its content review sites, provides full healthcare benefits, and offers mental health resources that far exceed that of the larger call center industry.\n\nAnd yet the more moderators I spoke with, the more I came to doubt the use of the call center model for content moderation. This model has long been standard across big tech companies \u2014 it\u2019s also used by Twitter and Google, and therefore YouTube. Beyond cost savings, the benefit of outsourcing is that it allows tech companies to rapidly expand their services into new markets and languages. But it also entrusts essential questions of speech and safety to people who are paid as if they were handling customer service calls for Best Buy.\n\nGot a tip for us? Use SecureDrop or Signal to securely send messages and files to The Verge without revealing your identity.\n\nEvery moderator I spoke with took great pride in their work, and talked about the job with profound seriousness. They wished only that Facebook employees would think of them as peers, and to treat them with something resembling equality.\n\n\u201cIf we weren\u2019t there doing that job, Facebook would be so ugly,\u201d Li says. \u201cWe\u2019re seeing all that stuff on their behalf. And hell yeah, we make some wrong calls. But people don\u2019t know that there\u2019s actually human beings behind those seats.\u201d\n\nThat people don\u2019t know there are human beings doing this work is, of course, by design. Facebook would rather talk about its advancements in artificial intelligence, and dangle the prospect that its reliance on human moderators will decline over time.\n\nBut given the limits of the technology, and the infinite varieties of human speech, such a day appears to be very far away. In the meantime, the call center model of content moderation is taking an ugly toll on many of its workers. As first responders on platforms with billions of users, they are performing a critical function of modern civil society, while being paid less than half as much as many others who work on the front lines. They do the work as long as they can \u2014 and when they leave, an NDA ensures that they retreat even further into the shadows.\n\nTo Facebook, it will seem as if they never worked there at all. Technically, they never did.\n\nHave you done content moderation work for a tech giant? Email Casey Newton at casey@theverge.com, send him a direct message on Twitter @CaseyNewton, or ask him for his Signal at either address.", "description": "In a damning new report, Casey Newton gives an unprecedented look at the day-to-day lives of Facebook moderators in America. His interviews with twelve current and former employees of Cognizant in Arizona reveal a workplace perpetually teetering on the brink of chaos.", "authors": ["Casey Newton", "Feb"], "top_image": "https://cdn.vox-cdn.com/thumbor/BwMxgW-XB-dayquTzD1a_T8QTSk=/0x40:2040x1108/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/14480950/VRG_3246_Social.jpg", "published_at": "2019-02-25"}