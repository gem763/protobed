{"pub": "theverge", "url": "https://theverge.com/2019/6/14/18678782/adobe-machine-learning-ai-tool-spot-fake-facial-edits-liquify-manipulations", "downloaded_at": "2019-10-05 14:24:57.363760+00:00", "title": "Adobe\u2019s prototype AI tool automatically spots Photoshopped faces", "language": "en", "text": "The world is becoming increasingly anxious about the spread of fake videos and pictures, and Adobe \u2014 a name synonymous with edited imagery \u2014 says it shares those concerns. Today, it\u2019s sharing new research in collaboration with scientists from UC Berkeley that uses machine learning to automatically detect when images of faces have been manipulated.\n\nIt\u2019s the latest sign the company is committing more resources to this problem. Last year its engineers created an AI tool that detects edited media created by splicing, cloning, and removing objects.\n\nThe algorithm spotted 99 percent of edited faces in Adobe\u2019s tests\n\nThe company says it doesn\u2019t have any immediate plans to turn this latest work into a commercial product, but a spokesperson told The Verge it was just one of many \u201cefforts across Adobe to better detect image, video, audio and document manipulations.\u201d\n\n\u201cWhile we are proud of the impact that Photoshop and Adobe\u2019s other creative tools have made on the world, we also recognize the ethical implications of our technology,\u201d said the company in a blog post. \u201cFake content is a serious and increasingly pressing issue.\u201d\n\nThe research is specifically designed to spot edits made with Photoshop\u2019s Liquify tool, which is commonly used to adjust the shape of faces and alter facial expressions. \u201cThe feature\u2019s effects can be delicate which made it an intriguing test case for detecting both drastic and subtle alterations to faces,\u201d said Adobe.\n\nTo create the software, engineers trained a neural network on a database of paired faces, containing images both before and after they\u2019d been edited using Liquify.\n\nThe resulting algorithm is impressively effective. When asked to spot a sample of edited faces, human volunteers got the right answer 53 percent of the time, while the algorithm was correct 99 percent of the time. The tool is even able to suggest how to restore a photo to its original, unedited appearance, though these results are often mixed.\n\n\u201cwe live in a world where it\u2019s becoming harder to trust the digital information we consume\u201d\n\n\u201cThe idea of a magic universal \u2018undo\u2019 button to revert image edits is still far from reality,\u201d Adobe researcher Richard Zhang, who helped conduct the work, said in a company blog post. \u201cBut we live in a world where it\u2019s becoming harder to trust the digital information we consume, and I look forward to further exploring this area of research.\u201d\n\nThe researchers said the work was the first of its kind designed to spot these sort of facial edits, and constitutes an \u201cimportant step\u201d toward creating tools that can identify complex changes including \u201cbody manipulations and photometric edits such as skin smoothing.\u201d\n\nWhile the research is promising, tools like this are no silver bullet for stopping the harmful effects of manipulated media. As we\u2019ve seen with the spread of fake news, even if content is obviously false or can be quickly debunked, it will still be shared and embraced on social media. Knowing something is fake is only half the battle, but at least it\u2019s a start.", "description": "The world is becoming increasingly anxious about the spread of fake videos and pictures, and Adobe \u2014 a name synonymous with edited imagery \u2014 says it shares those concerns. It\u2019s released new research that uses machine learning to automatically detect when images of faces have been manipulated.", "authors": ["James Vincent", "Jun"], "top_image": "https://cdn.vox-cdn.com/thumbor/ghR3mYfr_hshP6XoKoShIkiDBE0=/0x57:1826x1013/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/16344206/adobe_manipulated_faces.jpg", "published_at": "2019-06-14"}