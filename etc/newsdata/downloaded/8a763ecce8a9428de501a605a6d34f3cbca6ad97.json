{"pub": "theverge", "url": "https://theverge.com/2019/10/25/20932879/facebook-ai-facial-recognition-live-video-de-identification-deepfakes", "downloaded_at": "2019-10-25 23:38:03.982068+00:00", "title": "Facebook trained AI to fool facial recognition systems, and it works on live video", "language": "en", "text": "Facebook remains embroiled in a multibillion-dollar judgement lawsuit over its facial recognition practices, but that hasn\u2019t stopped its artificial intelligence research division from developing technology to combat the very misdeeds of which the company is accused. According to VentureBeat, Facebook AI Research (FAIR) has developed a state-of-the-art \u201cde-identification\u201d system that works on video, including even live video. It works by altering key facial features of a video subject in real time using machine learning, to trick a facial recognition system into improperly identifying the subject.\n\nThis de-identification technology has existed in the past and there are entire companies, like Israeli AI and privacy firm D-ID, dedicated to providing it for still images. There\u2019s also a whole category of facial recognition fooling imagery you can wear yourself, called adversarial examples, that work by exploiting weaknesses in how computer vision software has been trained to identify certain characteristics. Take for instance this pair of sunglasses with an adversarial pattern printed onto it that can make a facial recognition system think you\u2019re actress Milla Jovovich.\n\nAI software can trick facial recognition systems by altering key facial features\n\nBut that type of thwarting of facial recognition usually means altering a photograph or a still image captured from a security camera or some other source after the fact. Or in the case of adversarial examples, preemptively setting out to fool the system. Facebook\u2019s research supposedly does similar work in real time and on video footage, both pre-captured and live. That\u2019s a first for the industry, FAIR claims, and good enough to combat sophisticated facial recognition systems. You can see an example of it in action in this YouTube video, which, because it\u2019s de-listed, cannot be embedded elsewhere.\n\n\u201cFace recognition can lead to loss of privacy and face replacement technology may be misused to create misleading videos,\u201d reads the paper explaining the company\u2019s approach, as cited by VentureBeat. \u201cRecent world events concerning the advances in, and abuse of face recognition technology invoke the need to understand methods that successfully deal with de-identification. Our contribution is the only one suitable for video, including live video, and presents quality that far surpasses the literature methods.\u201d\n\nFacebook apparently does not intend to make use of this technology in any of its commercial products, VentureBeat reports. But the research may influence future tools developed to protect individuals\u2019 privacy and, as the research highlights with \u201cmisleading videos,\u201d prevent someone\u2019s likeness from being used in video deepfakes.\n\nThe AI industry is currently working on ways to combat the spread of deepfakes and the increasingly sophisticated tools used to create them. This is one method, and both lawmakers and tech companies are trying to come up with other tools, like deepfake detection software, and regulatory frameworks for how to control the spread of fake videos, images, and audio.\n\nThe other concern FAIR\u2019s research addresses is facial recognition, which is also unregulated and causing concern among lawmakers, academics, and activists who fear it may violate human rights if it continues to be deployed without oversight by law enforcement, governments, and corporations.", "description": "According to VentureBeat, Facebook AI Research (FAIR) has developed a state-of-the-art \"de-identification\" system that works on video, including even live video. It works by altering key facial features of a video subject in real time using machine learning, to trick a facial recognition system into improperly identifying the subject.", "authors": ["Nick Statt", "Oct"], "top_image": "https://cdn.vox-cdn.com/thumbor/zO07gWMc6K2CKH6Ri_Xjl_9k00U=/0x146:2040x1214/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/11900071/acastro_180730_1777_facial_recognition_0002.jpg", "published_at": "2019-10-25"}