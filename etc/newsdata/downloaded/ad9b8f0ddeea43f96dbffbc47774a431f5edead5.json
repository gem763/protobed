{"pub": "washingtonpost", "url": "https://washingtonpost.com/technology/2019/10/04/rashida-tlaib-isnt-only-one-who-thinks-race-biases-facial-recognition-results", "downloaded_at": "2019-10-04 23:54:32.173046+00:00", "title": "Rashida Tlaib isn\u2019t the only one who thinks race biases facial recognition results", "language": "en", "text": "\n\nRep. Rashida Tlaib (D-Mich.) touched off controversy by suggesting the Detroit Police Department should let only African Americans analyze facial recognition results. (Alex Edelman/Bloomberg)\n\nWhen Rep. Rashida Tlaib (D-Mich.) was invited to tour the Detroit Police Department\u2019s Real Time Crime Center, the purpose was to explain how officers use facial recognition when policing the streets of a city that is more than 80 percent black.\n\nBut the meeting quickly deteriorated when Tlaib told Chief James Craig that \u201canalysts need to be African Americans, not people that are not,\u201d because \u201cnon-African Americans think African Americans all look the same.\u201d\n\nCraig, who is African American, said the suggestion that white analysts would be less adept at their jobs than people of color was \u201cinsulting.\u201d\n\nTlaib\u2019s comments, however, were consistent with an enduring debate that rages around facial recognition software: the systems more accurately identify lighter-skinned faces than they do people of color. Researchers and numerous studies argue that\u2019s because the software is trained on vast sets of images that skew heavily toward white men, leaving women and minorities vulnerable to holes in mammoth databases.\n\nThat can be especially risky, critics argue, as facial recognition is embraced by government and law enforcement.\n\nCritics also worry that people aren\u2019t being trained adequately in how to use the technology and interpret its results. Researchers say that law enforcement agencies don\u2019t always disclose how its analysts are taught to use the systems, or who is conducting the training. And they worry that even if a department claims a strong training protocol, people will inevitably let biases about gender and race creep into how they assess a match.\n\n\u201cThere\u2019s a huge amount of reliance that this is going to be accurate if it spits out a match, or a candidate list of five people,\u201d said Jake Laperruque, senior counsel at The Constitution Project at the Project on Government Oversight. \u201cAnd that\u2019s just not the case.\u201d\n\n[Amazon facial-identification software used by police falls short on tests for accuracy and bias, new research finds]\n\nCamera quality, lighting and the size of a system\u2019s database can all affect facial recognition\u2019s accuracy. But researchers argue that improving those factors doesn\u2019t erase a system\u2019s hard-wired biases. One 2018 study conducted by Joy Buolamwini of the M.I.T. Media Lab found that the technology is correct 99 percent of the time with photos of white men. But the software misidentified the gender as often as 35 percent of the time when viewing an image of a darker-skinned woman.\n\nIn January, researchers with M.I.T. Media Lab reported that facial-recognition software developed by Amazon and marketed to local and federal law enforcement also fell short on basic accuracy tests, including correctly identifying a person\u2019s gender. Specifically, Amazon\u2019s Rekognition system was perfect in predicting the gender of lighter-skinned men, the researchers said, but misidentified the gender of darker-skinned women in roughly 30 percent of their tests.\n\nAmazon disputed those findings, saying the research used algorithms that work differently from the facial-recognition systems used by police departments. (Amazon founder and chief executive Jeff Bezos owns The Washington Post.)\n\nBut the results, researchers argue, offer a cautionary tale for millions of Americans. A 2016 report by Georgetown Law researchers found that the facial images of half of all American adults, or more than 117 million people, were accessible in a law-enforcement facial-recognition database.\n\nGreater scrutiny on these databases has spurred some progress. ImageNet, an online image database, recently said it would remove 600,000 pictures of people from its system after an art project showed the severity of the bias wired into its artificial intelligence. Artist Trevor Paglen and AI researcher Kate Crawford showed how the system could generate derogatory results when people uploaded photos of themselves. A woman might be called a \u201cslut,\u201d for example, and an African American user could be labeled a \u201cwrongdoer\" or with a racial epithet.\n\n[Facial recognition technology is finally more accurate in identifying people of color. Could that be used against immigrants?]\n\nUnlike many social and policy debates gripping Washington, facial-recognition has drawn sharp criticism from Republican and Democratic lawmakers alike. In May, members of the House Oversight and Reform Committee jointly condemned the technology, charging that it was inaccurate and threatened Americans\u2019 privacy and freedom of expression. But there are no current federal rules governing artificial intelligence or facial recognition software.\n\n\u201cWe have a technology that was created and designed by one demographic, that is only mostly effective on that one demographic, and they\u2019re trying to sell it and impose it on the entirety of the country,\u201d Rep. Alexandria Ocasio-Cortez (D-N.Y.) said earlier this year.\n\nDetroit\u2019s police board approved the use of facial recognition software last month. But the technology has not been embraced by all locales. San Francisco and Oakland, Calif., along with Somerville, Mass., have banned local government agencies, including police departments, from using the software. In September, California lawmakers temporarily banned state and local law enforcement from using facial-recognition software in body cameras.\n\nBeyond the software itself, critics worry that users will put too much faith in facial recognition, even as they acknowledge the software\u2019s pitfalls. Laperruque pointed to the \u201cCSI Effect\u201d \u2014 when people come to believe in the technology\u2019s infallibility because of how they see it used in a crime shows on TV.\n\n\u201cTraining in general is seen as a pretty essential feature of getting this to work,\" Laperruque said. \u201cThere\u2019s an expectation when law enforcement and the public see a new sci-fi-looking tool to say, \u2018This is a magical, futuristic technology,\u2019 when in reality, facial recognition is a lot more akin to outsourcing policing to glitchy computers.\"\n\nJennifer Lynch, surveillance litigation director of the Electronic Frontier Foundation, pointed to studies showing how poorly people identify images of people they don\u2019t know \u2014 especially when it comes to people of different races or ethnicity.\n\nResearchers argue that among police departments that use the software, there aren\u2019t always clear or transparent standards for how officials are trained on the systems, or how much weight is given to the results.\n\n\u201cThe police departments say, \u2018we are not considering this an exact match because we have humans that look at this after the fact and verify the technology,\u2019\u201d Lynch said, \u201cwhich is problematic because humans are not good at identifying people.\u201d\n\n[Microsoft calls for regulation of facial recognition, saying it\u2019s too risky to leave to tech industry alone]\n\nThe back and forth between Tlaib and Craig was tense, The Detroit News reported. Tlaib described seeing people on the House floor misidentify longtime Democratic congressmen John Lewis and Elijah Cummings, both of whom are black.\n\nBut Craig said that the department had \u201ca diverse group of crime analysts\u201d and that Tlaib\u2019s criticism was \"a slap in the face to all the men and women in the crime center.\u201d\n\nSpeaking to a local news channel, Tlaib said she stood by her comments \u201cthat facial recognition technology is broken.\" Tlaib said that as an elected official, her job was to make sure residents \u201care not going to be misidentified and detained or falsely arrested because [Craig] is using broken technology.\u201d\n\nTlaib\u2019s office did not respond to multiple requests for comment by The Post. Tlaib is a sponsor of a House bill that would ban facial and biometric recognition in public housing, plus another that would bar federal funds from being used to buy or use the technology.\n\nThe Detroit Police Department did not respond to multiple requests for comment, including about how the department trains its analysts who use facial recognition technology.\n\nBut Craig said his trust was in the analysts \u201cwho are trained, regardless of race, regardless of gender. It\u2019s about the training.\u201d In his meeting with Tlaib, Craig emphasized that \u201ca match is a tool only.\"\n\n\u201cAs a police chief who happens to be African American in this city, if I made a similar statement, people would be calling for my resignation right now,\u201d Craig told a local news station. \u201cSo is that a double standard? That\u2019s the number one question I have.\u201d", "description": "The congresswoman sparred with the chief of the Detroit police department, saying the force should only hire black analysts who work with facial recognition technology.", "authors": ["Rachel Siegel", "National Business Reporter", "October At Pm", "Rachel Siegel Is A National Business Reporter. She Previously Contributed To The Post'S Metro Desk", "The Marshall Project", "The Dallas Morning News."], "top_image": "https://www.washingtonpost.com/resizer/8iAQTvyNNzzPILJC1jB_uuA1UXA=/1484x0/arc-anglerfish-washpost-prod-washpost.s3.amazonaws.com/public/2IK5TDXATAI6TPT7JTEFAF6DN4.jpg", "published_at": "2019-10-04"}