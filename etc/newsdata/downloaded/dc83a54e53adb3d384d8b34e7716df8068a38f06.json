{"pub": "theverge", "url": "https://theverge.com/2019/10/1/20893516/apple-deep-fusion-camera-mode-iphone-11-pro-max-ios-13-beta", "downloaded_at": "2019-10-05 14:09:54.259066+00:00", "title": "The iPhone 11\u2019s Deep Fusion camera arrives with iOS 13.2 developer beta", "language": "en", "text": "Apple\u2019s Deep Fusion photography system has arrived as part of Apple\u2019s newest developer beta of iOS 13, version 13.2 beta 1, hopefully hinting that it will ship for the iPhone 11 and 11 Pro some time soon.\n\nUpdate October 2nd, 1:32PM ET: This article originally said the next iOS 13 developer beta would be released on October 1st, before Apple clarified an uncertain ship date for that beta. The software has since been released as part of today\u2019s iOS 13.2 beta 1. Deep Fusion is now available for developers to try, while a public beta is expected soon.\n\nTo refresh your memory, Deep Fusion is a new image processing pipeline for medium-light images, which Apple senior VP Phil Schiller called \u201ccomputational photography mad science\u201d when he introduced it onstage. But like much of iOS 13, Deep Fusion wasn\u2019t ready when the phones arrived two weeks ago. And although the iPhone 11 and 11 Pro have extremely impressive cameras, Deep Fusion\u2019s is meant to offer a massive step forward in indoor and medium-lighting situations. And since so many photos are taken indoors and in medium light, we\u2019re looking forward to testing it. Here\u2019s a sample shot shared by Apple:\n\nWith Deep Fusion, the iPhone 11 and 11 Pro cameras will have three modes of operation that automatically kick in based on light levels and the lens you\u2019re using:\n\nThe standard wide angle lens will use Apple\u2019s enhanced Smart HDR for bright to medium-light scenes, with Deep Fusion kicking in for medium to low light, and Night mode coming on for dark scenes.\n\nThe tele lens will mostly use Deep Fusion, with Smart HDR only taking over for very bright scenes. (Night mode always uses the standard wide angle lens, even when the camera app shows \u201c2x\u201d.)\n\nThe ultrawide will always use Smart HDR, as it does not support either Deep Fusion or Night mode.\n\nUnlike Night mode, which has an indicator on-screen and can be turned off, Deep Fusion is totally invisible to the user. There\u2019s no indicator in the camera app or in the photo roll, and it doesn\u2019t show up in the EXIF data. Apple tells me that is very much intentional, as it doesn\u2019t want people to think about how to get the best photo. The idea is that the camera will just sort it out for you.\n\nBut in the background, Deep Fusion is doing quite a lot of work and operating much differently than Smart HDR. Here\u2019s the basic breakdown:\n\nBy the time you press the shutter button, the camera has already grabbed four frames at a fast shutter speed to freeze motion in the shot and four standard frames. When you press the shutter it grabs one longer-exposure shot to capture detail. Those three regular shots and long-exposure shot are merged into what Apple calls a \u201csynthetic long.\u201d This is a major difference from Smart HDR. Deep Fusion picks the short-exposure image with the most detail and merges it with the synthetic long exposure. Unlike Smart HDR, Deep Fusion merges these two frames, not more \u2014 although the synthetic long is already made of four previously-merged frames. All the component frames are also processed for noise differently than Smart HDR, in a way that\u2019s better for Deep Fusion. The images are run through four detail processing steps, pixel by pixel, each tailored to increasing amounts of detail \u2014 the sky and walls are in the lowest band, while skin, hair, fabrics, and so on are the highest level. This generates a series of weightings for how to blend the two images \u2014 taking detail from one and tone, color, and luminance from the other. The final image is generated.\n\nThat all takes a tick longer than a normal Smart HDR image \u2014 somewhere around a second total. So if you take a bunch of shots and jump immediately into the camera roll, you\u2019ll first see a proxy image while Deep Fusion runs in the background, and then it\u2019ll pop to the final version with more detail, a process Apple says shouldn\u2019t take more than a quarter to half a second by the time switch to the camera roll.\n\nBut all of this means that Deep Fusion won\u2019t work in burst mode. You\u2019ll notice burst mode itself has been deemphasized throughout the camera app in iOS 13 since all of these new modes require the camera to take multiple exposures and merge them, and Apple\u2019s new hold-to-take video mode is a little more useful anyway.\n\nHere\u2019s another Deep Fusion image of a beautiful person in a sweater from Apple. It\u2019s certainly impressive. But we\u2019ll have to see how Deep Fusion works in practice as people get their hands on it with the developer beta. If it\u2019s as impressive as Apple claims, the iPhone 11 camera will leap even further ahead of the current competition and set a high bar for Google\u2019s upcoming Pixel 4 to clear.", "description": "The Deep Fusion \"computational photography mad science\" camera tech is now in the iOS 13 developer beta.", "authors": ["Nilay Patel", "Cameron Faulkner", "Oct"], "top_image": "https://cdn.vox-cdn.com/thumbor/LV1esy8NHQLBiD6rHPklV1sin3c=/0x238:2040x1306/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/19206398/akrales_190914_3666_0154.jpg", "published_at": "2019-10-01"}