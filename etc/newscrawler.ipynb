{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\infomax\\Documents\\GitHub\\protobed\\venv\\lib\\site-packages\\tqdm\\autonotebook\\__init__.py:18: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n",
      "C:\\Users\\infomax\\Documents\\GitHub\\protobed\\venv\\lib\\site-packages\\tqdm\\_tqdm.py:604: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "parentdir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parentdir)\n",
    "\n",
    "import django\n",
    "django.setup()\n",
    "\n",
    "from data.models import Newsdata\n",
    "import glob\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(text, remove_url=True, remove_mention=True, remove_hashtag=False):\n",
    "    LINEBREAK = r'\\n' # str.replace에서는 r'\\n'으로 검색이 안된다\n",
    "    RT = '((?: rt)|(?:^rt))[^ @]?'\n",
    "    EMOJI = r'[\\U00010000-\\U0010ffff]'\n",
    "    DOTS = '…'\n",
    "    LONG_BLANK = r'[ ]+'\n",
    "    SPECIALS = r'([^ a-zA-Z0-9_\\u3131-\\u3163\\uac00-\\ud7a3]+)|([ㄱ-ㅣ]+)'\n",
    "    PUNC = r'[{}]'.format(string.punctuation)\n",
    "    \n",
    "    # \\u3131-\\u3163\\uac00-\\ud7a3 는 한글을 의미함\n",
    "    # URL = r'(?P<url>(https?://)?(www[.])?[^ \\u3131-\\u3163\\uac00-\\ud7a3]+[.][a-z]{2,6}\\b([^ \\u3131-\\u3163\\uac00-\\ud7a3]*))'\n",
    "    URL1 = r'(?:https?:\\/\\/)?(?:www[.])?[^ :\\u3131-\\u3163\\uac00-\\ud7a3]+[.][a-z]{2,6}\\b(?:[^ \\u3131-\\u3163\\uac00-\\ud7a3]*)'\n",
    "    URL2 = r'pic.twitter.com/[a-zA-Z0-9_]+'\n",
    "    URL = '|'.join((URL1, URL2))\n",
    "    \n",
    "    HASHTAG = r'#(?P<inner_hashtag>[^ #@]+)'\n",
    "    MENTION = r'@(?P<inner_mention>[^ #@]+)' \n",
    "    \n",
    "    #PTNS = '|'.join((LINEBREAK, RT, URL, HASHTAG, MENTION, EMOJI))\n",
    "    \n",
    "    #out = {}\n",
    "    #text = re.sub('|'.join((LINEBREAK, RT, EMOJI, DOTS, SPECIALS)), '', text.lower())\n",
    "    text = text.lower()\n",
    "    \n",
    "    if remove_url:\n",
    "        text = re.sub(URL, ' ', text)\n",
    "\n",
    "    if remove_mention:\n",
    "        text = re.sub(MENTION, ' ', text)        \n",
    "    else:\n",
    "        text = re.sub(MENTION, ' \\g<inner_mention>', text)\n",
    "        \n",
    "    if remove_hashtag:\n",
    "        text = re.sub(HASHTAG, ' ', text)\n",
    "    else:\n",
    "        text = re.sub(HASHTAG, ' \\g<inner_hashtag>', text)\n",
    "        \n",
    "    text = re.sub('|'.join((LINEBREAK, RT, EMOJI, DOTS, SPECIALS, PUNC)), ' ', text)\n",
    "    return re.sub(LONG_BLANK, ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS; len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7441"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = glob.glob('newsdata/downloaded/*.json'); len(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, {'data.Newsdata': 32})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Newsdata.objects.all().delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07451729028d4a358a23f5fb70d62a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7441), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for i, fname in enumerate(tqdm(fnames[:])):\n",
    "    with open(fname, encoding='UTF-8-sig') as f:\n",
    "        content = json.load(f)\n",
    "        doc = nlp(preproc(content['text']))\n",
    "        \n",
    "        obj = Newsdata()\n",
    "        obj.id = i + 1\n",
    "        obj.pub = content['pub']\n",
    "        obj.authors = ','.join(content['authors'])\n",
    "        obj.title = content['title']\n",
    "        obj.published_at = pd.Timestamp(content['published_at'], tz='utc')\n",
    "        obj.downloaded_at = pd.Timestamp(content['downloaded_at'], tz='utc')\n",
    "        obj.text = content['text']\n",
    "        obj.description = content['description']\n",
    "        obj.top_image = content['top_image']\n",
    "        obj.url = content['url']\n",
    "        obj.words = ','.join([str(noun) for noun in doc.noun_chunks if str(noun) not in stopwords])\n",
    "        obj.language = content['language']\n",
    "        obj.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba4ceaadc3744c091926d13df1a3384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7441), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for obj in tqdm(Newsdata.objects.all()):\n",
    "    obj.words = json.dumps(obj.words.split(','))\n",
    "    obj.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://marketwatch.com/story/labor-day-which-markets-are-closed-2019-08-30?mod=mw_theo_homepage&mod=mw_theo_homepage&mod=mw_theo_homepage\n",
      "https://marketwatch.com/story/home-improvement-stocks-mixed-as-hurricane-expected-to-both-help-and-hurt-2019-08-30?mod=mw_theo_homepage&mod=mw_theo_homepage\n",
      "https://marketwatch.com/story/here-are-2019s-biggest-stock-market-winners-and-losers-in-the-dow-sp-500-and-nasdaq-2019-08-29?mod=mw_theo_homepage&mod=mw_theo_homepage\n",
      "https://marketwatch.com/story/heres-how-the-stocks-held-by-warren-buffetts-berkshire-hathaway-have-performed-in-2019-2019-08-29?mod=mw_theo_homepage&mod=mw_theo_homepage\n"
     ]
    }
   ],
   "source": [
    "for obj in Newsdata.objects.filter(pub='marketwatch', url__contains='&'):\n",
    "    print(obj.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Newsdata.objects.filter(pub='reuters', url__contains='?').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<QuerySet [<Newsdata: [2019-09-02 | marketwatch] Is It Time to Buy Stocks Again? One Indicator Says So.>, <Newsdata: [2019-09-02 | marketwatch] Is It Time to Buy Stocks Again? One Indicator Says So.>, <Newsdata: [2019-09-02 | marketwatch] Is It Time to Buy Stocks Again? One Indicator Says So.>, <Newsdata: [2019-09-02 | wsj] Is It Time to Buy Stocks Again? One Indicator Says So.>]>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Newsdata.objects.filter(title='Is It Time to Buy Stocks Again? One Indicator Says So.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc79cb9602f4061ad8b7fdda269bb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11086), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "tqdm.pandas()\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "fnames = glob.glob('newsdata/downloaded/*.json')\n",
    "ext = '.json'\n",
    "\n",
    "fnames_to_remove = []\n",
    "\n",
    "for fname in tqdm(fnames):\n",
    "    \n",
    "    with open(fname, encoding='UTF-8-sig') as f:\n",
    "        content = json.load(f)\n",
    "        pub = content['pub']\n",
    "        url = content['url']\n",
    "        \n",
    "        if (pub=='reuters' and '?il=0' in url) or (pub=='marketwatch' and '?mod=' in url):\n",
    "            _url = clean_url(pub, url)\n",
    "            hash_url = hashlib.sha1(_url.encode('utf-8')).hexdigest()\n",
    "            file = os.path.join('newsdata/downloaded', hash_url + ext)\n",
    "            if file in fnames:\n",
    "                fnames_to_remove.append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(pub, url):\n",
    "    url = url.replace('http://', 'https://')\n",
    "    \n",
    "    if pub!='zdnet':\n",
    "    # zdnet은 반드시 www가 붙어야되는 듯 (2019.08.30)\n",
    "        url = url.replace('https://www.', 'https://')\n",
    "    \n",
    "    if pub!='thinkprogress' and url[-1]=='/':\n",
    "    # thinkprogress는 뒤의 /가 반드시 필요한 듯 (2019.09.04)\n",
    "        url = url[:-1]\n",
    "    \n",
    "    if pub=='reuters':\n",
    "    # reuters는 뒤에 의미없이 ?il=0 이 붙는 경우가 허다. 무슨뜬인지는 모름 (2019.09.04)\n",
    "        try: url = url[:url.index('?il=0')]\n",
    "        except: pass\n",
    "    \n",
    "    if pub=='marketwatch':\n",
    "        try: url = url[:url.index('?mod=')]\n",
    "        except: pass        \n",
    "    \n",
    "    if pub=='wsj':\n",
    "    # wsj paywall 뚫기\n",
    "        try: url = url[:url.index('?mod=')]\n",
    "        except: pass\n",
    "        \n",
    "        url += '?mod=rsswn'\n",
    "        \n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[os.remove(ff) for ff in fnames_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
