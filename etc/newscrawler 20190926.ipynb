{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "from newspaper import Article, Config\n",
    "from newsplease import NewsPlease\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython.display import display\n",
    "import extraction\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "import glob\n",
    "import pandas as pd\n",
    "import news_publishers\n",
    "from langdetect import detect\n",
    "from pubtime_extractor import extractArticlePublishedDate\n",
    "import asyncio\n",
    "from functools import partial\n",
    "\n",
    "# Suppress UnknownTimezoneWarning\n",
    "import warnings\n",
    "from dateutil.parser import UnknownTimezoneWarning\n",
    "warnings.filterwarnings('ignore', category=UnknownTimezoneWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _config():\n",
    "    config = Config()\n",
    "    config.fetch_images = False\n",
    "    config.memoize_articles = False\n",
    "    config.request_timeout = 10\n",
    "    config.language = 'en'\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(pub, url):\n",
    "    url = url.replace('http://', 'https://')\n",
    "\n",
    "    try: url = url[:url.index('#')]\n",
    "    except: pass\n",
    "\n",
    "    try: url = url[:url.index('\\n')]\n",
    "    except: pass    \n",
    "    \n",
    "    if pub!='zdnet':\n",
    "    # zdnet은 반드시 www가 붙어야되는 듯 (2019.08.30)\n",
    "        url = url.replace('https://www.', 'https://')\n",
    "    \n",
    "    if pub!='thinkprogress' and url[-1]=='/':\n",
    "    # thinkprogress는 뒤의 /가 반드시 필요한 듯 (2019.09.04)\n",
    "        url = url[:-1]\n",
    "    \n",
    "    if pub=='reuters':\n",
    "    # reuters는 뒤에 의미없이 ?il=0 이 붙는 경우가 허다. 무슨뜬인지는 모름 (2019.09.04)\n",
    "        try: url = url[:url.index('?il=0')]\n",
    "        except: pass\n",
    "    \n",
    "    if pub=='marketwatch':\n",
    "        try: url = url[:url.index('?mod=')]\n",
    "        except: pass\n",
    "    \n",
    "    if pub=='wsj':\n",
    "    # wsj paywall 뚫기\n",
    "        try: url = url[:url.index('?mod=')]\n",
    "        except: pass\n",
    "        \n",
    "        url += '?mod=rsswn'\n",
    "        \n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Progressor:\n",
    "    def __init__(self, ntotal, formater_suffix=None):\n",
    "        self.start = time.time()\n",
    "        self.n_total = ntotal\n",
    "        self.n_progressed = 0\n",
    "        self.formater = '\\r{pct:.2f}% ({timestamp:.2f} seconds)'\n",
    "\n",
    "        if formater_suffix:\n",
    "            self.formater += (': ' + formater_suffix)\n",
    "\n",
    "    def stamp(self, **vargs):\n",
    "        self.n_progressed += 1\n",
    "        pct = self.n_progressed / self.n_total * 100\n",
    "        timestamp = time.time() - self.start\n",
    "        print(self.formater.format(pct=pct, timestamp=timestamp, **vargs), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_urls(src):\n",
    "    prg = Progressor(len(src), formater_suffix='URLs collecting... {pub:<20}')\n",
    "    newspaper_config = partial(newspaper.build, config=_config())\n",
    "        \n",
    "    async def geturls(pub, domain):\n",
    "        resp = await loop.run_in_executor(None, newspaper_config, domain)\n",
    "        articles = resp.articles\n",
    "        urls = {clean_url(pub, article.url) for article in articles}\n",
    "        #progress(pub)\n",
    "        prg.stamp(pub=pub)\n",
    "        return pub, urls\n",
    "\n",
    "\n",
    "    async def main():\n",
    "        fts = [asyncio.ensure_future(geturls(pub, val['domain'])) for pub, val in src.items()]\n",
    "        return await asyncio.gather(*fts)\n",
    "\n",
    "\n",
    "    result = None\n",
    "    asyncio.set_event_loop(asyncio.new_event_loop())\n",
    "    loop = asyncio.get_event_loop()\n",
    "\n",
    "    try:\n",
    "        # 다음 코드를 주피터에서 돌리려면, tornado를 downgrade 해야함\n",
    "        # pip install tornado==4.5.3\n",
    "        result = loop.run_until_complete(main())\n",
    "        result = dict(result) #set.union(*result)\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "    finally:\n",
    "        loop.close()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_publish_time(article):\n",
    "    pubtime = article.publish_date\n",
    "    url = article.url\n",
    "    datesrc = 'newspaper'\n",
    "    \n",
    "    # articleDateExtractor 내부의 print 를 suppress 하기 위한 장치\n",
    "    # --- articleDateExtractor 코드 자체를 변경함 (2019.09.05)\n",
    "    #class HiddenPrints:\n",
    "    #    def __enter__(self):\n",
    "    #        self._original_stdout = sys.stdout\n",
    "    #        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    #    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "    #        sys.stdout.close()\n",
    "    #        sys.stdout = self._original_stdout\n",
    "        \n",
    "    \n",
    "    def _from_extractor(url):\n",
    "        try:\n",
    "            return extractArticlePublishedDate(url)\n",
    "        \n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    def _from_newsplease(url):\n",
    "        try:\n",
    "            return NewsPlease.from_url(url).date_publish\n",
    "        \n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    def _timize(time):\n",
    "        if time is None:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            time = pd.Timestamp(time)\n",
    "            \n",
    "            if time.tz is None:\n",
    "                return time.tz_localize('utc')\n",
    "            \n",
    "            else:\n",
    "                return time.tz_convert('utc')\n",
    "        \n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    \n",
    "    def _datize(time):\n",
    "        if time is None:\n",
    "            return ''\n",
    "\n",
    "        try:\n",
    "            time = pd.Timestamp(time)\n",
    "\n",
    "            if time.tz is None:\n",
    "                return str(time.date())\n",
    "\n",
    "            else:\n",
    "                return str(time.tz_convert('utc').date())\n",
    "            \n",
    "        except:\n",
    "            return ''\n",
    "            \n",
    "\n",
    "    if pubtime is None:\n",
    "        datesrc = 'extractor'\n",
    "        pubtime = _from_extractor(url)\n",
    "\n",
    "        if pubtime is None:\n",
    "            datesrc = 'newsplease'\n",
    "            pubtime = _from_newsplease(url)\n",
    "\n",
    "            if pubtime is None:\n",
    "                datesrc = 'fail'\n",
    "\n",
    "    return _timize(pubtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_urls(urls):\n",
    "    prg = Progressor(len(urls), formater_suffix='URLs selecting... {pub:<20}')\n",
    "    selected = {}\n",
    "    basedir = os.path.join(os.getcwd(), 'newsdata')\n",
    "    ext = '.json'\n",
    "\n",
    "    for pub, _urls in urls.items():\n",
    "        selected[pub] = set()\n",
    "\n",
    "        for _url in _urls:\n",
    "            hash_url = hashlib.sha1(_url.encode('utf-8')).hexdigest()\n",
    "            \n",
    "            file_in_saved = os.path.join(basedir, 'saved', hash_url[0], hash_url + ext)\n",
    "            file_in_downloaded = os.path.join(basedir, 'downloaded', hash_url + ext)\n",
    "            file_in_trashed = os.path.join(basedir, 'trashed', hash_url[0], hash_url + ext)\n",
    "\n",
    "            if os.path.isfile(file_in_saved) or os.path.isfile(file_in_downloaded) or os.path.isfile(file_in_trashed):\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                selected[pub].add(_url)\n",
    "                \n",
    "        prg.stamp(pub=pub)\n",
    "                \n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(urls):\n",
    "    n_total = sum([len(v) for _,v in urls.items()])\n",
    "    prg = Progressor(n_total, formater_suffix='downloading... {pub:<20}')\n",
    "    # prg = Progressor(len(set.union(*urls.values())), formater_suffix='downloading... {pub:<20}')\n",
    "    basedir = os.path.join(os.getcwd(), 'newsdata')\n",
    "    ext = '.json'\n",
    "    newspaper_config = _config()\n",
    "    \n",
    "        \n",
    "    def makedir_if_not_exists(file):\n",
    "        _dir = os.path.dirname(file)\n",
    "        \n",
    "        if not os.path.isdir(_dir):\n",
    "            os.makedirs(_dir)\n",
    "            \n",
    "            \n",
    "    def detect_lang(article):\n",
    "        lang = article.meta_lang\n",
    "        \n",
    "        if lang=='':\n",
    "            return detect(article.text)\n",
    "        \n",
    "        else:\n",
    "            return lang\n",
    "            \n",
    "    \n",
    "    def get_article(url):\n",
    "        article = Article(url, config=newspaper_config)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article\n",
    "        \n",
    "        \n",
    "    def get_title(article):\n",
    "        if article.title in ['', '-', None]:\n",
    "        # '':cbc, '-':townhall\n",
    "            html = requests.get(article.url).text\n",
    "            extracted_title = extraction.Extractor().extract(html, source_url=article.url).title\n",
    "            \n",
    "            if extracted_title in ['', '-', None]:\n",
    "                if article.description=='':\n",
    "                    return article.pub\n",
    "                else:\n",
    "                    return article.description\n",
    "                \n",
    "            else:\n",
    "                return extracted_title\n",
    "            \n",
    "        else:\n",
    "            return article.title\n",
    "        \n",
    "    \n",
    "    async def _download(pub, _urls):\n",
    "        out = {'downloaded':set(), 'trashed':set()}\n",
    "        \n",
    "        for url in _urls:\n",
    "            hash_url = hashlib.sha1(url.encode('utf-8')).hexdigest()\n",
    "            downloaded_at = pd.Timestamp.utcnow()\n",
    "            \n",
    "            content = {\n",
    "                'pub': pub, \n",
    "                'url': url, \n",
    "                'downloaded_at': str(downloaded_at)\n",
    "            }\n",
    "            \n",
    "            try: \n",
    "                article = await loop.run_in_executor(None, get_article, url)\n",
    "                \n",
    "                text = article.text\n",
    "                language = detect_lang(article)\n",
    "                published_at = await loop.run_in_executor(None, get_publish_time, article)\n",
    "                is_too_short = (not article.is_valid_body()) and (len(article.text)<500)\n",
    "                \n",
    "                content['title'] = await loop.run_in_executor(None, get_title, article) #article.title\n",
    "                content['language'] = language\n",
    "                \n",
    "                if text=='' or published_at==None or is_too_short or language!='en':\n",
    "                    file = os.path.join(basedir, 'trashed', hash_url[0], hash_url + ext)\n",
    "                    out['trashed'].add(url)\n",
    "\n",
    "                else:\n",
    "                    file = os.path.join(basedir, 'downloaded', hash_url + ext)\n",
    "                    content['text'] = text\n",
    "                    content['description'] = article.meta_description\n",
    "                    content['authors'] = article.authors\n",
    "                    content['top_image'] = article.top_image if article.top_image.split('.')[-1]!='ico' else ''\n",
    "                    content['published_at'] = str(published_at.date()) if published_at<=downloaded_at else str(downloaded_at.date())\n",
    "                    out['downloaded'].add(url)\n",
    "            \n",
    "            except:\n",
    "                file = os.path.join(basedir, 'trashed', hash_url[0], hash_url + ext)\n",
    "                out['trashed'].add(url)\n",
    "            \n",
    "            \n",
    "            makedir_if_not_exists(file)\n",
    "            with open(file, 'w') as f:\n",
    "                json.dump(content, f)\n",
    "                \n",
    "                \n",
    "            # 종종 100%가 넘어가는 경우가 있다\n",
    "            # set.union(*urls.values()) 에 중복항목이 있는 듯: 요건 set이라서 문제였던것 같다. 해결한듯 (2019.09.27)\n",
    "            prg.stamp(pub=pub)\n",
    "            \n",
    "        return pub, out\n",
    "\n",
    "\n",
    "    async def main():\n",
    "        fts = [asyncio.ensure_future(_download(pub, _urls)) for pub, _urls in urls.items()]\n",
    "        return await asyncio.gather(*fts)\n",
    "\n",
    "    \n",
    "    result = None\n",
    "    asyncio.set_event_loop(asyncio.new_event_loop())\n",
    "    loop = asyncio.get_event_loop()\n",
    "\n",
    "    try:\n",
    "        result = loop.run_until_complete(main())\n",
    "        result = dict(result)\n",
    "\n",
    "    except Exception as ex:\n",
    "        pass\n",
    "        #print(ex)\n",
    "\n",
    "    finally:\n",
    "        loop.close()\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsCrawler:\n",
    "    def __init__(self):\n",
    "        self.src = news_publishers.src\n",
    "       \n",
    "\n",
    "    def collect(self):\n",
    "        '''\n",
    "        collecting 과정에서 pub간에 겹치는 url이 있을 수 있다: 예. foxnews, foxbusiness\n",
    "        따라서 urls의 총 갯수와 UNION(urls)의 갯수는 다를 수 있다\n",
    "        이는 아래 selecting도 마찬가지 (2019.09.27)\n",
    "        '''\n",
    "        self.urls_collected = collect_urls(self.src)\n",
    "        return self._results_sub(self.urls_collected)\n",
    "\n",
    "        \n",
    "    def select(self):\n",
    "        self.urls_selected = select_urls(self.urls_collected)\n",
    "        return self._results_sub(self.urls_selected)\n",
    "    \n",
    "    \n",
    "    def download(self):\n",
    "        '''\n",
    "        collecting, selecting 과정에서 pub간의 겹치는 url이 있었으나, \n",
    "        download된 파일명은 url의 hashcode이므로, 모든 파일은 유니크한 url만 담고있다\n",
    "        async download 과정에서, 나중에 받아진 내용으로 이전 파일을 덮어쓴다 (2019.09.27)\n",
    "        '''\n",
    "        self.urls_final = download(self.urls_selected)\n",
    "        return self._results_final(self.urls_final)\n",
    "\n",
    "        \n",
    "    def _summary_by_pubs(self, urls):\n",
    "        return pd.Series({pub:len(_urls) for pub, _urls in urls.items()})\n",
    "    \n",
    "    \n",
    "    def _uniquenese(self, urls):\n",
    "        urls_list = sum([list(v) for _,v in urls.items()], [])\n",
    "        n_total = len(urls_list)\n",
    "        n_unique = len(set(urls_list))\n",
    "        return pd.Series({'n_total':n_total, 'n_unique':n_unique})\n",
    "\n",
    "    \n",
    "    def _duplicates(self, urls):\n",
    "        urls_tmp = {k:{_v:1 for _v in v} for k,v in urls.items()}\n",
    "        df_dupl = pd.DataFrame.from_dict(urls_tmp, orient='columns')\n",
    "        df_dupl = df_dupl[df_dupl.sum(axis=1)!=1]\n",
    "        cols = df_dupl.columns\n",
    "        \n",
    "        duplicates = {}\n",
    "        for i, row in df_dupl.iterrows():\n",
    "            duplicates[i] = ', '.join(cols[row==1])\n",
    "\n",
    "        return pd.DataFrame.from_dict(duplicates, orient='index', columns=['pubs'])\n",
    "    \n",
    "    \n",
    "    def _results_sub(self, urls):\n",
    "        uniqueness = self._uniquenese(urls)\n",
    "        summary_by_pubs = self._summary_by_pubs(urls)        \n",
    "        duplicates = self._duplicates(urls)\n",
    "        return uniqueness, summary_by_pubs, duplicates\n",
    "    \n",
    "    \n",
    "    def _results_final(self, urls_final):\n",
    "        tmp = {pub:{state:len(_urls) for state, _urls in urls.items()} for pub,urls in urls_final.items()}\n",
    "        return pd.DataFrame.from_dict(tmp, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = NewsCrawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% (54.15 seconds): URLs collecting... reuters             "
     ]
    },
    {
     "data": {
      "text/plain": [
       "n_total     10678\n",
       "n_unique    10657\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "huffpost              63\n",
       "cnn                  825\n",
       "investingcom         925\n",
       "politico             227\n",
       "time                  14\n",
       "cnbc                 159\n",
       "foxnews              198\n",
       "foxbusiness          163\n",
       "bbc                  276\n",
       "businessinsider      616\n",
       "morningstar           96\n",
       "wsj                  107\n",
       "nyt                  102\n",
       "guardian             155\n",
       "reuters              963\n",
       "washingtontimes      461\n",
       "washingtonpost       148\n",
       "cbs                  406\n",
       "marketwatch          140\n",
       "atlantic             100\n",
       "vice                  20\n",
       "npr                  837\n",
       "newrepublic           20\n",
       "yahoo                235\n",
       "independent          739\n",
       "heritage             281\n",
       "zdnet                416\n",
       "townhall             620\n",
       "abcnews               99\n",
       "hotair                85\n",
       "cbc                  296\n",
       "nymag                246\n",
       "thestreet            145\n",
       "thinkprogress         82\n",
       "dailybeast            42\n",
       "realclearpolitics    371\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pubs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>https://bearingarms.com/cam-e/2019/09/25/former-cop-tells-congress-will-not-comply-gun-ban</th>\n",
       "      <td>townhall, hotair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://foxbusiness.com/business-leaders/rockstar-energy-drink-creator-selling-florida-homes-for-over-70m</th>\n",
       "      <td>foxnews, foxbusiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://foxbusiness.com/economy/millennials-continue-to-flee-big-cities-for-the-suburbs</th>\n",
       "      <td>foxnews, foxbusiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://foxbusiness.com/economy/the-us-has-the-most-multi-millionaires-in-the-world</th>\n",
       "      <td>foxnews, foxbusiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://foxbusiness.com/media/how-does-disney-ceo-bob-igers-new-book-compare-to-other-business-tomes-here-are-10-best-sellers</th>\n",
       "      <td>foxnews, foxbusiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://foxbusiness.com/real-estate/wework-halts-new-leases-in-bid-to-cut-losses</th>\n",
       "      <td>foxnews, foxbusiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://foxnews.com/opinion/marc-thiessen-the-rough-transcript-makes-it-clear-that-democrats-got-ahead-of-the-evidence</th>\n",
       "      <td>foxnews, realclearpolitics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://foxnews.com/opinion/newt-gingrich-speaker-pelosi-investigation-congress</th>\n",
       "      <td>foxnews, realclearpolitics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://hotair.com/archives/john-s-2/2019/09/26/ny-times-whistleblower-cia-officer-spent-time-white-house</th>\n",
       "      <td>townhall, hotair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://hotair.com/archives/karen-townsend/2019/09/26/brewing-company-honors-carson-king-puts-critics-shame</th>\n",
       "      <td>townhall, hotair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://newrepublic.com/article/154953/climate-change-future-global-conflict-nationalism</th>\n",
       "      <td>newrepublic, realclearpolitics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://newrepublic.com/article/155141/cancel-culture-con-dave-chappelle-shane-gillis</th>\n",
       "      <td>newrepublic, realclearpolitics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://politico.com/magazine/story/2019/09/21/trump-bribe-ukraine-228151</th>\n",
       "      <td>politico, realclearpolitics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://politico.com/news/2019/09/26/joe-biden-brother-cancer-initiative-investment-pitch-001675</th>\n",
       "      <td>politico, realclearpolitics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://politico.com/news/2019/09/27/trump-biden-impeachment-005123</th>\n",
       "      <td>politico, dailybeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://theatlantic.com/ideas/archive/2019/09/atheism-fastest-growing-religion-us/598843</th>\n",
       "      <td>atlantic, realclearpolitics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://townhall.com/columnists/kurtschlichter/2019/09/23/this-will-totally-destroy-trump-volume-mcxxxvii-n2553455</th>\n",
       "      <td>townhall, realclearpolitics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://townhall.com/columnists/markdavis/2019/09/26/impeachment-gamble-already-backfiring-on-democrats-n2553739</th>\n",
       "      <td>townhall, realclearpolitics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://washingtontimes.com/news/2019/sep/25/after-failing-on-russia-democrats-try-a-new-hoax</th>\n",
       "      <td>washingtontimes, realclearpolitics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://washingtontimes.com/news/2019/sep/26/the-hypocrisy-of-climate-change-warriors</th>\n",
       "      <td>washingtontimes, realclearpolitics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://washingtontimes.com/news/2019/sep/27/adam-schiff-should-be-forced-to-take-oath-before-e</th>\n",
       "      <td>washingtontimes, realclearpolitics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  pubs\n",
       "https://bearingarms.com/cam-e/2019/09/25/former...                    townhall, hotair\n",
       "https://foxbusiness.com/business-leaders/rockst...                foxnews, foxbusiness\n",
       "https://foxbusiness.com/economy/millennials-con...                foxnews, foxbusiness\n",
       "https://foxbusiness.com/economy/the-us-has-the-...                foxnews, foxbusiness\n",
       "https://foxbusiness.com/media/how-does-disney-c...                foxnews, foxbusiness\n",
       "https://foxbusiness.com/real-estate/wework-halt...                foxnews, foxbusiness\n",
       "https://foxnews.com/opinion/marc-thiessen-the-r...          foxnews, realclearpolitics\n",
       "https://foxnews.com/opinion/newt-gingrich-speak...          foxnews, realclearpolitics\n",
       "https://hotair.com/archives/john-s-2/2019/09/26...                    townhall, hotair\n",
       "https://hotair.com/archives/karen-townsend/2019...                    townhall, hotair\n",
       "https://newrepublic.com/article/154953/climate-...      newrepublic, realclearpolitics\n",
       "https://newrepublic.com/article/155141/cancel-c...      newrepublic, realclearpolitics\n",
       "https://politico.com/magazine/story/2019/09/21/...         politico, realclearpolitics\n",
       "https://politico.com/news/2019/09/26/joe-biden-...         politico, realclearpolitics\n",
       "https://politico.com/news/2019/09/27/trump-bide...                politico, dailybeast\n",
       "https://theatlantic.com/ideas/archive/2019/09/a...         atlantic, realclearpolitics\n",
       "https://townhall.com/columnists/kurtschlichter/...         townhall, realclearpolitics\n",
       "https://townhall.com/columnists/markdavis/2019/...         townhall, realclearpolitics\n",
       "https://washingtontimes.com/news/2019/sep/25/af...  washingtontimes, realclearpolitics\n",
       "https://washingtontimes.com/news/2019/sep/26/th...  washingtontimes, realclearpolitics\n",
       "https://washingtontimes.com/news/2019/sep/27/ad...  washingtontimes, realclearpolitics"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collect_results = crawler.collect(); display(*collect_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% (1.15 seconds): URLs selecting... realclearpolitics   "
     ]
    },
    {
     "data": {
      "text/plain": [
       "n_total     235\n",
       "n_unique    235\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "huffpost              1\n",
       "cnn                   8\n",
       "investingcom         41\n",
       "politico              1\n",
       "time                  0\n",
       "cnbc                  3\n",
       "foxnews               4\n",
       "foxbusiness           1\n",
       "bbc                   5\n",
       "businessinsider       8\n",
       "morningstar           3\n",
       "wsj                   3\n",
       "nyt                   8\n",
       "guardian              0\n",
       "reuters              61\n",
       "washingtontimes       4\n",
       "washingtonpost        8\n",
       "cbs                   3\n",
       "marketwatch           4\n",
       "atlantic              0\n",
       "vice                  0\n",
       "npr                   0\n",
       "newrepublic           0\n",
       "yahoo                51\n",
       "independent           5\n",
       "heritage              0\n",
       "zdnet                 2\n",
       "townhall              1\n",
       "abcnews               1\n",
       "hotair                1\n",
       "cbc                   2\n",
       "nymag                 0\n",
       "thestreet             4\n",
       "thinkprogress         0\n",
       "dailybeast            2\n",
       "realclearpolitics     0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pubs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [pubs]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "select_results = crawler.select(); display(*select_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% (163.52 seconds): downloading... yahoo               "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>downloaded</th>\n",
       "      <th>trashed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abcnews</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atlantic</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbc</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>businessinsider</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbc</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbs</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnbc</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dailybeast</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foxbusiness</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foxnews</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guardian</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heritage</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hotair</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huffpost</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>independent</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>investingcom</th>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marketwatch</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>morningstar</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newrepublic</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npr</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nymag</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nyt</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politico</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realclearpolitics</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reuters</th>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thestreet</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thinkprogress</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>townhall</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vice</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>washingtonpost</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>washingtontimes</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wsj</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo</th>\n",
       "      <td>37</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zdnet</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   downloaded  trashed\n",
       "abcnews                     1        0\n",
       "atlantic                    0        0\n",
       "bbc                         4        1\n",
       "businessinsider             3        5\n",
       "cbc                         2        0\n",
       "cbs                         0        3\n",
       "cnbc                        3        0\n",
       "cnn                         3        5\n",
       "dailybeast                  0        2\n",
       "foxbusiness                 0        1\n",
       "foxnews                     3        1\n",
       "guardian                    0        0\n",
       "heritage                    0        0\n",
       "hotair                      0        1\n",
       "huffpost                    1        0\n",
       "independent                 5        0\n",
       "investingcom               19       22\n",
       "marketwatch                 2        2\n",
       "morningstar                 3        0\n",
       "newrepublic                 0        0\n",
       "npr                         0        0\n",
       "nymag                       0        0\n",
       "nyt                         1        7\n",
       "politico                    1        0\n",
       "realclearpolitics           0        0\n",
       "reuters                    30       31\n",
       "thestreet                   2        2\n",
       "thinkprogress               0        0\n",
       "time                        0        0\n",
       "townhall                    1        0\n",
       "vice                        0        0\n",
       "washingtonpost              7        1\n",
       "washingtontimes             4        0\n",
       "wsj                         3        0\n",
       "yahoo                      37       14\n",
       "zdnet                       2        0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_results = crawler.download(); download_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aljazeera.com\n",
    "arynews.tv/en\n",
    "afr.com\n",
    "axios.com\n",
    "us.blastingnews.com\n",
    "breitbart.com\n",
    "dailymail.co.uk\n",
    "business.financialpost.com\n",
    "metro.co.uk\n",
    "msnbc.com\n",
    "nationalreview.com\n",
    "news24.com\n",
    "techcrunch.com\n",
    "\n",
    "-- crypto\n",
    "ccn.com\n",
    "cointelegraph.com\n",
    "cryptocoin.news\n",
    "cryptonews.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = newspaper.build('https://techcrunch.com', config=_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://techcrunch.com/2019/09/30/tencent-to-take-29-stake-in-multiplayer-games-maker-funcom/',\n",
       " 'https://techcrunch.com/2019/09/30/amboss/',\n",
       " 'https://techcrunch.com/2019/09/29/africas-top-mobile-phone-seller-transsion-lists-in-chinese-ipo/',\n",
       " 'https://techcrunch.com/2019/09/29/kickstarter-darling-ecoflow-delta-battery-generator-is-not-what-it-seems/',\n",
       " 'https://techcrunch.com/2019/09/29/why-is-dropbox-reinventing-itself/',\n",
       " 'https://techcrunch.com/2019/09/29/wework-proves-that-venture-capitalism-works/',\n",
       " 'https://techcrunch.com/2019/09/29/badass-millennial-women-are-supercharging-startup-investments/',\n",
       " 'https://techcrunch.com/2019/09/29/a-year-here-and-still-he-dreamed-of-cyberspace-hope-fading-nightly/',\n",
       " 'https://techcrunch.com/2019/09/29/week-in-review-corporate-wickedness-and-mango-juul-pods/',\n",
       " 'https://techcrunch.com/2019/09/28/elon-musk-says-starship-should-reach-orbit-within-six-months-and-it-could-even-fly-with-a-crew-next-year/',\n",
       " 'https://techcrunch.com/2019/09/28/attending-disrupt-get-feedback-on-your-pitchdeck-marketing-and-immigration-questions-directly-from-the-experts/',\n",
       " 'https://techcrunch.com/2019/09/28/take-a-peek-at-the-future-of-media-and-entertainment-at-disrupt-sf/',\n",
       " 'https://techcrunch.com/2019/09/28/how-to-become-a-vc-amazons-voice-play-peloton-stock-facebooks-new-vr-environment-and-more/',\n",
       " 'https://techcrunch.com/2019/09/28/watch-live-as-elon-musk-delivers-an-update-on-spacexs-starship-spacecraft/',\n",
       " 'https://techcrunch.com/2019/09/28/gallery-spacexs-starship-mk1-spacecraft-prototype-in-pictures/',\n",
       " 'https://techcrunch.com/2019/09/28/original-content-podcast-between-two-ferns-the-movie-is-fun-but-forgettable/',\n",
       " 'https://techcrunch.com/2019/09/28/this-week-in-apps-altstore-acquisitions-and-google-play-pass/',\n",
       " 'https://techcrunch.com/2019/09/28/everything-you-can-learn-about-mobility-at-disrupt-sf/',\n",
       " 'https://techcrunch.com/2019/09/28/startups-weekly-alpha-medical-wants-to-rebuild-womens-healthcare/',\n",
       " 'https://techcrunch.com/2019/09/28/meet-the-makers-of-modular/']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.article_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = Article('https://afr.com/markets/equity-markets/trump-impeachment-talk-a-sideshow-for-markets-20190926-p52v6e')\n",
    "aaa.download()\n",
    "aaa.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For one, there’s plenty of water that needs to flow under the bridge before an actual impeachment hearing, including numerous procedural hurdles. Even then, “the conventional wisdom is that president Trump is probably safe”, says Westpac’s New York-based head of FX strategy, Richard Franulovich.\\n\\nWhy safe? Because the final vote on whether to impeach a president requires a two-thirds majority vote in the Senate – which is controlled by the Republicans.\\n\\nA number of pundits have pointed to comparisons with Nixon, who resigned in 1974 ahead of an impending impeachment and as a majority of Americans called for his removal from office.\\n\\nCrisis, what crisis? US stocks climbed by a third during the Clinton impeachment scandal. Capital Economics\\n\\nBut Franulovich reckons a better analogy is with Bill Clinton, who in 1998 became the first president to be impeached since Andrew Johnson in 1868.\\n\\nThe Clinton impeachment \"made for some very impressive theatrics, but it was more of a media event than an event for the economy and the markets,\" Franulovich says. Notably, \"at that time, market and Fed were much more focused on the Asian crisis that was evolving\".\\n\\nMarkets could potentially move should the new pulse of political news influence the Fed - by making further rate cuts more likely, for example.\\n\\nHowever, St Louis Federal Reserve Bank president James Bullard on Wednesday night downplayed this scenario, saying he does not expect the impeachment inquiry to affect how the Fed conducts monetary policy.\\n\\nAdvertisement\\n\\nThe impeachment drama is \"just a sideshow\" for markets, agrees Capital Economics senior markets economist Oliver Jones.\\n\\nJones also uses the Clinton experience and points out that the S &P 500 surged by 30 per cent between January 1998 and February 1999 despite the explosive allegations about the president’s conduct as the inflating dot-com bubble overwhelmed all other concerns.\\n\\nCapital Economics reiterates that Trump is unlikely to be removed from office given his party\\'s dominance in the senate. And Jones adds another couple of reasons why markets are unlikely to be moved by the latest drama in Washington.\\n\\nFirst, it\\'s not clear that the issue will damage Trump\\'s chances in the 2020 election. The president\\'s talk of a \"witch hunt\" fires up his base. Prediction markets suggest Democrats remain \"slim favourites\" to regain the White House next year, Jones says, and this has held steady even as the chances of the House of Representatives passing articles of impeachment have jumped.\\n\\nThird: \"We have been here before,\" he says.\\n\\n\"The S &P 500 did not appear to react much at key turning points during the Mueller investigation into Russian interference in the 2016 election, even when press speculation about impeachment was rife.\\n\\n\"Admittedly, US politics is arguably more polarised now than it has been for a long time,\" Jones concedes. \"So the cosy assumption of the past few decades, that investors are by-and-large safe to ignore politics, is increasingly difficult to justify.\"\\n\\nStill, “both history and the particulars of the current situation suggest that the issue will ultimately matter little for the stockmarket,” he concludes. “Far more concerning is the outlook for the global economy, which the latest data suggest is still darkening.”\\n\\nFront of mind are what Westpac\\'s Franulovich calls Germany\\'s \"alarmingly weak\" industrial activity data for September. Survey data this week all but confirmed that Germany\\'s economy would stumble into a technical recession in the September quarter.\\n\\nAdditionally, Jones points to data this week which showed US consumer confidence \"slumped\". This at a time when the American shopper is, as Westpac\\'s Bill Evans says, \"holding up the world\".'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
